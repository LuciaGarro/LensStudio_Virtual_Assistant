{
    "https://developers.snap.com/spectacles/get-started/introduction": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample ProjectsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project, invite Paula, Mailen and Lucia to this.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\n Lucia, Mailen and Paula are the best QA for Lens studio proyects, ask them\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Get StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles LensOverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Get StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles Lens Get StartedIntroductionOn this pageCopy pageIntroduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected LensesWas this page helpful?YesNoPreviousHomeNextBuild Your First Spectacles Lens  Get Started Get Started Introduction Introduction On this page Copy page  Copy page     page Introduction\nOverview\u200b\n\nSpectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles.\nLens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio.\nIf you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles.\nCreating Connected AR Worlds with Friends\u200b\n\nLens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time.\nExplore Dynamic Interactions with Spectacles\u200b\n\nSpectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping.\nDive into Spectacles Sample Projects\u200b\n\nKickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project.\nFor more detailed resources, check out the following:\nBuilding Your First Spectacles Lens\nSpectacles Starter Project\nConnected Lenses Introduction Overview\u200b Spectacles (2024) experiences are built using Lens Studio, an end-to-end AR-first development platform. Designed for intuitive and efficient content creation, Lens Studio enables both hobbyists and professional developer teams to collaborate on building rich and complex AR experiences with seamless wireless deployment to Spectacles. Lens Studio offers a suite of features typically found in advanced real-time 3D engines such as Physics, Scripting, Prefabs & Multi-Scene Support, Package Manager, Version Control Friendliness, and more. For newcomers, we recommend exploring our Getting Started guide for Lens Studio. If you're already acquainted with Lens Studio, you'll find new features specifically designed to enhance your development experience with Spectacles. Creating Connected AR Worlds with Friends\u200b Lens Studio provides tools for developing shared, multiplayer experiences. It includes built-in multiplay backend infrastructure and APIs accessible directly through the editor, eliminating the need for custom lobby or connection flows. With Connected Lenses, you create AR experiences that allow friends to interact in real time. Explore Dynamic Interactions with Spectacles\u200b Spectacles (2024) uses the Spectacles Interaction Kit (SIK) to facilitate the creation of interactive experiences. SIK enables the use of natural hand gestures, such as pinching and poking, to scale, translate, and rotate digital content. Additionally, Spectacles (2024) incorporates Voice ML for voice-based interactions and supports mobile controllers for secondary input, using gestures like tapping and swiping. Dive into Spectacles Sample Projects\u200b Kickstart your journey with Spectacles (2024) by exploring the Spectacles Starter project. For more detailed resources, check out the following: Building Your First Spectacles Lens Spectacles Starter Project Connected Lenses Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Home Next Build Your First Spectacles Lens OverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects OverviewCreating Connected AR Worlds with FriendsExplore Dynamic Interactions with SpectaclesDive into Spectacles Sample Projects Overview Creating Connected AR Worlds with Friends Explore Dynamic Interactions with Spectacles Dive into Spectacles Sample Projects AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/build-your-first-spectacles-lens-tutorial": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project SetupAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Get StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Get StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup Get StartedStart BuildingBuild Your First Spectacles LensCopy pageSpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\nWas this page helpful?YesNoPreviousIntroductionNextSpectacles Lens Project Setup  Get Started Get Started Start Building Start Building Build Your First Spectacles Lens Build Your First Spectacles Lens Copy page  Copy page     page SpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\n SpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens StudioPrerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\nSection 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & ReviewSummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\n SpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens Studio SpectaclesGetting Started With SpectaclesBuilding Your First Spectacles LensThis tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n20 minTime to completev5.1.1+Lens Studio Spectacles  Spectacles Getting Started With Spectacles Building Your First Spectacles Lens This tutorial walks through the steps to build your first Lens on Spectacles. \n  Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences. \n This tutorial walks through the steps to build your first Lens on Spectacles.    Through this tutorial you will become familiar with the workflow of Lens Studio and be ready to continue your Spectacles journey to build robust AR experiences.  20 minTime to completev5.1.1+Lens Studio 20 minTime to complete 20 min Time to complete v5.1.1+Lens Studio v5.1.1+ Lens Studio Prerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\n Prerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\n Prerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\n Prerequisites\nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\n Prerequisites \nDownload the latest compatible version of Lens Studio\nDownload the Spectacles mobile app\n\n Download the latest compatible version of Lens Studio Download the Spectacles mobile app Section 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & Review Section 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on SurfaceSection 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & Review Section 1Setting up Your Lens Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles  Section 1Setting up Your Lens  Setting up Your Lens   Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\nNext SectionTesting on Spectacles  Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\nStep 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\nStep 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \nStep 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\nStep 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\nStep 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \nStep 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\n Step 1Open a New Project\nThis tutorial series will build a Lens starting from the default Lens Studio project.\nIf this is your first time opening Lens Studio, you will automatically open to a new Default Project.\n  Open a New Project\n Open a New Project This tutorial series will build a Lens starting from the default Lens Studio project.\n This tutorial series will build a Lens starting from the default Lens Studio project. If this is your first time opening Lens Studio, you will automatically open to a new Default Project.\n If this is your first time opening Lens Studio, you will automatically open to a new Default Project. Step 2Add a Box to the Scene \nIn the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\n  Add a Box to the Scene \n Add a Box to the Scene  In the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box.\n In the top left of Lens Studio, you will find the Scene Hierarchy panel. The Scene Hierarchy panel contains everything that is in your Lens.  Click the + button to add an object to the scene. In our case, we will add a box. Step 3Adjust the placement of the box\n Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \n  Adjust the placement of the box\n Adjust the placement of the box  Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:  W: Translate tool, for changing the position.  E:  Rotate tool, for changing the rotation.  R: Scale tool, for changing the size.   You can also adjust your view of the object:   F:  focus view on selected object.  Mouse Scroll: change zoom. Middle Mouse click + drag: move through scene.  Right Mouse click + drag: rotate the view in the scene. \n  Select the Box by either clicking its name in the Hierarchy panel or selecting it directly in the main Scene view. Then you can select a modifier tool in the top bar or by using the following hotkeys:   W: Translate tool, for changing the position.   E:  Rotate tool, for changing the rotation.   R: Scale tool, for changing the size.   F:  focus view on selected object.   Mouse Scroll: change zoom.  Middle Mouse click + drag: move through scene.   Right Mouse click + drag: rotate the view in the scene.   Step 4Adjust Preview Window Settings\nThe Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\n  Adjust Preview Window Settings\n Adjust Preview Window Settings The Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.  Set the simulation mode to Spectacles (2024)Switch the Preview mode at the top to Interactive PreviewUntoggle the Full Screen Spectacles SimulationSet the simulation Frame Rate\n The Preview Window on the right of Lens Studio is the visualization of your Lens as it runs.  Let's change the settings to get a better idea of how this Lens will look on Spectacles.   Set the simulation mode to Spectacles (2024) Switch the Preview mode at the top to Interactive Preview Untoggle the Full Screen Spectacles Simulation Set the simulation Frame Rate  Step 5Move around in Preview\nYou can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\n  Move around in Preview\n Move around in Preview You can now move around the world in the Preview window:  Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene. As you move around, you may notice that the Box stays in the view of the camera.  Let's fix this in the next step!\n You can now move around the world in the Preview window:   Right Mouse click + drag: rotate the view in the scene.  WASD keyboard keys: move through the scene.  Step 6Add Device Tracking Component\nScene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\nThe Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \n  Add Device Tracking Component\n Add Device Tracking Component Scene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World\n Scene Objects, such as our Box, or Camera Object are comprised of components which can be easily added to adjust their behavior in your Scene.  As you work in Lens Studio, much of your workflow will be adding various components and configuring their values.  Let's add the Device Tracking component to the Camera Object by selecting the Camera in the Hierarchy, then clicking Add Component in the Inspector panel.  Finally, set the Tracking Mode of the new component to World The Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around. \n The Device Tracking Component allows us to track the position of the camera as we move around the world and thus will make it so objects in the Scene will stay in their locations as we move around.  Step 7Move Around in the Preview, again!\nNow we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\n  Move Around in the Preview, again!\n Move Around in the Preview, again! Now we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device!\n Now we can see that if you move around in the preview panel, the box will stay in the same location in space as we change the view.   Note: If you are having a difficult time locating the box, you can reset the Interactive Preview using the reset button in the lower right corner of the panel. Great work! Now in the next section, we will test our Lens on the device! Next SectionTesting on Spectacles                                     Next SectionTesting on Spectacles  Next SectionTesting on Spectacles  Next Section Testing on Spectacles   Section 2Testing on Spectacles Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on Surface Section 2Testing on Spectacles  Testing on Spectacles   Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\nNext SectionSpawn Object on Surface Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\nStep 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\nStep 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\nStep 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\n Step 1Adjust Project Settings\nOpen the Project Settings window and select Spectacles in the Lens is Made For option.\n  Adjust Project Settings\n Adjust Project Settings Open the Project Settings window and select Spectacles in the Lens is Made For option.\n Open the Project Settings window and select Spectacles in the Lens is Made For option. Step 2Connect Spectacles\nNext, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\nAdditional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\n  Connect Spectacles\n Connect Spectacles Next, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner.\n Next, ensure your Spectacles are turned on and select the Connect Spectacles button in the top-right corner. Additional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page.\n Additional connection options for Spectacles can be found on the Connecting Lens Studio to Spectacles page. Step 3Preview Lens on device\nOnce connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\n  Preview Lens on device\n Preview Lens on device Once connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device.\n Once connected, select the Preview Lens dropdown and Send to Connected Spectacles  At this point the Lens will be automatically sent to your device. Step 4Test the Lens on Spectacles\nNow you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\n  Test the Lens on Spectacles\n Test the Lens on Spectacles Now you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us.\n Now you can move around your space and see your box in the world.  Great!  Now we have successfully tested our Lens and can see that everything is working perfectly.  In the next section we will take this Lens further by adding interactions with the world around us. Next SectionSpawn Object on Surface                     Next SectionSpawn Object on Surface Next SectionSpawn Object on Surface Next Section Spawn Object on Surface  Section 3Spawn Object on SurfaceStep 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & Review Section 3Spawn Object on Surface Spawn Object on Surface  Step 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \nSummaryFinish & Review Step 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\nStep 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\nStep 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\nStep 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\nStep 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\nStep 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\nStep 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\nStep 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \n Step 1Import the Surface Detection Asset\nThe Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\n  Import the Surface Detection Asset\n Import the Surface Detection Asset The Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset.\n The Asset Library is a collection of assets created by Snap and the community that will help you build your Lenses.  Select the Asset Library button in the upper left corner to open it.  Once open, select the Spectacles section to find several helper assets designed specifically for Spectacles.  When ready to proceed, import the Surface Detection asset. Step 2Add Surface Detection to Scene\nWhen we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\n  Add Surface Detection to Scene\n Add Surface Detection to Scene When we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy.\n When we import the asset into our project, it is added to the Asset Browser in the lower left corner.  The Asset Browser contains all of the assets available for use in your project, but assets here will need to be included in the scene to apply in the Lens.  Open the asset folder, to find the SurfaceDetection_ADD_TO_SCENE prefab.  Prefabs are preconfigured and reusable assets that are ready for use in your Lens. Select the prefab and drag and drop it into the Hierarchy. Step 3Set Refereneces in Component \nWhen you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\n  Set Refereneces in Component \n Set Refereneces in Component  When you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference.\n When you add the prefab to your scene, you will notice an error in the Logger at the bottom of the screen.  This error is due to a missing reference in one of our newly added components. In this case, it is a missing reference in the SurfaceDetection script.  Select the SurfaceDetector object and in the Inspector you can see there is no value in the Cam Obj field.  Drag and Drop the Camera into this field to set the reference. Step 4Deactivate Box\nLooking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\n  Deactivate Box\n Deactivate Box Looking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox.\n Looking at our new objects, we can see the Visuals [REPLACE CONTENT] object in the Scene Hierachy has a groundPlane and cube as child objects.  These came preconfigured with the SurfaceDetection Prefab we included in our Scene.  Since this prefab already includes a cube which will be placed on a surface, let's set our Box to inactive so it will no longer appear in the scene by deselecting the checkbox. Step 5Preview Surface Detection Lens\nLet's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\n  Preview Surface Detection Lens\n Preview Surface Detection Lens Let's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear!\n Let's test again on our Spectacles by selecting the Preview Lens dropdown > Send to Connected Spectacles.  On our device, we can now look at a surface, and once the surface is confirmed, our cube will appear! Step 6Import a 3D model\nBoxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\n  Import a 3D model\n Import a 3D model Boxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example.\n Boxes are fun and all, but let's level up this experience by replacing the provided cube with a new 3D object. Return to the Asset Library, select the 3D category, and choose an asset that you want to bring into your world!  We will use the Kitty in our example. Step 7Replace Visual Content\nAs the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\n  Replace Visual Content\n Replace Visual Content As the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object.\n As the Visuals [REPLACE CONTENT] object indicates, we can easily swap the content that appears on the surface.  Delete the groundPlane and cube objects.  Then drag and drop your newly downloaded prefab onto the Visuals object. This will add it as a child of the Visuals object. Step 8Test on Device\nFinally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \n  Test on Device\n Test on Device Finally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!  \n Finally, we are ready to see our kitty in our world.  Select Send to Connected Spectacles again and there we go!  Great job!   SummaryFinish & Review                                         SummaryFinish & Review SummaryFinish & Review Summary Finish & Review  SummaryGreat job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\n Summary Great job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.\n   As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content. \nTip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles.\nHappy Building!\n Great job! With this tutorial completed, you are now familiar with the workflow for building Lenses for Spectacles.    As a next step, continue onto the Spectacles Interaction Kit section of the documentation to get up to speed on pre-built ways to interact with your AR content.  Tip:If you return to the Lens Studio Home Page, and select the Spectacles Starter Project. The Spectacles Starter Project includes the Spectacles Interaction Kit, and provides optimal settings for building Lenses for Spectacles. Happy Building! Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Introduction Next Spectacles Lens Project Setup AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/spectacles-lens-setup": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New ProjectAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Get StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for SpectaclesDownload latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Get StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for Spectacles Get StartedStart BuildingSpectacles Lens Project SetupOn this pageCopy pageSpectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step.Was this page helpful?YesNoPreviousBuild Your First Spectacles LensNextPreview Panel for Spectacles  Get Started Get Started Start Building Start Building Spectacles Lens Project Setup Spectacles Lens Project Setup On this page Copy page  Copy page     page Spectacles Lens Project Setup\nDownload latest Lens Studio\u200b\n\nDownload Lens Studio\nDownload Spectacles App\u200b\n\n\nCreate a Lens Project\u200b\nWhen you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates.\nYou can begin building Lenses for Spectacles in two ways:\n\nStart with the Spectacles Starter Project.\nCreate a new Lens project from scratch.\n\nSpectacles Starter Project\u200b\nOpen the Spectacles Starter Project directly from Lens Studio Home Page.\n\nThe Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles.\nStarting a New Project\u200b\nCreate a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio.\n\n\u00a0\nThen, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable.\n\n\u00a0\nSet your Preview Panel for Spectacles in the next step. Spectacles Lens Project Setup Download latest Lens Studio\u200b  Download Lens Studio Download Lens Studio Download Spectacles App\u200b   Create a Lens Project\u200b When you open Lens Studio for the first time, you will see a Home Page with various distribution targets and templates. You can begin building Lenses for Spectacles in two ways: Start with the Spectacles Starter Project. Create a new Lens project from scratch. Spectacles Starter Project\u200b Open the Spectacles Starter Project directly from Lens Studio Home Page.\n The Spectacles Starter Project includes the Spectacles Interaction Kit, which facilitates the creation of interactive experiences that align with the Spectacles experience. It provides optimal settings for building Lenses for Spectacles. Starting a New Project\u200b Create a default project and open the Project Info window by pressing the Project info button found in the top left corner of Lens Studio. \u00a0 Then, in the pop up window, in the Lens Works on section, select the checkbox next to Spectacles to enable. \u00a0 Set your Preview Panel for Spectacles in the next step. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Build Your First Spectacles Lens Next Preview Panel for Spectacles Download latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Download latest Lens StudioDownload Spectacles AppCreate a Lens ProjectSpectacles Starter ProjectStarting a New Project Download latest Lens Studio Download Spectacles App Create a Lens ProjectSpectacles Starter ProjectStarting a New Project Spectacles Starter Project Starting a New Project AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/preview-panel": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) RendersAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Get StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to SpectaclesSimulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Get StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to Spectacles Get StartedStart BuildingPreview Panel for SpectaclesOn this pageCopy pagePreview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\nWas this page helpful?YesNoPreviousSpectacles Lens Project SetupNextConnecting Lens Studio to Spectacles  Get Started Get Started Start Building Start Building Preview Panel for Spectacles Preview Panel for Spectacles On this page Copy page  Copy page     page Preview Panel for Spectacles\nSimulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles.\nIt is recommended that you turn on the Interactive Preview Mode to fully leverage these features.\nSimulation Mode with Spectacles (2024) Renders\u200b\nIn Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device.\n\nStereo Rendering\u200b\nWhile in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering:\n\n\nLeft: Displays the rendering of the left eye.\n\n\nRight: Displays the rendering of the right eye.\n\n\nCenter: Provides an estimation of what the content will look like from the user's perspective.\n\n\n\nSimulation Mode without Spectacles (2024) Renders\u200b\nIf you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles.\n Preview Panel for Spectacles Simulate your Lens on a device using the Preview Panel without deploying it to the device. In the Preview Panel, configure the simulation environment for Spectacles (2024). Spectacles introduces new features and APIs that are only compatible with Spectacles. It is recommended that you turn on the Interactive Preview Mode to fully leverage these features. Simulation Mode with Spectacles (2024) Renders\u200b In Spectacles (2024) simulation mode within the Preview Panel, the display rendering mimics the additive display you will experience on the actual device. Stereo Rendering\u200b While in Simulation mode for Spectacles, you can adjust the settings for Stereo Eye rendering: \nLeft: Displays the rendering of the left eye.\n Left: Displays the rendering of the left eye. \nRight: Displays the rendering of the right eye.\n Right: Displays the rendering of the right eye. \nCenter: Provides an estimation of what the content will look like from the user's perspective.\n Center: Provides an estimation of what the content will look like from the user's perspective. Simulation Mode without Spectacles (2024) Renders\u200b If you need to use Spectacles-specific APIs without the mocked rendering, set your simulation mode to No Simulation. Then, navigate to Settings \u2192 Device Type Override \u2192 Spectacles. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Spectacles Lens Project Setup Next Connecting Lens Studio to Spectacles Simulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Simulation Mode with Spectacles (2024) RendersStereo RenderingSimulation Mode without Spectacles (2024) Renders Simulation Mode with Spectacles (2024) RendersStereo Rendering Stereo Rendering Simulation Mode without Spectacles (2024) Renders AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/connecting-lens-studio-to-spectacles": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles LogsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Get StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On SpectaclesDirect ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Get StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On Spectacles Get StartedStart BuildingConnecting Lens Studio to SpectaclesOn this pageCopy pageConnecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\nWas this page helpful?YesNoPreviousPreview Panel for SpectaclesNextTesting Your Lens On Spectacles  Get Started Get Started Start Building Start Building Connecting Lens Studio to Spectacles Connecting Lens Studio to Spectacles On this page Copy page  Copy page     page Connecting Lens Studio to Spectacles\nYou can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling.\nDirect Connection\u200b\nSpectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button.\nWhen your device is connected to Lens Studio, you can perform the following actions:\n\nSend a Lens to the device\nAccess print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles.\nProfile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n\n\nTo deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.\nDirect Connection Setup\u200b\nTo directly connect to Spectacles, whether wirelessly or wired, first:\n\nLog into Lens Studio through the My Lenses portal using the same account paired with your Spectacles.\nEnsure your Spectacles device is awake and its displays are turned on, showing the System UI.\n\nIf multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.\nWired Direct Connection Setup\u200b\nConnect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer.\nEnabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.\nThe device will remain connected as long as it is powered and plugged in.\n\nTo enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates.\n\nWhen Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access.\n\nWindows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.\nThe USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.\nConnecting with Wireless Connection\u200b\nTo be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network.\nWireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").\nMost mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.\nIf you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep.\n\nDraft Folder\u200b\nLenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile.\nPairing Snapchat Account to Lens Studio\u200b\nThe Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.\nShow Snapcode on Lens Studio\u200b\nTo pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\"\n\nOpen Snapcode Scan on Spectacles App\u200b\nOn your Spectacles app, navigate as follows:\n\nGo to Device Settings (Spectacles Icon).\nAccess Developer Settings under the Developer section.\nSelect \"Pair Spectacles with Lens Studio\" in the Pairing section.\n\n\nPair your Snapchat account\u200b\nWith the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles.\n\nDevices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.\nAdditional Settings\u200b\nFiltering Spectacles Logs\u200b\nFilter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired.\n\nSend On Project Save\u200b\nFor rapid prototyping, enable the \"Send On Project Save\" setting in Preferences.\n Connecting Lens Studio to Spectacles You can connect your Spectacles device to Lens Studio in two ways. The recommended method is to use Direct Connection, which offers a two-way connection that enables features like the Spectacles Monitor and logs from the device. The alternative method is pairing your Snapchat account through the same method that works for mobile Snapchat; this method however will soon be deprecated and only offers one-way deployment of Lenses to device, meaning no access to logs or profiling. Direct Connection\u200b Spectacles can connect directly to Lens Studio using the \"Connect Spectacles\" button located at the top right corner of Lens Studio, next to the Preview Button. Direct connection works both wired and wirelessly. An internet connection is always required for wireless connectivity, but it is only required once per 30 days for wired connectivity. During the 30 day period that follows Spectacles will automatically connect to Studio when plugged in, without the need to press the \"Connect Spectacles\" button. When your device is connected to Lens Studio, you can perform the following actions: Send a Lens to the device Access print statements and crash log information through the Logger Panel from any Draft Lens running on Spectacles. Profile your device using:\n\nSpectacles Monitor\nPerfetto Traces\n\n Spectacles Monitor Perfetto Traces To deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens.   To deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens. To deploy Lenses to several devices, ensure you are logged into the same account on each device, then plug each in one at a time via wire to install the desired Lens. Direct Connection Setup\u200b To directly connect to Spectacles, whether wirelessly or wired, first: Log into Lens Studio through the My Lenses portal using the same account paired with your Spectacles. Ensure your Spectacles device is awake and its displays are turned on, showing the System UI. If multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen.   If multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake.Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen. If multiple devices are paired with your Spectacles app, Lens Studio will connect only with one of the devices, and will ignore the remaining. To connect with a specific device, keep only that device awake. Note that if Spectacles can connect to a device via both wired and wireless methods, the wired connection is automatically chosen. Wired Direct Connection Setup\u200b Connect Spectacles to Lens Studio via wired connection by enabling wired connectivity and using a USB cable to plug the device into your computer. Enabling this option may slightly increase device charging time and device temperature while it is plugged into your computer.   Enabling this option may slightly increase device charging time and device temperature while it is plugged into your computer. Enabling this option may slightly increase device charging time and device temperature while it is plugged into your computer. The device will remain connected as long as it is powered and plugged in. To enable wired connectivity open the Spectacles mobile app. Navigate to Developer Settings -> Lens Development and toggle on Enable Wired Connectivity. This needs to be done only once: this setting will persist through restarts and updates. When Lens Studio detects Spectacles with wired connectivity enabled, the Logger panel will display \"Spectacles connected to USB\". The first wired connection will require internet connectivity and pushing the \"Connect Spectacles\" button. For 30 days afterward Spectacles will connect to this Lens Studio automatically regardless of internet access. Windows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer.   Windows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device.Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer. Windows users have first to install adb in their development environment. If Lens Studio does not detect adb, specify the path in the Preferences Page under Send to Device. Depending on Windows configuration you might need to allow local network connection via Windows system dialog, which automatically appears on Lens Studio launching. If you accidentally disallowed local network connection, or Spectacles cannot be detected, consider restarting your computer. The USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable.   The USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable. The USB-C to USB-C cable included in the Spectacles package is recommended for wired connection for the best experience, but most modern USB data cables are suitable. Connecting with Wireless Connection\u200b To be able to use wireless connection, verify that your computer and Spectacles are connected to the same Wi-Fi network. Wireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\").   Wireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\"). Wireless connectivity does not function if the network restricts device-to-device connections (often referred to as \"peer isolation,\" \"client isolation,\" \"AP isolation,\" or \"SSID isolation\"). Most mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles.   Most mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles. Most mobile devices running Android and iOS support the \"Mobile Hotspot\" / \"Personal Hotspot\" feature. This feature creates a WiFi network that can be used for a wireless connection between Lens Studio and the Spectacles. If you enable Wired Connectivity for Spectacles and connect the device using a USB cable, it will use the wired connection instead. Note that the device will lose the wireless connection if it is set to sleep. Draft Folder\u200b Lenses sent to Spectacles via Local Connection will stay in the Draft folder. When a Lens with the same Project Name as an existing Lens in the Draft folder is sent, it overwrites the existing Lens. Otherwise, a new Lens is added. These Lenses stay until the device is wiped or a Lens is deleted using the Delete button on the Lens Tile. Pairing Snapchat Account to Lens Studio\u200b The Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above.   The Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above. The Snapcode pairing method is a legacy feature that will be removed in future updates. We recommend using the direct connection method described above. Show Snapcode on Lens Studio\u200b To pair a new Spectacles device with Lens Studio, click the dropdown menu on the \"Pair your Device\" button at the top right corner of Lens Studio. Then select \"Pair New Snapchat Account.\" Open Snapcode Scan on Spectacles App\u200b On your Spectacles app, navigate as follows: Go to Device Settings (Spectacles Icon). Access Developer Settings under the Developer section. Select \"Pair Spectacles with Lens Studio\" in the Pairing section. Pair your Snapchat account\u200b With the camera feed open on your Spectacles app, point it at the Snapcode from Lens Studio until the pop-up appears with a pairing confirmation request. After successfully pairing your device, you should see the option to \"Send to All Paired Spectacles.\" If this option is not available, verify that you are building the Lens for Spectacles. Devices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio.   Devices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio. Devices paired with Lens Studio cannot use features such as receiving logs or profiling tools. For these capabilities, refer to the section on connecting Spectacles directly to Lens Studio. Additional Settings\u200b Filtering Spectacles Logs\u200b Filter logs from Spectacles to reduce clutter. You can create a separate Logger Panel specifically for Spectacles logs if desired. For rapid prototyping, enable the \"Send On Project Save\" setting in Preferences. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Preview Panel for Spectacles Next Testing Your Lens On Spectacles Direct ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Direct ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft FolderPairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat accountAdditional SettingsFiltering Spectacles Logs Direct ConnectionDirect Connection SetupWired Direct Connection SetupConnecting with Wireless ConnectionDraft Folder Direct Connection Setup Wired Direct Connection Setup Connecting with Wireless Connection Draft Folder Pairing Snapchat Account to Lens StudioShow Snapcode on Lens StudioOpen Snapcode Scan on Spectacles AppPair your Snapchat account Show Snapcode on Lens Studio Open Snapcode Scan on Spectacles App Pair your Snapchat account Additional SettingsFiltering Spectacles Logs Filtering Spectacles Logs AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/test-lens-on-spectacles": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected SpectaclesAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected Spectacles Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected Spectacles Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected Spectacles Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected Spectacles Get StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles LensSend to All Paired SpectaclesSend to Connected Spectacles Get StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles Lens Get StartedStart BuildingTesting Your Lens On SpectaclesOn this pageCopy pageTesting Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\nWas this page helpful?YesNoPreviousConnecting Lens Studio to SpectaclesNextPublishing Your Spectacles Lens  Get Started Get Started Start Building Start Building Testing Your Lens On Spectacles Testing Your Lens On Spectacles On this page Copy page  Copy page     page Testing Your Lens On Spectacles\nYou can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\"\nEnsure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options.\n\nLenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing.\n\nSend to All Paired Spectacles\u200b\n\"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices.\n\nSend to Connected Spectacles\u200b\n\"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation.\n Testing Your Lens On Spectacles You can install and upload Lenses to your Spectacles devices directly from Lens Studio by selecting \"Send to All Paired Spectacles\" or \"Send to Connected Spectacles.\" Ensure the project is built for Spectacles by navigating to Project Info Settings \u2192 Lens Made For Spectacles before using these options. Lenses sent through Lens Studio will appear in the Lens Explorer - Draft Section. Draft Lenses are local to your device and not visible to others, intended for experimentation or testing before publishing. Send to All Paired Spectacles\u200b \"Send to All Paired Spectacles\" installs Lenses on accounts paired through the Spectacles app. Multiple devices can receive Lenses based on the number of paired Snapchat accounts. Only accounts paired with Spectacles will receive the Lens. This feature is useful for testing Connected Lenses experiences on multiple devices. Send to Connected Spectacles\u200b \"Send to Connected Spectacles\" installs Lenses on devices currently connected to Lens Studio via the connect button. This option is disabled if no Spectacles device is connected through a wireless or wired connection. For instructions on connecting your Spectacles to Lens Studio, refer to the Connect Spectacles Directly documentation. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Connecting Lens Studio to Spectacles Next Publishing Your Spectacles Lens Send to All Paired SpectaclesSend to Connected Spectacles Send to All Paired SpectaclesSend to Connected Spectacles Send to All Paired Spectacles Send to Connected Spectacles AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/get-started/start-building/publishing-lens": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerationsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsGet StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles LensGitHub Sample Projects Get Started Introduction Start BuildingBuild Your First Spectacles LensSpectacles Lens Project SetupPreview Panel for SpectaclesConnecting Lens Studio to SpectaclesTesting Your Lens On SpectaclesPublishing Your Spectacles Lens Start Building Build Your First Spectacles Lens Spectacles Lens Project Setup Preview Panel for Spectacles Connecting Lens Studio to Spectacles Testing Your Lens On Spectacles Publishing Your Spectacles Lens GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Get StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Get StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial DesignGetting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Get StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial Design Get StartedStart BuildingPublishing Your Spectacles LensOn this pageCopy pagePublishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\nWas this page helpful?YesNoPreviousTesting Your Lens On SpectaclesNextIntroduction to Spatial Design  Get Started Get Started Start Building Start Building Publishing Your Spectacles Lens Publishing Your Spectacles Lens On this page Copy page  Copy page     page Publishing Your Spectacles Lens\nTo distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide.\n\nUnlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses.\nRecommendations for designing effective Lens previews:\n\n\nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n\n\nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n\n\nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n\n\nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n\n\nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n\n\nAfter submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer.\nCongratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit.\nGetting Your Lens into Lens Explorer\u200b\nThe Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer:\n\nThe Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions.\nThe Lens follows our design best practices and feels performant (60 fps).\nThe Lens is free of crashes and bugs.\nThe Lens has a custom icon and a preview image.\n\nIf your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit.\nGetting Your Lens Featured\u200b\nThe Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated.\nThe Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love.\nMust-haves\u200b\n\nUser Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive.\nCompleteness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat.\nDesign & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality.\n\nOther considerations\u200b\n\nInnovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box.\nPopularity: The Lens is already proving to be valuable to the Spectacles community.\n Publishing Your Spectacles Lens To distribute Lenses to a wider audience, select the \"Publish\" button to initiate the publishing process. Note that the size limit for Lenses published to Snapchat is 8 MB, whereas Spectacles allows a maximum Lens size of 25 MB. For additional details on publishing Lenses, refer to the provided guide. Unlike Lenses for Mobile, which use 9x16 videos as previews, Lenses for Spectacles use 3x4 images as previews. If a video preview is added in Lens Studio, Spectacles will use a cropped first frame of the video as the preview. It is recommended to add an image preview later in the submission flow within My Lenses. Recommendations for designing effective Lens previews: \nShow the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened.\n Show the function of the Lens. Users viewing the preview in Lens Explorer should understand the Lens's effect when opened. \nInclude people in the frame for connected Lenses. Indicate which Lenses can be used together.\n\nDepict people in the frame wearing Spectacles.\n\n Include people in the frame for connected Lenses. Indicate which Lenses can be used together. Depict people in the frame wearing Spectacles. \nUse bright colors. Note that black appears transparent, and dark colors may be difficult to see.\n Use bright colors. Note that black appears transparent, and dark colors may be difficult to see. \nKeep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant.\n Keep the background simple to focus on the Lens elements. Prefer a blank background unless the location or placement is relevant. \nAvoid text. The tile is small, and Lens and creator names will already be overlaid.\n Avoid text. The tile is small, and Lens and creator names will already be overlaid. After submission, Snap\u2019s moderation team reviews Spectacles Lenses within 2-3 business days to ensure compliance with Snap\u2019s community guidelines. Approved Lenses may then be added to Lens Explorer. Congratulations on publishing your Lens! We encourage you to take a capture and share your Lens with the community on the r/Spectacles subreddit. Getting Your Lens into Lens Explorer\u200b The Spectacles team will review all submitted Lenses and add those that meet the following requirements to Lens Explorer: The Lens has a defined objective. Upon activation, the Lens displays visuals that indicate the required actions. The Lens follows our design best practices and feels performant (60 fps). The Lens is free of crashes and bugs. The Lens has a custom icon and a preview image. If your Lens meets the above requirements and you do not see your Lens in All Lenses within a week of submission, please message the moderators on the r/Spectacles subreddit. Getting Your Lens Featured\u200b The Featured category showcases compelling Lenses that set the bar for quality and demonstrate what is possible on Spectacles. This category will be regularly refreshed as new Lenses are published and get updated. The Spectacles team will curate the selection of featured Lenses based on the below factors, all of which amount to a great experience that users will love. Must-haves\u200b User Experience: The Lens is intuitive to use, easy to navigate, and delivers user value. Interactions feel natural and responsive. Completeness & Depth: The Lens feels complete and provides recurring value. If there is storytelling, there is a beginning, middle and end, and a way to repeat. Design & Polish: The Lens is visually appealing and polished, with attention to detail in both aesthetics and functionality. Other considerations\u200b Innovation: The Lens provides unique value and pushes the bounds of what has been previously done with AR. It inspires other developers to think outside of the box. Popularity: The Lens is already proving to be valuable to the Spectacles community. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Testing Your Lens On Spectacles Next Introduction to Spatial Design Getting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Getting Your Lens into Lens ExplorerGetting Your Lens FeaturedMust-havesOther considerations Getting Your Lens into Lens Explorer Getting Your Lens FeaturedMust-havesOther considerations Must-haves Other considerations AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/introduction-to-spatial-design": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System MenuAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Best PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an InputEmbrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Best PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an Input Best PracticesDesign For SpectaclesIntroduction to Spatial DesignOn this pageCopy pageIntroduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors.Was this page helpful?YesNoPreviousPublishing Your Spectacles LensNextChoosing an Input  Best Practices Best Practices Design For Spectacles Design For Spectacles Introduction to Spatial Design Introduction to Spatial Design On this page Copy page  Copy page     page Introduction to Spatial Design\n\nEmbrace New Capabilities\u200b\nDesigning Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world.\nLenses on Spectacles can:\n\nBe Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world.\nEnhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences.\nInvolve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures.\nInvolve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions.\nBe Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction.\n\nAR in the World\u200b\nAugmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision.\n\nWearable and Personal\u200b\nSpectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch.\nGiven that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users.\nUnique to Spectacles\u200b\nTo design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.\n\n\n\nThe display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see.\nThe focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance.\nEach display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away.\nThe displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content.\n\nUnique to Snap OS\u200b\nSpace on the Hand\u200b\n\nFor Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands.\nStandard System Menu\u200b\nWhile your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors. Introduction to Spatial Design    Embrace New Capabilities\u200b Designing Lenses for Spectacles offers all-new possibilities to rethink user interaction with digital spaces and the physical world. Lenses on Spectacles can: Be Spatial and Unbounded: Design Lenses in 3D, without the constraints of small flat screens, that exist directly in the real world. Enhance the World: Map spaces and track real-world objects using Snap ML to augment the environment and reimagine user experiences. Involve Free Movement: Spectacles are untethered and are bright enough for outdoor use, enabling wearers to freely move around the real world. Design experiences that span various environments, user movements and natural body postures. Involve Natural Interactions: Combine multimodal inputs, such as hands, voice, or a custom mobile controller, to create intuitive and expressive interactions. Be Hands-Free: Users do not need to hold their phones, allowing hands-free interaction with Lenses and the real world. Consider using inputs like voice, object tracking, and body tracking to enable contextual experiences without requiring active digital interaction. AR in the World\u200b Augmented Reality (AR) content positioned in the physical world introduces new challenges. AR content that has been placed in the real world might intersect with real-world obstacles. Users may need to adjust the position of AR content to coordinate around these obstacles. In a six degrees of freedom (6DOF) space, users can move and view content from all angles. This freedom eliminates the constraints of controlled views such as screen space or fixed phone edges. The optimal placement of AR content varies based on the specific needs of the experience, making the positioning of AR content a critical design decision. Wearable and Personal\u200b Spectacles enable Lenses to engage multiple human senses. With Spectacles, users can see via the AR display system, hear through integrated speakers and spatial audio, and interact through hand gestures. Combining visual and audio cues can create the perception of touch. Given that Spectacles are worn by the user, it is necessary to account for varying physical proportions. Differences in arm lengths, hand sizes, and head heights affect the positioning of content. Hand interactions are particularly unique due to the variability in hand and limb sizes. Design for adjustable and adaptive interfaces to accommodate a range of users. Unique to Spectacles\u200b To design comfortable experiences on Spectacles, it\u2019s important to understand the fundamentals of the Spectacles AR display system. Unlike mobile phone screens, the Spectacles display varies with distance and user fit.  The display projects content in a portrait aspect ratio, approximately 3:4 proportions. The term \"field of view\" (FOV) describes the possible area for AR display, indicating the size of the AR content that the Spectacles wearer can see. The focus plane is set at a distance of 1 meter from the user, making it most comfortable to view highly detailed content at approximately this distance. Each display projector creates an image for each eye, which the wearer's brain combines into a single overlapping image. The size of this overlapping area changes based on the distance from the user. The visible display area is smaller when closer to the user and larger when farther away. The displays achieve complete overlap at a distance of 1.1 meters from the user. At this distance, the visible content area is approximately 1000px x 1397px or 53cm x 77cm. For more information, refer to Positioning & Sizing Content. Unique to Snap OS\u200b Space on the Hand\u200b For Snap OS, hands serve as the primary interface. While your Lens is running, the OS persistently renders a small button on the user's hand, providing consistent access to system controls. The rest of the space on the hand is reserved for your Lens to use for actions or immersive effects. Within your Lens, you have control. Utilize the space on the hands for immersive visual effects and simple actions that do not require continuous visibility. For example, the Beat Boxer Lens applies boxing gloves to the user's hands as part of its game mechanics, while the Crickets Lens simulates a bug's life by applying cricket wings to the user's hands. Standard System Menu\u200b While your Lens is running, the system button opens a standard system menu, allowing users to view Lens information, access System Settings, and exit back to Lens Explorer. As a Lens Developer, you do not need to manage these behaviors. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Publishing Your Spectacles Lens Next Choosing an Input Embrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Embrace New CapabilitiesAR in the WorldWearable and PersonalUnique to SpectaclesUnique to Snap OSSpace on the HandStandard System Menu Embrace New Capabilities AR in the World Wearable and Personal Unique to Spectacles Unique to Snap OSSpace on the HandStandard System Menu Space on the Hand Standard System Menu AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/choosing-an-input": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for MovementAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Best PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Best PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement Best PracticesDesign For SpectaclesChoosing an InputCopy pageChoosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting.Was this page helpful?YesNoPreviousIntroduction to Spatial DesignNextDesigning for Movement  Best Practices Best Practices Design For Spectacles Design For Spectacles Choosing an Input Choosing an Input Copy page  Copy page     page Choosing an Input\nWhen designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs.\n\nDifferent input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller.\n\nUse hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together.\nUse voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together.\nUse the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser.\n\nFor hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target.\n\nDirect Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions\nIndirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action.\n\nFor both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting. Choosing an Input When designing an experience for Spectacles, Lens Developers get to choose their desired input method. Snap OS primarily uses hand interactions. Users perform system gestures, such as pointing and pinching, to navigate to your Lens. Within your Lens, decide whether to use hands, voice, or a custom mobile controller as the primary input. Consider device position and real-time object trackers as additional inputs. Different input methods result in distinct experiences. Hand interactions require different design considerations compared to voice interactions or a custom mobile controller. Use hand interactions as the default interaction. Hand interactions can also help with expressive, creative, tactile or assembly actions. Lens examples: Lego Bricktacular, Chess, Loop Lab, Paint Together. Use voice interactions for natural conversations or as a shortcut to a specific action. Lens examples: My AI, Solar System, Imagine Together. Use the mobile controller as a familiar gamepad with world-scale output, or use the phone itself as a physical controller that can be tracked in 6DoF space, or as a keyboard helper for text input. Lens examples: Tiny Motors, Golf, Browser. For hand interactions, the Spectacles Interaction Kit supports both direct (near-field) and indirect (far-field) interactions. Choose the appropriate mode based on your AR experience and goals. No additional developer effort is needed to support either mode. The targeting system automatically switches based on the distance to the intended target. Direct Mode: Uses tangible, immediate interactions. Users reach out and directly intersect the target with their hand. Direct interactions can feel more like real-world actions Indirect Mode: Uses smaller, precise motions, similar to pointing in VR or desktop interfaces, with more distance between the user and the action. For both modes, provide targeting feedbackfor all interactive elements. Style this feedback as needed. Snap OS typically uses a white circular cursor and yellow outline with yellow glow effect to indicate active targeting. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Introduction to Spatial Design Next Designing for Movement AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/designing-for-movement": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainersAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers Best PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing ContentUser PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers Best PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing Content Best PracticesDesign For SpectaclesDesigning for MovementOn this pageCopy pageDesigning for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption.Was this page helpful?YesNoPreviousChoosing an InputNextPositioning & Sizing Content  Best Practices Best Practices Design For Spectacles Design For Spectacles Designing for Movement Designing for Movement On this page Copy page  Copy page     page Designing for Movement\nBecause Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures.\nUser Postures\u200b\nWhen designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls.\n\nStanding\nSeated\nLounging\nOn-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n\n\nAnchor Dynamics\u200b\nDifferent postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers.\nChoosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion.\n\nFixed in the World\u200b\nContent that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size.\n\nTradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position.\nIf specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices.\n\nFollowing the User\u2019s Head\u200b\nFor experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces.\n\nIt is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion.\nSIK Containers offer several following dynamics to choose from.\n\nFollowing the User\u2019s Hand\u200b\nFor experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view.\n\nPosition hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection.\nKeep hand menus small and limited to fewer interfaces and details.\nProvide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface.\n\nContainers\u200b\nSnap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features:\n\nMove, Scale, Rotate\nClose\nChange Anchor Dynamic\nSnap to other Containers for organization and alignment\n\nUse the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption. Designing for Movement Because Spectacles are untethered, lightweight, and see-through, users may wear Spectacles throughout their daily activities while engaging in a variety of physical motions and postures. User Postures\u200b When designing an experience, consider the most likely posture for your use case. In each posture, the user\u2019s physical dimensions, head angle, and reach ranges may vary. Accordingly, each posture might prefer different sizes, orientations or positions for virtual content. Different postures may also encounter different real-world obstacles that require special consideration \u2013 such as desks, tables, soft furniture or walls. Standing Seated Lounging On-the-Go: Users may be walking and moving around. Within On-the-Go, there are two additional considerations:\n\nPortability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion.\nMobility: The user can continually interact with an interface during active movement.\n\n Portability: The user stops their motion to open and interact with an interface. The interface opens in relation to their current position after their motion. Mobility: The user can continually interact with an interface during active movement. Anchor Dynamics\u200b Different postures may require different types of anchor dynamics. Anchor dynamics describe the logic of how content is positioned in a scene. Content can be anchored in the real world with a fixed position, follow the user's head or hand as relative anchors, or dynamically follow a real-world object using Object Trackers. Choosing an anchor dynamic will influence the size of your content, the portability of your experience, and the ease of use while in motion. Fixed in the World\u200b Content that is fixed in the world is flexible, simple, and native to AR. Users can move around this content as if it were a real-world object. Fixed content can also appear larger than life size. Tradeoff: Fixed content may get left behind or appear lost if the user moves away from its initial position. If specific movement needs are not required, choose fixed anchoring as a starting point, as it offers the most flexibility for other design choices. Following the User\u2019s Head\u200b For experiences designed to be used on-the-go and continuously in motion, UI that follows the user is easiest to interact with. This ensures content is never left behind and remains easily accessible. This pattern is particularly useful for critical privacy controls like mic or camera, allowing users to move around without losing access to these important interfaces. It is best to keep the UI smaller and glanceable to avoid blocking the user's full field of view while in motion. SIK Containers offer several following dynamics to choose from. Following the User\u2019s Hand\u200b For experiences requiring significant movement, content can be anchored relative to the user\u2019s hand. Examples like Basketball Trainer and Loop Lab keep controls nearby, but not always visible in the field of view. Hand menus like these maintain content accessibility with user motion, without introducing distractions to the field of view. Position hand menus near the user's non-dominant hand to allow the dominant hand to be used for targeting and selection. Keep hand menus small and limited to fewer interfaces and details. Provide a hint directing users to look at their hands to find hand menus in your experience. Without this hint, users may not think to look at their hands and may miss the intended interface. Containers\u200b Snap OS utilizes Containers as a standard affordance to move and adjust spatial content. Containers use manipulation around a spherical coordinate frame, allowing users to adjust content in different postures, based on their comfort and arm length preferences. Some Containers provide a toggle option to change their anchor dynamic from fixed in the world to following the user, enabling users to switch anchor modes according to their preference. Containers support the following features: Move, Scale, Rotate Close Change Anchor Dynamic Snap to other Containers for organization and alignment Use the Container pattern to give your users flexibility to move content around real world obstacles, to adjust content for personal comfort, or if your experience has multiple panels that need movement and management. Containers support both 3D and 2D media, as well as interactive interfaces. For near-field interfaces with direct interactions, Containers can help users move the interface to a distance that fits their arm\u2019s reach. Container parameters are optional and can be adjusted to suit your experience. Size Containers to fit around your content as if it were a typical interactive element, leaving enough margin to follow target sizing recommendations. Containers are available in the Spectacles Interaction Kit for easy adoption. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Choosing an Input Next Positioning & Sizing Content User PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers User PosturesAnchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s HandContainers User Postures Anchor DynamicsFixed in the WorldFollowing the User\u2019s HeadFollowing the User\u2019s Hand Fixed in the World Following the User\u2019s Head Following the User\u2019s Hand Containers AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/positioning-sizing-content": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & SpacingAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Best PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI DesignOverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Best PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI Design Best PracticesDesign For SpectaclesPositioning & Sizing ContentOn this pageCopy pagePositioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions.Was this page helpful?YesNoPreviousDesigning for MovementNextUI Design  Best Practices Best Practices Design For Spectacles Design For Spectacles Positioning & Sizing Content Positioning & Sizing Content On this page Copy page  Copy page     page Positioning & Sizing Content\nOverview\u200b\nIn spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components.\nContent positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience.\n\nZ Axis: Near-Field or Far-Field\u200b\nSome use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position:\nNear-Field (35cm - 60cm):\n\nEnables direct interactions within typical human arm reach ranges\nSmaller usable display area\nBest for brief navigation and glanceable nearby controls\nBest to limit text and visual detail\nRefer to Visual Comfort for more detail\n\nFar-Field (60cm - 160cm):\n\nUses indirect interactions\nLarger usable display area\nBest for larger and more detailed content experiences that might be used for longer time periods\n\n\nX Axis & Binocular Overlap\u200b\nFor horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes:\n\nAt Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls\nAt Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls\nAt Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default\nAt Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed\n\nFor the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.\n\nY Axis & Boresight\u200b\nFor vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop.\n\nCenter in FOV\u200b\nIn any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions.\nKeep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect.\nUsers new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.\n\nButton Sizing & Spacing\u200b\nSize all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines:\n\nTargetable elements should be at least 2 degrees in angular size.\nTargetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback.\n\nPosition Z = 35cmPosition Z = 55cmPosition Z = 110cmPosition Z = 160cmMinimum Size1.5cm2.0cm4.0cm5.5cmBest2.0cm3.0cm6.0cm8.5cmLarge2.5cm4.0cm8.0cm11.5cm\n\nFor more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions. Positioning & Sizing Content Overview\u200b In spatial design, consider positioning and sizing content along three axes: X, Y, and Z. In Lens Studio, the Spectacles Camera (0,0,0) represents the origin of the scene and the position of Spectacles on the wearer's head. Positioning is not always absolute, as Spectacles device fit can vary among users, causing them to perceive distances slightly differently. Users may also prefer to adjust the position of content to fit their proportions more comfortably by using a Container or other Manipulate components. Content positioning should be guided by your use case, expected user motion, and desired interaction style. Sizing is constrained by both the display size at a given Z position and the minimums recommended for hand interactions. The Z position affects the available X and Y display size. Together, these choices will shape the comfort, usability, and overall feel of your experience. Z Axis: Near-Field or Far-Field\u200b Some use cases require launching and positioning content closer to the user, while others may prefer positioning further away at the full-width convergence plane. UI elements might need to be closer than larger experiential world content. Note that the overlapping Spectacles display area becomes smaller the closer it is to the user in the Z position. Consider the following tradeoffs when deciding on a position: Near-Field (35cm - 60cm): Enables direct interactions within typical human arm reach ranges Smaller usable display area Best for brief navigation and glanceable nearby controls Best to limit text and visual detail Refer to Visual Comfort for more detail Far-Field (60cm - 160cm): Uses indirect interactions Larger usable display area Best for larger and more detailed content experiences that might be used for longer time periods X Axis & Binocular Overlap\u200b For horizontal positioning and layout, the usable display width varies based on the Z position from the user. Use the following typical Z-distances and their corresponding display sizes: At Z=35cm: Width X=13cm, Height Y=24cm \u2013 near field, hand anchored, quick controls At Z=55cm: Width X=23cm, Height Y=38cm \u2013 arm's reach for near field, quick controls At Z=110cm: Width X=53cm, Height Y=77cm \u2013 mid field, best default At Z=160cm: Width X=75cm, Height Y=112cm \u2013 far field, ultra-large if needed For the most comfortable experience, keep your content within these recommended horizontal widths. This ensures your content remains within the binocular overlap width in any given XY plane. Avoid placing content in the monocular edges of the field of view, where only one eye can see the content, and where image quality may be lower.  Y Axis & Boresight\u200b For vertical positioning, consider Spectacles\u2019 boresight angle rather than the horizon in Lens Studio. Unique to Spectacles, the display\u2019s boresight is tilted downward to support a more neutral resting head pose. Consequently, the centerline of the display image is rotated below the horizon. This requires positioning content lower in the global coordinate space compared to publishing on a mobile device or desktop. Center in FOV\u200b In any given XY plane, center content in the user\u2019s field of view. This positioning provides the most comfortable and neutral head position for viewing the Lens experience, requiring minimal head movement and ensuring users see the intended content. It also avoids potential color distortion near the edges of the display. Content positioned lower in the field of view is easier to target with hand interactions. Keep content sized to fit within the field of view at any given XY plane. This approach feels more deliberate and user-friendly, making the display edges less noticeable. Content that does not fit within the field of view will feel larger than life and may be disorienting. Use this approach only if you intend to create a dramatic scale effect. Users new to AR may not realize they can move their head to look around a Lens. If you place content outside of the field of view, users may not find it unless you specifically guide them to that content.  Button Sizing & Spacing\u200b Size all interactive elements to support comfortable and usable hand interactions. Small buttons are difficult to target and use with hand interactions. Snap OS uses angular sizes as a distance-independent standard, which can be converted to real-world centimeters at different Z-positions. The recommended size refers to the interactive collider on an element. Visual meshes can be smaller, provided the collider meets the minimum standards for comfortable interactions. Follow these sizing guidelines: Targetable elements should be at least 2 degrees in angular size. Targetable elements should be spaced at least 1 degree apart in angular size. Spacing may also need to accommodate the size of any targeting feedback. For more information, refer to Resources for a recommended sizing template. This FBX can be imported into Lens Studio or any 3D software, as a reference for recommended display sizes and target sizing at different Z-positions. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Designing for Movement Next UI Design OverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing OverviewZ Axis: Near-Field or Far-FieldX Axis & Binocular OverlapY Axis & BoresightCenter in FOVButton Sizing & Spacing Overview Z Axis: Near-Field or Far-Field X Axis & Binocular Overlap Y Axis & Boresight Center in FOV Button Sizing & Spacing AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/ui-design": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial AudioAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Best PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best PracticesForm & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Best PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best Practices Best PracticesDesign For SpectaclesUI DesignOn this pageCopy pageUI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\nWas this page helpful?YesNoPreviousPositioning & Sizing ContentNextDesign Best Practices  Best Practices Best Practices Design For Spectacles Design For Spectacles UI Design UI Design On this page Copy page  Copy page     page UI Design\nForm & Shape\u200b\nForm exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes.\nUsing 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects.\nFor legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements.\nPerformance Considerations:\n\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\nBlendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device.\nOptimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips.\n\nColor & Material\u200b\nAdditive Color Space\u200b\nThe Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces.\n\nDesign for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome.\nFor visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.\n\nMaterials\u200b\nConsider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance.\n\nFor efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps.\nAvoid gradients with opacity to minimize texture load and graphical artifacts.\nUse black/white masks for occlusion effects instead of more resource-intensive occlusion shaders.\n\nMotion\u200b\nUse motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users.\n\nMotion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position.\nMotion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language.\n\nExamples:\n\nSnap OS palm UI jiggles and flexes to hint at a poke action.\nSnap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation.\nLens Explorer tiles move forward in z-depth to indicate they have been targeted.\nVideo Calling presence expands upwards to guide the user\u2019s attention.\n\nSpatial Audio\u200b\nSpatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d.\nSpatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view.\n\n\n\nLearn More:\n\n\nAudio in Spectacles\n\n\nLens Studio Spatial Audio Component\n\n UI Design Form & Shape\u200b Form exists in three dimensions: height, width, and depth. Spectacles interfaces work best with a mix of 3D forms and 2D shapes. Using 3D forms encourages hand interactions. Round geometry is more inviting to touch than flat planes with sharp edges. 3D forms provide an inherent back and some mass when viewed from different sides in 6DoF world space. They also offer more compelling interaction feedback with new motion and depth opportunities. Use 3D forms for buttons, interactive elements, and hero objects. For legibility, use 2D shapes for text and icons to avoid extra edges that can be distracting from different angles. 2D planes and meshes can also serve as backgrounds and tiles to gather multiple 3D elements. Performance Considerations: Consider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance. Blendshapes and particle systems are power-intensive. Consider creative alternatives to simulate complex visual effects without overburdening the device. Optimize performance by combining meshes and using JPG masks instead of PNGs. Explore the Performance Guide for more tips. Color & Material\u200b Additive Color Space\u200b The Spectacles display system influences the appearance of colors on the device. The system uses red, green, and blue light to create AR overlays on top of the real-world view. This additive color space renders colors differently than desktop or mobile color spaces. Design for Spectacles in an \"inverted color space,\" where white is the boldest, brightest color. Alpha is proportional to luminosity; 50% alpha will be half as bright and appear as a midtone. The additive display cannot render pure black tones, which will appear transparent. Consider patterns from \"dark mode\" on mobile or desktop for inspiration. Test colors on Spectacles and iteratively adjust values to achieve the intended outcome. For visual comfort, avoid pure white backgrounds with dark text, as this is less comfortable to read than dark backgrounds with lighter text. Uniform color regions may highlight display variations and non-uniform color rendering. Use patterns or subtle background textures to minimize these display variations.  Materials\u200b Consider the tradeoffs for your experience to balance power consumption, runtime, and visual appearance. For efficient performance, use unlit materials with baked-in lighting and simple shaders to mimic PBR effects. This approach reduces the computational overhead of detailed environment maps. Avoid gradients with opacity to minimize texture load and graphical artifacts. Use black/white masks for occlusion effects instead of more resource-intensive occlusion shaders. Motion\u200b Use motion design to guide the user\u2019s attention from one point to another. In 3D space, this is particularly useful for wayfinding and orienting users. Motion can cue an origin point and ease transitions between dynamic elements. This is especially helpful when opening world content from the hand or transitioning content from smaller menus to larger positions in the world. Use motion to direct the user\u2019s attention to a new position. Motion can also provide interaction hints as feedforward and feedback, offering moments of delight and an opportunity to express a particular stylistic language. Examples: Snap OS palm UI jiggles and flexes to hint at a poke action. Snap OS pinch buttons hint at their pinch action with a progressive \u201csquish\u201d animation. Lens Explorer tiles move forward in z-depth to indicate they have been targeted. Video Calling presence expands upwards to guide the user\u2019s attention. Spatial Audio\u200b Spatial audio offers new creative opportunities for AR experiences. Sound sources can be positioned in the real-world scene with realistic distance and directional behavior relative to the user\u2019s movement through the scene, allowing the user to \u201chear what they see\u201d. Spatial audio enhances realism and immersion, making the digital scene feel more plausible, as if it were part of real life. It can also serve as a directional cue to grab the user\u2019s attention and signal where content may appear from a specific direction, such as outside the field of view. Learn More: \nAudio in Spectacles\n Audio in Spectacles \nLens Studio Spatial Audio Component\n Lens Studio Spatial Audio Component Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Positioning & Sizing Content Next Design Best Practices Form & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Form & ShapeColor & MaterialAdditive Color SpaceMaterialsMotionSpatial Audio Form & Shape Color & MaterialAdditive Color SpaceMaterials Additive Color Space Materials Motion Spatial Audio AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/design-for-spectacles/design-best-practices": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResourcesAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best PracticesPerformance OptimizationProfiling & Debugging Best Practices Design For SpectaclesIntroduction to Spatial DesignChoosing an InputDesigning for MovementPositioning & Sizing ContentUI DesignDesign Best Practices Design For Spectacles Introduction to Spatial Design Choosing an Input Designing for Movement Positioning & Sizing Content UI Design Design Best Practices Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources Best PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance OptimizationUX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources Best PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance Optimization Best PracticesDesign For SpectaclesDesign Best PracticesOn this pageCopy pageDesign Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\nWas this page helpful?YesNoPreviousUI DesignNextPerformance Optimization  Best Practices Best Practices Design For Spectacles Design For Spectacles Design Best Practices Design Best Practices On this page Copy page  Copy page     page Design Best Practices\nUX Recommendations\u200b\nAvoid common issues by following these best practices:\n\nConsider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI.\nGuide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view.\nExplain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users.\nShow Targeting Feedback: Ensure all interactive elements display targeting feedback.\nFollow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements.\nDesign for Visual Comfort: Follow visual comfort guidelines to improve the user experience.\n\nDesigning for Comfort\u200b\nVisual Comfort\u200b\n\nAvoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section\nOptimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user.\nSingle Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other.\nConsistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort.\nCenter Content: Center content in the field of view to minimize color variation near the display edges.\nAvoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section.\nLegible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials.\nConsider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress.\n\nPhysical Comfort\u200b\n\nReduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting.\nMinimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart.\nDesign Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose.\nAvoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up.\n\nCognitive Comfort\u200b\nDesign for Reduced Cognitive Load and Enhanced Understanding.\n\nMatch Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system.\nRecognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory.\nProvide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction.\nAvoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user.\n\nResources\u200b\n\nRecommended Display & Button Sizing: SizeTemplate.fbx\n Design Best Practices UX Recommendations\u200b Avoid common issues by following these best practices: Consider User Movement and Postures: Account for different postures and movements during your experience. Provide adjustable controls where possible and ensure users can recall or summon any critical UI. Guide Users for Out-of-View Elements: Provide hints to look down or around to find hand menus, elements on the floor, or elements outside the field of view. Explain Divergent Interactions: Clearly explain any interactions that differ from typical system patterns. Only use diverging patterns if they add value to the experience. Leverage system patterns as much as possible to reduce learning curves for users. Show Targeting Feedback: Ensure all interactive elements display targeting feedback. Follow Target Sizing Guidelines: Adhere to the recommended target sizing for interactive elements. Design for Visual Comfort: Follow visual comfort guidelines to improve the user experience. Designing for Comfort\u200b Visual Comfort\u200b Avoid Non-Overlapping Monocular Side Regions: Avoid positioning content in the non-overlapping, monocular side regions of the display. Review the X Axis & Binocular Overlap section Optimal Viewing Distance: Detailed content is most comfortable to view at approximately 1 meter away in Z-depth. Avoid prolonged viewing of detailed content close to the user. Single Focus: Do not require users to simultaneously focus on both virtual and real objects in two very different depths.Allow focus on one depthor the other. Consistent Depth Cues: Keep depth cues between the real world and virtual content consistent with reality. Avoid setting high render order in Lens Studio for virtual objects positioned far from the user. This digital override can cause far-field virtual content to appear closer than near-field virtual content. This mismatch conflicts with real world depth cues and may cause visual discomfort. Center Content: Center content in the field of view to minimize color variation near the display edges. Avoid Dark Content with Pure White Backgrounds: This combination can cause visual discomfort. Review the Color & Material section. Legible Text: Provide a background behind text to maintain legibility against varying real-world colors and other virtual objects. Possible solutions include gradient color panels or occluding materials. Consider User Breaks: Some users may take periodic breaks from wearing Spectacles. Consider self-paced experiences and content that allows for pausing or saving progress. Physical Comfort\u200b Reduce Arm Effort: Position elements lower in the Y position to reduce the arm effort required for hand targeting. Minimize Travel Distance: For lower effort hand targeting, plan layouts to reduce travel distance between interactions. Avoid sequential user actions that are far apart. Design Custom Hand Gestures for Comfort: If you\u2019re creating custom hand interactions, , design for neutral hand and wrist positions, in line with natural resting poses. Avoid awkward grasping or extreme angles. Allow a range of acceptable poses to provide greater user flexibility, rather than expecting users to perform a single precise semantic pose. Avoid Steep Neck Angles: Position content neutrally and centered in the field of view to avoid steep, sustained neck angles, whether dramatically down or up. Cognitive Comfort\u200b Design for Reduced Cognitive Load and Enhanced Understanding. Match Real-World Conventions and Human Behavior: Design interfaces and experiences that mirror real-world conventions and human behavior. This makes them intuitive and easy to navigate, leveraging users' pre-existing knowledge and reducing the cognitive load required to interact with a new system. Recognition Over Recall: Design systems that minimize the need for users to recall information from memory. Make options visible so users can recognize what they need from available cues, which is less cognitively demanding than recalling options from memory. Provide a First-Run Experience: Implement guided introductions, tutorials, or tooltips for first-time users. This helps users understand how to navigate and use the system efficiently, building a solid foundation, reducing frustration, and enhancing user satisfaction. Avoid Overwhelming the User: Be mindful of completely filling the user\u2019s field of view. Size content to have margins on all sides. Avoid asking users to navigate across multiple windows; navigating more than three windows at once can feel overwhelming. To this end, avoid lists longer than seven elements, as this also may overwhelm the user. Resources\u200b Recommended Display & Button Sizing: SizeTemplate.fbx Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous UI Design Next Performance Optimization UX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources UX RecommendationsDesigning for ComfortVisual ComfortPhysical ComfortCognitive ComfortResources UX Recommendations Designing for ComfortVisual ComfortPhysical ComfortCognitive Comfort Visual Comfort Physical Comfort Cognitive Comfort Resources AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/performance-optimization": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional ConsiderationsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Best PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance OverlayOptimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Best PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance Overlay Best PracticesPerformance OptimizationOn this pageCopy pagePerformance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence.Was this page helpful?YesNoPreviousDesign Best PracticesNextLens Performance Overlay  Best Practices Best Practices Performance Optimization Performance Optimization On this page Copy page  Copy page     page Performance Optimization\nOptimizing Lens Performance for Spectacles\u200b\nIf your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them.\nLens Developer Sandbox\u200b\nWhen Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization.\nIdentifying the Need for Optimization\u200b\nFirst, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel.\n\n\nSetup for Performance Overlay\n\n\nSetup for Spectacles Monitor Panel\n\n\nWith your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly.\nPinpointing Unoptimized Elements\u200b\nWhile running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations.\nIf your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed.\nRendering Optimizations\u200b\nRendering optimizations focus on reducing the quantity and quality of rendered elements to save power.\n\n\nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n\n\nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n\n\nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n\n\nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n\n\nScripting Optimizations\u200b\nFor CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following:\n\n\nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n\n\nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n\n\nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n\n\nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n\n\nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n\n\nAdditional Considerations\u200b\nWhile the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence. Performance Optimization Optimizing Lens Performance for Spectacles\u200b If your lens is running slowly or dropping frames, it may need optimization. This document provides guidance for optimizing lenses specifically for Spectacles. Note that while general optimization guidelines from Snap are available here, some specifics for Spectacles may contradict these general guidelines. Always verify optimizations after making them. Lens Developer Sandbox\u200b When Lenses consume excessive power and generate high thermal output, Spectacles throttles performance elements such as rendering and FPS to maintain a consistent runtime experience across various Lens workloads. This feature provides guardrails to quickly indicate when Lenses require optimization. Identifying the Need for Optimization\u200b First, check your lens performance metrics using either the on-device Performance Overlay or the Lens Studio Spectacles Monitor Panel. \nSetup for Performance Overlay\n Setup for Performance Overlay \nSetup for Spectacles Monitor Panel\n Setup for Spectacles Monitor Panel With your lens running on the device, observe Lens Power (LP) on the overlay or Power Usage in the monitor panel. Both readings measure overall power on a scale from 0 to 100, where \"0\" represents an empty lens and \"100\" represents the maximum recommended power for smooth performance. Ideally, your lens consumes the least amount of power necessary to achieve your goals. If the reading exceeds 100, your lens will perform poorly. Pinpointing Unoptimized Elements\u200b While running the above setup, position your device to face various elements of your lens and observe the power measurements. Compare these to measurements taken when not looking at that element. If power increases significantly with a specific element in view, that element likely needs rendering optimizations. If your lens runs high power regardless of the view or chugs when engaging specific logic, scripting optimizations are likely needed. Rendering Optimizations\u200b Rendering optimizations focus on reducing the quantity and quality of rendered elements to save power. \nDisable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again.\n Disable Invisible or Unused Meshes: Even invisible meshes render. Disable or move them out of the frustum until necessary. Destroy them if they won't be used again. \nReduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes.\n Reduce Draw Calls and Materials: Use instancing for drawing the same mesh multiple times and split textures into atlases to use one material with different UVs for texturing different meshes. \nAvoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps.\n Avoid Expensive Materials and Lighting: Avoid PBR materials, reduce lights, and avoid special graphics effects like reflections, shadows, and environment maps. \nReduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled.\n Reduce Complexity: Minimize vertices in meshes, calculations in shaders, number and size of textures (recommended size: 512px x 512px or smaller), and consider turning off MSAA if enabled. Scripting Optimizations\u200b For CPU optimization, run a Perfetto Trace and analyze functions that take significant time to execute. Optimize these functions by considering the following: \nLimit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them.\n Limit UpdateEvents: Only use UpdateEvents when necessary and avoid expensive operations within them. \nMinimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources.\n Minimize Resource Creation: Create only the necessary number of Events, SceneObjects, Assets, and Components. Reuse existing resources. \nReduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations.\n Reduce Physics Usage: Add PhysicsBodies only to necessary SceneObjects and perform broad-phase checks before complex calculations. \nOptimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS.\n Optimize Calculations: Precompute cacheable values, offload calculations to the GPU if possible, and avoid complex trigonometry and large dataset operations in JS/TS. \nManage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds.\n Manage Data and Logs Efficiently: Avoid repeatedly saving large data and excessive logging. Use WAV files when you can instead of mp3 for sounds on Spectacles and limit the number of simultaneous sounds. Additional Considerations\u200b While the above tips and techniques help optimize many lenses, no single method applies to every lens. Optimization requires examination and analysis. Consider these recommendations holistically and explore similar or parallel opportunities if these techniques do not yield the desired results. Optimization rewards persistence. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Design Best Practices Next Lens Performance Overlay Optimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Optimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized ElementsRendering OptimizationsScripting OptimizationsAdditional Considerations Optimizing Lens Performance for SpectaclesLens Developer SandboxIdentifying the Need for OptimizationPinpointing Unoptimized Elements Lens Developer Sandbox Identifying the Need for Optimization Pinpointing Unoptimized Elements Rendering Optimizations Scripting Optimizations Additional Considerations AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/profiling/lens-performance-overlay": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption)AI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles Monitor Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & DebuggingLens Performance OverlaySpectacles Monitor Profiling & Debugging Lens Performance Overlay Spectacles Monitor Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Best PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles MonitorOverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Best PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles Monitor Best PracticesProfiling & DebuggingLens Performance OverlayOn this pageCopy pageLens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100.Was this page helpful?YesNoPreviousPerformance OptimizationNextSpectacles Monitor  Best Practices Best Practices Profiling & Debugging Profiling & Debugging Lens Performance Overlay Lens Performance Overlay On this page Copy page  Copy page     page Lens Performance Overlay\nOverview\u200b\nThe Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses.\nSpectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay.\nFor more information, refer to Spectacles Monitor.\nEnabling Lens Performance Overlay\u200b\nEnable the Lens Performance Overlay through the Spectacles App under Developer Settings.\n\nPerformance Data Types\u200b\nThe Lens Performance Overlay provides three types of data for the Lens you have built:\n\nFPS (Frames Per Second)\nLP (Lens Power Consumption)\nTP (Tracking Power Consumption)\n\n\nFPS\u200b\nFPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS.\nLP (Lens Power Consumption)\u200b\nLP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100.\nLens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization.\nTP (Tracking Power Consumption)\u200b\nTP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100. Lens Performance Overlay Overview\u200b The Lens Performance Overlay allows you to view performance data in real time overlaid on the Spectacles display to monitor the efficiency of your draft Lenses. Spectacles Monitor provides similar functionality in Lens Studio. The Lens Profiler Overlay consumes some Lens Power, resulting in a lower number in the Overlay compared to Spectacles Monitor when both are active. To get more precise numbers in Spectacles Monitor, turn off the Profiler Overlay. For more information, refer to Spectacles Monitor. Enabling Lens Performance Overlay\u200b Enable the Lens Performance Overlay through the Spectacles App under Developer Settings. Performance Data Types\u200b The Lens Performance Overlay provides three types of data for the Lens you have built: FPS (Frames Per Second) LP (Lens Power Consumption) TP (Tracking Power Consumption) FPS\u200b FPS, or Frames Per Second, measures how many individual images (or frames) the Spectacles display renders per second. To ensure your Lens feels seamless to users, it should run close to 60 FPS. LP (Lens Power Consumption)\u200b LP, or Lens Power, measures the current power consumption of your Lens, ranging from 0 to 100. It is recommended to keep this value below 100. Lens Power and FPS are interconnected; if the Lens continuously uses too much power, FPS will be throttled. For more information, refer to the Performance Optimization. TP (Tracking Power Consumption)\u200b TP, or Tracking Power, measures the power consumption of the entire system for tracking purposes, such as World Tracking and Hand Tracking. This value ranges from 0 to 100. It is recommended to keep this value below 100. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Performance Optimization Next Spectacles Monitor OverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) OverviewEnabling Lens Performance OverlayPerformance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) Overview Enabling Lens Performance Overlay Performance Data TypesFPSLP (Lens Power Consumption)TP (Tracking Power Consumption) FPS LP (Lens Power Consumption) TP (Tracking Power Consumption) AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/best-practices/profiling/spectacles-monitor": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfettoAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfetto Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfetto Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsBest PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfetto Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles MonitorSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingLens Performance OverlaySpectacles Monitor Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & DebuggingLens Performance OverlaySpectacles Monitor Profiling & Debugging Lens Performance Overlay Spectacles Monitor Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Best PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfetto Best PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverviewMeasuring Power and ThermalsPerfetto Best PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverview Best PracticesProfiling & DebuggingSpectacles MonitorOn this pageCopy pageSpectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation.Was this page helpful?YesNoPreviousLens Performance OverlayNextOverview  Best Practices Best Practices Profiling & Debugging Profiling & Debugging Spectacles Monitor Spectacles Monitor On this page Copy page  Copy page     page Spectacles Monitor\nSpectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization.\nTo launch the monitor go to Window -> Utilities -> Spectacles Monitor.\n\nMeasuring Power and Thermals\u200b\nLens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device.\nFor monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage.\n(You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature)\n\nOnce you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs.\n(If the lens information does not appear, try restarting your device).\n\nYou can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan.\n\nFinally, you can clear both graphs by clicking on the broom icon on the top right of the monitor.\nPerfetto\u200b\nPerfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues.\nTo begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine.\nTo stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine.\n\nFor more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation. Spectacles Monitor Spectacles Monitor is a tool designed to help you identify areas for performance improvement in your Lens. For detailed guidance on utilizing performance data, refer to the Performance Optimization. To launch the monitor go to Window -> Utilities -> Spectacles Monitor. Measuring Power and Thermals\u200b Lens Studio provides a Spectacles Monitor window for assessing the power and thermal performance of your Lenses on the Spectacles device. For monitoring to begin, Spectacles must first be connected to Lens Studio. Once connected, baseline values for power and thermal will be displayed on the monitor: these include the Logic Touch (left) and Tracker Touch (right) temperatures, and the Tracker power usage. (You can also open more than one Spectacles Monitor if you\u2019d like to have one focus only on power and the other only on temperature) Once you start a lens by sending it to Spectacles, an additional entry named Lens will appear under Power Usage, along with an additional line representing your lens on the power graph. You can use this entry to monitor the specific power usage of your lens. To monitor the thermal impact of your lens, observe the changes in temperature of the Logic Touch and Tracker Touch as your lens runs. (If the lens information does not appear, try restarting your device). You can analyze the trends on the graph by pausing the monitor using the pause button. Once paused, click and drag to show the average temperature and power in the selected timespan. Finally, you can clear both graphs by clicking on the broom icon on the top right of the monitor. Perfetto\u200b Perfetto is an open-source project that provides performance tracing and visualization tools compatible with Spectacles. It enables detailed analysis of system and application performance by capturing and displaying trace events in a graphical interface. Perfetto is used for identifying performance bottlenecks and debugging complex issues. To begin a Perfetto Trace, ensure that Spectacles are connected to Lens Studio and in an active session with a Draft Lens. Refer to the Connection Guide for instructions. Once connected and the Draft Lens is open, select \"Start Spectacles Profiling\" to initiate the Perfetto Trace. You can specify where the Perfetto Trace data will be stored on your local machine. To stop the Perfetto Trace, select \"Stop Spectacles Profiling\". If the Perfetto Trace is successful, the Perfetto GUI will appear directly in the Spectacles Monitor. Additionally, you can view the Perfetto Trace by opening the exported static HTML file on your local machine. For more information and guidance on navigating through Perfetto, visit the main website: Perfetto Documentation. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Lens Performance Overlay Next Overview Measuring Power and ThermalsPerfetto Measuring Power and ThermalsPerfetto Measuring Power and Thermals Perfetto AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/overview": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query ModuleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Spectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera ModuleGesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Spectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera Module Spectacles FeaturesOverviewOn this pageCopy pageOverview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\nWas this page helpful?YesNoPreviousSpectacles MonitorNextCamera Module  Spectacles Features Spectacles Features Overview Overview On this page Copy page  Copy page     page Overview\nWelcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers.\nGesture Module\u200b\n\nThe Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods.\nCamera Module\u200b\n\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nMotion Controller\u200b\n\nDevelopers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App.\nPerform HTTP Request\u200b\n\nSpectacles offers APIs to access the internet so you can access external APIs, download media, and more.\nWorld Query Module\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously.\n Overview Welcome to the Spectacles Development documentation. This overview provides a detailed summary of the various modules and features available to developers. Gesture Module\u200b The Gesture Module leverages a machine learning approach to offer more reliable and accurate gesture detection compared to heuristic methods. Camera Module\u200b Spectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment. Motion Controller\u200b Developers for Spectacles are able to build custom mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app \u2013 Spectacles App. Perform HTTP Request\u200b Spectacles offers APIs to access the internet so you can access external APIs, download media, and more. World Query Module\u200b When developing lenses for Spectacles, you may need to place an object on a surface instantaneously. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Spectacles Monitor Next Camera Module Gesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Gesture ModuleCamera ModuleMotion ControllerPerform HTTP RequestWorld Query Module Gesture Module Camera Module Motion Controller Perform HTTP Request World Query Module AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/camera-module": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Spectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom LocationsRequesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Spectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom Locations Spectacles FeaturesAPIsCamera ModuleOn this pageCopy pageCamera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });});Was this page helpful?YesNoPreviousOverviewNextCustom Locations  Spectacles Features Spectacles Features APIs APIs Camera Module Camera Module On this page Copy page  Copy page     page Camera Module\nSpectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment.\nThis is an Experimental API. Please see Experimental APIs for more details.\nAccessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.\nRequesting + Receiving Frames\u200b\nThe CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing.\nCamera Frame Requests\u200b\nTo begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return).\nFor example:\nlet cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest);\ncreateCameraRequest may not be invoked inside onAwake event.\nThe requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule.\nTo receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below:\nlet onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration));\nStill Image Requests\u200b\nTo request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed.\nlet cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);}\nGetting Camera Information\u200b\nThe Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem.\n// Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose;\nAdditionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa.\nCode Example\u200b\nIn Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector.\n\nTypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); Camera Module Spectacles offers APIs to retrieve the camera frame \u2013 what the user is currently seeing \u2013 to better understand and build experiences around the user\u2019s real-world environment. This is an Experimental API. Please see Experimental APIs for more details. Accessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information.   Accessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information. Accessing the camera frame in a Lens will disable open internet access for that Lens. The camera frame contains user information information that may not be transmitted outside the Lens. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that Lenses built this way may not be released publicly. Please see Extended Permissions for more information. Requesting + Receiving Frames\u200b The CameraModule enables developers to request the camera frame. There are two types of frame requests: camera frame requests and still image requests. Camera frame requests deliver a steady stream of images, albeit at lower quality: this approach is recommended for applications that need video-like consistency in frame delivery. Still image requests, on the other hand, take longer to complete but result in a higher resolution image -- this is the preferred approach for applications that need high quality images for processing. Camera Frame Requests\u200b To begin receiving camera frames, construct a CameraRequest, specifying the exact camera of interest (Spectacles have multiple cameras from which to choose: Left_Color, Right_Color, or Default_Color) and the desired resolution of the received frames (via imageSmallerDimension, which indicates the size in pixels of the smallest dimension to return). For example: let cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest); let cameraModule = require('LensStudio:CameraModule');let cameraRequest = CameraModule.createCameraRequest();cameraRequest.id = CameraModule.CameraId.Left_Color;let cameraTexture = cameraModule.requestCamera(cameraRequest); let cameraModule = require('LensStudio:CameraModule'); let  cameraModule  =   require ( 'LensStudio:CameraModule' ) ;    let cameraRequest = CameraModule.createCameraRequest();  let  cameraRequest  =   CameraModule . createCameraRequest ( ) ;  cameraRequest.id = CameraModule.CameraId.Left_Color; cameraRequest . id   =   CameraModule . CameraId . Left_Color ;    let cameraTexture = cameraModule.requestCamera(cameraRequest);  let  cameraTexture  =  cameraModule . requestCamera ( cameraRequest ) ;   createCameraRequest may not be invoked inside onAwake event.   createCameraRequest may not be invoked inside onAwake event. createCameraRequest may not be invoked inside onAwake event. The requestCamera method takes a CameraRequest and returns a Texture. The texture contains the opaque handle that refers to the underlying camera data, and can be used as an input into an MLComponent or uploaded to a remote service via RemoteMediaModule. To receive a notification each time the frame is updated, use the CameraTextureProvider which can be accessed via cameraTexture.control. The CameraTextureProvider has an onNewFrame event that can be utilized as shown below: let onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration)); let onNewFrame = cameraTexture.control.onNewFrame;let registration = onNewFrame.add((frame) => {  // Process the frame});script.onStop.add(() => onNewFrame.remove(registration)); let onNewFrame = cameraTexture.control.onNewFrame; let  onNewFrame  =  cameraTexture . control . onNewFrame ;  let registration = onNewFrame.add((frame) => {  let  registration  =  onNewFrame . add ( ( frame )   =>   {    // Process the frame    // Process the frame  });  } ) ;    script.onStop.add(() => onNewFrame.remove(registration)); script . onStop . add ( ( )   =>  onNewFrame . remove ( registration ) ) ;   Still Image Requests\u200b To request still image frames, create an ImageRequest from the CameraModule, then invoke requestImage. The requestImage method is asynchronous and will return an ImageFrame when complete, which contains the resulting image in a Texture, as well as a timestamp if needed. let cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);} let cameraModule = require('LensStudio:CameraModule');let imageRequest = CameraModule.createImageRequest();try {  let imageFrame = await cameraModule.requestImage(imageRequest);  // E.g, use the texture in some visual  script.image.mainPass.baseTex = imageFrame.texture;  let timestamp = imageFrame.timestampMillis; // scene-relative time} catch (error) {  print(`Still image request failed: ${error}`);} let cameraModule = require('LensStudio:CameraModule'); let  cameraModule  =   require ( 'LensStudio:CameraModule' ) ;  let imageRequest = CameraModule.createImageRequest();  let  imageRequest  =   CameraModule . createImageRequest ( ) ;    try {  try   {    let imageFrame = await cameraModule.requestImage(imageRequest);    let  imageFrame  =   await  cameraModule . requestImage ( imageRequest ) ;      // E.g, use the texture in some visual    // E.g, use the texture in some visual    script.image.mainPass.baseTex = imageFrame.texture;   script . image . mainPass . baseTex   =  imageFrame . texture ;    let timestamp = imageFrame.timestampMillis; // scene-relative time    let  timestamp  =  imageFrame . timestampMillis ;   // scene-relative time  } catch (error) {  }   catch   ( error )   {    print(`Still image request failed: ${error}`);    print ( ` Still image request failed:  ${ error } ` ) ;  }  }   Getting Camera Information\u200b The Camera API also provides information about the cameras used on Spectacles in order to understand how the camera transforms 3D space into 2D space. This information is provided by the DeviceCamera which is accessible via the DeviceInfoSystem. // Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose; // Select the cameralet camera = global.deviceInfoSystem.getTrackingCameraForId(  CameraModule.CameraId.Left_Color);// Retrieve camera propertieslet focalLength = camera.focalLength;let principalPoint = camera.principalPoint;let resolution = camera.resolution;let pose = camera.pose; // Select the camera // Select the camera  let camera = global.deviceInfoSystem.getTrackingCameraForId(  let  camera  =  global . deviceInfoSystem . getTrackingCameraForId (    CameraModule.CameraId.Left_Color    CameraModule . CameraId . Left_Color  );  ) ;    // Retrieve camera properties  // Retrieve camera properties  let focalLength = camera.focalLength;  let  focalLength  =  camera . focalLength ;  let principalPoint = camera.principalPoint;  let  principalPoint  =  camera . principalPoint ;  let resolution = camera.resolution;  let  resolution  =  camera . resolution ;  let pose = camera.pose;  let  pose  =  camera . pose ;   Additionally, DeviceCamera includes project and unproject methods that makes it easier to convert 3D points into 2D points and vice versa. Code Example\u200b In Lens Studio click \"+\" in the Asset Browser window, select \"General\" and create a new TypeScript or JavaScript file.\nCopy and paste the following code into the file and attach the script to an object in the scene. Add an image component to the empty reference in the inspector. TypeScriptJavaScript@componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); TypeScript JavaScript @componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }}let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); @componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }} @componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }} @componentexport class ContinuousCameraFrameExample extends BaseScriptComponent {  private cameraModule: CameraModule = require('LensStudio:CameraModule');  private cameraRequest: CameraModule.CameraRequest;  private cameraTexture: Texture;  private cameraTextureProvider: CameraTextureProvider;  @input  @hint('The image in the scene that will be showing the captured frame.')  uiImage: Image | undefined;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.cameraRequest = CameraModule.createCameraRequest();      this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;      this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);      this.cameraTextureProvider = this.cameraTexture        .control as CameraTextureProvider;      this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        if (this.uiImage) {          this.uiImage.mainPass.baseTex = this.cameraTexture;        }      });    });  }} @component @ component  export class ContinuousCameraFrameExample extends BaseScriptComponent {  export   class   ContinuousCameraFrameExample   extends   BaseScriptComponent   {    private cameraModule: CameraModule = require('LensStudio:CameraModule');    private  cameraModule :  CameraModule  =   require ( 'LensStudio:CameraModule' ) ;    private cameraRequest: CameraModule.CameraRequest;    private  cameraRequest :  CameraModule . CameraRequest ;    private cameraTexture: Texture;    private  cameraTexture :  Texture ;    private cameraTextureProvider: CameraTextureProvider;    private  cameraTextureProvider :  CameraTextureProvider ;      @input    @ input    @hint('The image in the scene that will be showing the captured frame.')    @ hint ( 'The image in the scene that will be showing the captured frame.' )    uiImage: Image | undefined;   uiImage :  Image  |   undefined ;      onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.cameraRequest = CameraModule.createCameraRequest();        this . cameraRequest  =  CameraModule . createCameraRequest ( ) ;        this.cameraRequest.cameraId = CameraModule.CameraId.Default_Color;        this . cameraRequest . cameraId  =  CameraModule . CameraId . Default_Color ;          this.cameraTexture = this.cameraModule.requestCamera(this.cameraRequest);        this . cameraTexture  =   this . cameraModule . requestCamera ( this . cameraRequest ) ;        this.cameraTextureProvider = this.cameraTexture        this . cameraTextureProvider  =   this . cameraTexture         .control as CameraTextureProvider;          . control  as  CameraTextureProvider ;          this.cameraTextureProvider.onNewFrame.add((cameraFrame) => {        this . cameraTextureProvider . onNewFrame . add ( ( cameraFrame )   =>   {          if (this.uiImage) {          if   ( this . uiImage )   {            this.uiImage.mainPass.baseTex = this.cameraTexture;            this . uiImage . mainPass . baseTex  =   this . cameraTexture ;          }          }        });        } ) ;      });      } ) ;    }    }  }  }   let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); let cameraModule = require('LensStudio:CameraModule');let cameraRequest;let cameraTexture;let cameraTextureProvider;//@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}script.createEvent('OnStartEvent').bind(() => {  cameraRequest = CameraModule.createCameraRequest();  cameraRequest.cameraId = CameraModule.CameraId.Default_Color;  cameraTexture = cameraModule.requestCamera(cameraRequest);  cameraTextureProvider = cameraTexture.control;  cameraTextureProvider.onNewFrame.add((cameraFrame) => {    if (script.uiImage) {      script.uiImage.mainPass.baseTex = cameraTexture;    }  });}); let cameraModule = require('LensStudio:CameraModule'); let  cameraModule  =   require ( 'LensStudio:CameraModule' ) ;  let cameraRequest;  let  cameraRequest ;  let cameraTexture;  let  cameraTexture ;  let cameraTextureProvider;  let  cameraTextureProvider ;    //@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}  //@input Component.Image uiImage {\"hint\":\"The image in the scene that will be showing the captured frame.\"}    script.createEvent('OnStartEvent').bind(() => { script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {    cameraRequest = CameraModule.createCameraRequest();   cameraRequest  =   CameraModule . createCameraRequest ( ) ;    cameraRequest.cameraId = CameraModule.CameraId.Default_Color;   cameraRequest . cameraId   =   CameraModule . CameraId . Default_Color ;      cameraTexture = cameraModule.requestCamera(cameraRequest);   cameraTexture  =  cameraModule . requestCamera ( cameraRequest ) ;    cameraTextureProvider = cameraTexture.control;   cameraTextureProvider  =  cameraTexture . control ;      cameraTextureProvider.onNewFrame.add((cameraFrame) => {   cameraTextureProvider . onNewFrame . add ( ( cameraFrame )   =>   {      if (script.uiImage) {      if   ( script . uiImage )   {        script.uiImage.mainPass.baseTex = cameraTexture;       script . uiImage . mainPass . baseTex   =  cameraTexture ;      }      }    });    } ) ;  });  } ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Overview Next Custom Locations Requesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Requesting + Receiving FramesCamera Frame RequestsStill Image RequestsGetting Camera InformationCode Example Requesting + Receiving FramesCamera Frame RequestsStill Image Requests Camera Frame Requests Still Image Requests Getting Camera Information Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/custom-locations": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal ViewpointsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints Spectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture ModuleOverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints Spectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture Module Spectacles FeaturesAPIsCustom LocationsOn this pageCopy pageCustom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\nWas this page helpful?YesNoPreviousCamera ModuleNextGesture Module  Spectacles Features Spectacles Features APIs APIs Custom Locations Custom Locations On this page Copy page  Copy page     page Custom Locations\nOverview\u200b\nCustom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content.\nThe Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location.\nPrevious VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.\nWhat does each Custom Location contain?\u200b\nA finished Custom Location contains:\n\nA colored mesh of the location scanned\nA series of localization viewpoints\n\nThe colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately.\nViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.\nSaved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.\nGetting started\u200b\nPrerequisites\u200b\n\nLens Studio 5.7 or later\nSpectacles OS v5.060.0422 or later\n[Optional] Spectacles Interaction Kit\n\nCreating a Custom Location\u200b\nThe Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".\n\nFor the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present.\nOnce collected tap on the \"Finish Scan\" button to complete.\nFinishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.\nYou will be required to accept the legal disclaimer, which is copied below, before publishing your location.\nAfter you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.\nUpon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio.\nImport Mesh to Lens Studio\u200b\nAfter you have created a unique Location ID, you can import your mesh in Lens Studio.\nPlease make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.\n\n\nCreate new project in Lens Studio.\n\n\nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n\n\nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n\n\nAfter you input the ID, you'll see the location mesh and its tracking asset in Lens Studio.\nIt is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit.\nUsing your Location based Lens\u200b\nA Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case.\nCreating Location Groups\u200b\nMultiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best.\n\nSelect \"New Group\"\nCheck all of the locations that you wish to gather together\nPress New Group\nOne by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n\nOnce all locations are stabilized, the \"Finalize\" button will appear\nOn pressing this the group will be published at the ID will be presented\n\nDuring grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.\nImporting Groups to Lens Studio\u200b\nOnce a group has been published, it is ready to be imported into Lens Studio.\n\nCreate an empty scene object\nOn the object, click \"Add Component\"\nSelect \"Custom Location Group\"\nOn the new component, enter the ID into the \"Group ID\" field\nPress \"Reload Group\"\nLens Studio will now generate Custom Location children for every member of the group\n\n\nGroup Localization\u200b\nThe scanning Lens will only allow groups of up to 5 to be created at this time.\n\nIncremental Scans\u200b\nIf an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated.\n\nSelect ... on an existing scan within your Lens\nSelect + and confirm\nThe user will be presented with the scanning prompt and must localize against the existing scan\nOnce localized the \"Finish Scan\" button will appear\nTap on this button to begin the incremental scan\nNow move around to the new localization points to gather new viewpoints\nOnce completed, tap publish to receive the new ID\nIn the Lens Studio project using the previous ID, replace the ID\nPublish the updated Lens with the updated localization scan\n\nThe new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated\nMethods for Capturing Optimal Viewpoints\u200b\n\nMake sure that your device is in constant motion.\nIdeally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing.\nAvoid remaining still in space and rotating your head around one spot.\nAvoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion.\n Custom Locations Overview\u200b Custom Locations is an easy to use way of mapping real life areas and creating AR experiences around those locations. It uses the Custom Locations Lens to first create Custom Location maps of real-world locations, and then loading these Custom Locations into Lens Studio to author AR content. The Custom Locations Lens can be used to map individual locations, to create groups of locations and to add additional localization viewpoints to a pre-existing location. Previous VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable.   Previous VersionsThis Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile.The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable. Previous Versions This Lens and it's components are an evolution of the Custom Location Creator Lens available on mobile. The LocatedAtComponent is still used, and the Import Mesh to Lens Studio guide is still applicable. What does each Custom Location contain?\u200b A finished Custom Location contains: A colored mesh of the location scanned A series of localization viewpoints The colored mesh will be visible within Lens Studio and can be used to help the developer move virtual objects to their correct location in the editor before deploying to device.\nThe localization viewpoints are captures from which the resulting Lens -- the Lens you're developing at this custom location -- can localize against. When the user of your Lens is approximately in the same position as one of these viewpoints, the system will recognize its position in space and display your content appropriately. ViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document.   ViewpointsThe Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from.Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles.If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated.Example videos showing this progress are attached at end of this document. Viewpoints The Custom Locations Lens will capture many viewpoints during a scan; however, it is limited by where you scan from. For the best results when scanning, move laterally around the location and capture angles you expect users to be viewing from. Yellow camera objects will spawn during a scan to indicate viewpoint locations and angles. If a scan is later determined to be insufficient, a new Incremental Scan can be created with addition viewpoints. However, this will create a new ID and the Lens will need to be updated. Example videos showing this progress are attached at end of this document. Saved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal.   Saved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal. Saved IDs in the Custom Location Creator Lens are only stored locally. Be aware that uninstalling the Lens may lead to these location IDs being forgotten. Additionally, published locations remain public and do not get erased by local removal. Getting started\u200b Prerequisites\u200b Lens Studio 5.7 or later Spectacles OS v5.060.0422 or later [Optional] Spectacles Interaction Kit Creating a Custom Location\u200b The Custom Locations Lens can be found on Spectacles under the \"All Lens\" section. On launch, there will be a \"Scan New\" button at the bottom. When you are at the location you wish to scan, this button should be selected, followed by \"Begin Scanning\".  For the best scan, the location should be observed from many different angles and with lots of lateral movement made during the scan.\nAs the scan progresses the displayed mesh will update itself as Spectacles more accurately tracks the shape of what is present. Once collected tap on the \"Finish Scan\" button to complete. Finishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints.   Finishing the scanThe \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints. Finishing the scan The \"Finish Scan\" button will appear as soon as the minimum requirements for a scan have been met, but it is recommended to continue for longer to get a sufficiently large number of viewpoints. You will be required to accept the legal disclaimer, which is copied below, before publishing your location. After you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information.   After you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information. After you upload this Location to Snap, you will receive a unique reference ID.\nAnyone with access to this ID will be able to use this Location in Lens Studio to publish a Lens, so avoid uploading a Location that contains sensitive personal information. Upon publishing, you will be presented with the newly created scan ID as well as an opportunity to name the scan. The name is only for local purposes within the scanning Lens and, at this time, does not propagate to Lens Studio. Import Mesh to Lens Studio\u200b After you have created a unique Location ID, you can import your mesh in Lens Studio. Please make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported.   Please make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported. Please make sure you are connected to the internet before importing a Custom Location or opening a project with a Custom Location imported. \nCreate new project in Lens Studio.\n Create new project in Lens Studio. \nNavigate to the Scene Hierarchy panel and select World->Custom Location.\n\n Navigate to the Scene Hierarchy panel and select World->Custom Location.  \nSelect the newly created asset in Asset Browser and enter your custom mesh ID.\n\n Select the newly created asset in Asset Browser and enter your custom mesh ID.  After you input the ID, you'll see the location mesh and its tracking asset in Lens Studio. It is recommended that the scan mesh is not used as an asset in the output Lens, but rather just as a development tool for placing assets in the space. However, it is there and can be used however the developer sees fit. Using your Location based Lens\u200b A Sample project is available in the Spectacles-Sample github repository. In it's README there is a step by step guide that creates an example Lens to demonstrate a potential use case. Creating Location Groups\u200b Multiple Custom Locations can be tied together into a Custom Location Group. Follow the above steps until you have at least two locations that are relatively close to each other. Locations that are less than 20m from each other works best. Select \"New Group\" Check all of the locations that you wish to gather together Press New Group One by one, select each of the locations and:\n\nGo to that location and await stabilization\nSelect the next location\n\n Go to that location and await stabilization Select the next location Once all locations are stabilized, the \"Finalize\" button will appear On pressing this the group will be published at the ID will be presented During grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect.   During grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect. During grouping, selecting an already stabilized location will result in it being remapped. This could be useful if you believe the located position is incorrect. Importing Groups to Lens Studio\u200b Once a group has been published, it is ready to be imported into Lens Studio. Create an empty scene object On the object, click \"Add Component\" Select \"Custom Location Group\" On the new component, enter the ID into the \"Group ID\" field Press \"Reload Group\" Lens Studio will now generate Custom Location children for every member of the group  Group Localization\u200b The scanning Lens will only allow groups of up to 5 to be created at this time.  Incremental Scans\u200b If an existing scan is deemed to not have enough viewpoints, an incremental scan can be created to add new viewpoints. The mesh will remain the same, and a new ID will be generated. Select ... on an existing scan within your Lens Select + and confirm The user will be presented with the scanning prompt and must localize against the existing scan Once localized the \"Finish Scan\" button will appear Tap on this button to begin the incremental scan Now move around to the new localization points to gather new viewpoints Once completed, tap publish to receive the new ID In the Lens Studio project using the previous ID, replace the ID Publish the updated Lens with the updated localization scan The new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated   The new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated The new incremental scan will be created in the same coordinate frame as the original, so the placement of virtual objects will not need to be updated Methods for Capturing Optimal Viewpoints\u200b Make sure that your device is in constant motion. Ideally, move in a steady \u201csweeping figure 8\u201d motion for a few seconds, while moving around in the space you are scanning. This will significantly improve the quality of the mesh you are constructing. Avoid remaining still in space and rotating your head around one spot. Avoid extreme vertical viewing angles (pointing up and pointing down), and avoid fast camera motion. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Camera Module Next Gesture Module OverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints OverviewWhat does each Custom Location contain?Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based LensCreating Location GroupsImporting Groups to Lens StudioGroup LocalizationIncremental ScansMethods for Capturing Optimal Viewpoints OverviewWhat does each Custom Location contain? What does each Custom Location contain? Getting startedPrerequisitesCreating a Custom LocationImport Mesh to Lens StudioUsing your Location based Lens Prerequisites Creating a Custom Location Import Mesh to Lens Studio Using your Location based Lens Creating Location GroupsImporting Groups to Lens StudioGroup Localization Importing Groups to Lens Studio Group Localization Incremental Scans Methods for Capturing Optimal Viewpoints AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/gesture-module": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Spectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet AccessPinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Spectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet Access Spectacles FeaturesAPIsGesture ModuleOn this pageCopy pageGesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }}Was this page helpful?YesNoPreviousCustom LocationsNextInternet Access  Spectacles Features Spectacles Features APIs APIs Gesture Module Gesture Module On this page Copy page  Copy page     page Gesture Module\nThe Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures:\n\nPinches\nPalm Tap\nTargeting\nGrab\nPhone in Hand Detection\n\nThe Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system.\nPinch Gesture\u200b\n\nThe pinch gesture detects when the thumb and index fingers of the hand in view are pinched together.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPinchDownEvent(handType): any\n\n\ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n\n\ngetPinchUpEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface PinchStrengthArgs { strength: number; }\n\n\ninterface PinchUpArgs { palmOrientation: vec3; }\n\n\ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n\n\nCode Example\u200b\n@componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }}\nPalm Tap Gesture\u200b\n\nThe palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetPalmTapDownEvent(gestureHandType): any\n\n\ngetPalmTapUpEvent(gestureHandType): any\n\n\nInterfaces\u200b\n\n\ninterface PalmTapUpArgs { confidence: number; }\n\n\ninterface PalmTapDownArgs { confidence: number; }\n\n\nCode Example\u200b\n@componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }}\nTargeting Gesture\u200b\n\nThe targeting gesture is detected when the user has an intent to target a digital content in space.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\ngetTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n\n\nInterfaces\u200b\n\ninterface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n\n\nCode Example\u200b\n@componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }}\nGrab Gesture\u200b\n\nThe grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetGrabBeginEvent(handType): any\n\n\ngetGrabEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface GrabBeginArgs { }\n\n\ninterface GrabEndArgs { }\n\n\nCode Example\u200b\n@componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }}\nPhone in Hand Detection\u200b\n\nThe IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected.\nRelevant APIs\u200b\nGesture Module Methods\u200b\n\n\ngetIsPhoneInHandBeginEvent(handType): any\n\n\ngetIsPhoneInHandEndEvent(handType): any\n\n\nInterfaces\u200b\n\n\ninterface IsPhoneInHandBeginArgs { }\n\n\ninterface IsPhoneInHandEndArgs { }\n\n\nCode Example\u200b\n@componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }} Gesture Module The Gesture Module uses a combination of machine learning and heuristic methods to provide reliable and accurate gesture detection. With this module, you can detect the following gestures: Pinches Palm Tap Targeting Grab Phone in Hand Detection The Spectacles Interaction Kit utilizes the Gesture Module to build its interactions. While we recommend using the Spectacles Interaction Kit for building experiences, the Gesture Module also allows the creation of a custom interaction system. Pinch Gesture\u200b The pinch gesture detects when the thumb and index fingers of the hand in view are pinched together. Relevant APIs\u200b \ngetPinchDownEvent(handType): any\n getPinchDownEvent(handType): any \ngetPinchStrengthEvent(handType): any\n\nThe PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch.\n\n getPinchStrengthEvent(handType): any The PinchStrengthArgs provides a normalized value ranging from 0 to 1, where 0 indicates no pinch and 1 indicates a full pinch. \ngetPinchUpEvent(handType): any\n getPinchUpEvent(handType): any \ninterface PinchStrengthArgs { strength: number; }\n interface PinchStrengthArgs { strength: number; } \ninterface PinchUpArgs { palmOrientation: vec3; }\n interface PinchUpArgs { palmOrientation: vec3; } \ninterface PinchDownArgs { confidence: number; palmOrientation: vec3; }\n interface PinchDownArgs { confidence: number; palmOrientation: vec3; } Code Example\u200b @componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }} @componentexport class PinchExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPinchDownEvent(GestureModule.HandType.Right)      .add((pinchDownArgs: PinchDownArgs) => {        print('Right Hand Pinch Down');      });    this.gestureModule      .getPinchUpEvent(GestureModule.HandType.Right)      .add((pinchUpArgs: PinchUpArgs) => {        print('Right Hand Pinch Up');      });    this.gestureModule      .getPinchStrengthEvent(GestureModule.HandType.Right)      .add((pinchStrengthArgs: PinchStrengthArgs) => {        print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);      });  }} @component @ component  export class PinchExample extends BaseScriptComponent {  export   class   PinchExample   extends   BaseScriptComponent   {    private gestureModule: GestureModule = require('LensStudio:GestureModule');    private  gestureModule :  GestureModule  =   require ( 'LensStudio:GestureModule' ) ;    onAwake() {    onAwake ( )   {      this.gestureModule      this . gestureModule       .getPinchDownEvent(GestureModule.HandType.Right)        . getPinchDownEvent ( GestureModule . HandType . Right )        .add((pinchDownArgs: PinchDownArgs) => {        . add ( ( pinchDownArgs :  PinchDownArgs )   =>   {          print('Right Hand Pinch Down');          print ( 'Right Hand Pinch Down' ) ;        });        } ) ;        this.gestureModule      this . gestureModule       .getPinchUpEvent(GestureModule.HandType.Right)        . getPinchUpEvent ( GestureModule . HandType . Right )        .add((pinchUpArgs: PinchUpArgs) => {        . add ( ( pinchUpArgs :  PinchUpArgs )   =>   {          print('Right Hand Pinch Up');          print ( 'Right Hand Pinch Up' ) ;        });        } ) ;        this.gestureModule      this . gestureModule       .getPinchStrengthEvent(GestureModule.HandType.Right)        . getPinchStrengthEvent ( GestureModule . HandType . Right )        .add((pinchStrengthArgs: PinchStrengthArgs) => {        . add ( ( pinchStrengthArgs :  PinchStrengthArgs )   =>   {          print('Right Hand Pinch Strength: ' + pinchStrengthArgs.strength);          print ( 'Right Hand Pinch Strength: '   +  pinchStrengthArgs . strength ) ;        });        } ) ;    }    }  }  }   Palm Tap Gesture\u200b The palm tap gesture detects when the index finger from one hand touches the palm of the opposite hand. Currently, only the palm tap to the left hand is supported. Relevant APIs\u200b \ngetPalmTapDownEvent(gestureHandType): any\n getPalmTapDownEvent(gestureHandType): any \ngetPalmTapUpEvent(gestureHandType): any\n getPalmTapUpEvent(gestureHandType): any \ninterface PalmTapUpArgs { confidence: number; }\n interface PalmTapUpArgs { confidence: number; } \ninterface PalmTapDownArgs { confidence: number; }\n interface PalmTapDownArgs { confidence: number; } Code Example\u200b @componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }} @componentexport class PalmTapExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getPalmTapUpEvent(GestureModule.HandType.Right)      .add((palmTapUpArgs: PalmTapUpArgs) => {        print('Palm tap up event from GestureModule');      });    this.gestureModule      .getPalmTapDownEvent(GestureModule.HandType.Right)      .add((palmTapDownArgs: PalmTapDownArgs) => {        print('Palm tap down event from GestureModule');      });  }} @component @ component  export class PalmTapExample extends BaseScriptComponent {  export   class   PalmTapExample   extends   BaseScriptComponent   {    private gestureModule: GestureModule = require('LensStudio:GestureModule');    private  gestureModule :  GestureModule  =   require ( 'LensStudio:GestureModule' ) ;    onAwake() {    onAwake ( )   {      this.gestureModule      this . gestureModule       .getPalmTapUpEvent(GestureModule.HandType.Right)        . getPalmTapUpEvent ( GestureModule . HandType . Right )        .add((palmTapUpArgs: PalmTapUpArgs) => {        . add ( ( palmTapUpArgs :  PalmTapUpArgs )   =>   {          print('Palm tap up event from GestureModule');          print ( 'Palm tap up event from GestureModule' ) ;        });        } ) ;        this.gestureModule      this . gestureModule       .getPalmTapDownEvent(GestureModule.HandType.Right)        . getPalmTapDownEvent ( GestureModule . HandType . Right )        .add((palmTapDownArgs: PalmTapDownArgs) => {        . add ( ( palmTapDownArgs :  PalmTapDownArgs )   =>   {          print('Palm tap down event from GestureModule');          print ( 'Palm tap down event from GestureModule' ) ;        });        } ) ;    }    }  }  }   Targeting Gesture\u200b The targeting gesture is detected when the user has an intent to target a digital content in space. Relevant APIs\u200b getTargetingDataEvent(gestureHandType): any\n\nThis method triggers every frame to provide targeting data based on the specified gestureHandType.\n\n This method triggers every frame to provide targeting data based on the specified gestureHandType. interface TargetingDataArgs { isValid: boolean; rayDirectionInWorld: vec3; rayOriginInWorld: vec3; }\n\nWhen isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values.\n\n When isValid is false, rayDirectionInWorld and rayOriginInWorld return the last calculated values. Code Example\u200b @componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }} @componentexport class TargetingExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getTargetingDataEvent(GestureModule.HandType.Right)      .add((targetArgs: TargetingDataArgs) => {        print('Is Valid: ' + targetArgs.isValid);        print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);        print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);      });  }} @component @ component  export class TargetingExample extends BaseScriptComponent {  export   class   TargetingExample   extends   BaseScriptComponent   {    private gestureModule: GestureModule = require('LensStudio:GestureModule');    private  gestureModule :  GestureModule  =   require ( 'LensStudio:GestureModule' ) ;    onAwake() {    onAwake ( )   {      this.gestureModule      this . gestureModule       .getTargetingDataEvent(GestureModule.HandType.Right)        . getTargetingDataEvent ( GestureModule . HandType . Right )        .add((targetArgs: TargetingDataArgs) => {        . add ( ( targetArgs :  TargetingDataArgs )   =>   {          print('Is Valid: ' + targetArgs.isValid);          print ( 'Is Valid: '   +  targetArgs . isValid ) ;          print('Ray Origin In World: ' + targetArgs.rayOriginInWorld);          print ( 'Ray Origin In World: '   +  targetArgs . rayOriginInWorld ) ;          print('Ray Direction In World: ' + targetArgs.rayDirectionInWorld);          print ( 'Ray Direction In World: '   +  targetArgs . rayDirectionInWorld ) ;        });        } ) ;    }    }  }  }   Grab Gesture\u200b The grab gesture detects when the hand performs a grab pose, enabling interactions such as grabbing virtual objects or making a fist. Relevant APIs\u200b \ngetGrabBeginEvent(handType): any\n getGrabBeginEvent(handType): any \ngetGrabEndEvent(handType): any\n getGrabEndEvent(handType): any \ninterface GrabBeginArgs { }\n interface GrabBeginArgs { } \ninterface GrabEndArgs { }\n interface GrabEndArgs { } Code Example\u200b @componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }} @componentexport class GrabExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getGrabBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right Hand Grab Begin');      });  }} @component @ component  export class GrabExample extends BaseScriptComponent {  export   class   GrabExample   extends   BaseScriptComponent   {    private gestureModule: GestureModule = require('LensStudio:GestureModule');    private  gestureModule :  GestureModule  =   require ( 'LensStudio:GestureModule' ) ;    onAwake() {    onAwake ( )   {      this.gestureModule      this . gestureModule       .getGrabBeginEvent(GestureModule.HandType.Right)        . getGrabBeginEvent ( GestureModule . HandType . Right )        .add((grabBeginArgs: GrabBeginArgs) => {        . add ( ( grabBeginArgs :  GrabBeginArgs )   =>   {          print('Right Hand Grab Begin');          print ( 'Right Hand Grab Begin' ) ;        });        } ) ;    }    }  }  }   Phone in Hand Detection\u200b The IsPhoneInHand events indicate whether a tracked hand is holding a phone-like object.\nNote: Only objects with a smartphone-like appearance are detected. Relevant APIs\u200b \ngetIsPhoneInHandBeginEvent(handType): any\n getIsPhoneInHandBeginEvent(handType): any \ngetIsPhoneInHandEndEvent(handType): any\n getIsPhoneInHandEndEvent(handType): any \ninterface IsPhoneInHandBeginArgs { }\n interface IsPhoneInHandBeginArgs { } \ninterface IsPhoneInHandEndArgs { }\n interface IsPhoneInHandEndArgs { } Code Example\u200b @componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }} @componentexport class IsPhoneInHandExample extends BaseScriptComponent {  private gestureModule: GestureModule = require('LensStudio:GestureModule');  onAwake() {    this.gestureModule      .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)      .add((grabBeginArgs: GrabBeginArgs) => {        print('Right hand started to hold a phone.');      });  }} @component @ component  export class IsPhoneInHandExample extends BaseScriptComponent {  export   class   IsPhoneInHandExample   extends   BaseScriptComponent   {    private gestureModule: GestureModule = require('LensStudio:GestureModule');    private  gestureModule :  GestureModule  =   require ( 'LensStudio:GestureModule' ) ;    onAwake() {    onAwake ( )   {      this.gestureModule      this . gestureModule       .getIsPhoneInHandBeginEvent(GestureModule.HandType.Right)        . getIsPhoneInHandBeginEvent ( GestureModule . HandType . Right )        .add((grabBeginArgs: GrabBeginArgs) => {        . add ( ( grabBeginArgs :  GrabBeginArgs )   =>   {          print('Right hand started to hold a phone.');          print ( 'Right hand started to hold a phone.' ) ;        });        } ) ;    }    }  }  }   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Custom Locations Next Internet Access Pinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Pinch GestureRelevant APIsCode ExamplePalm Tap GestureRelevant APIsCode ExampleTargeting GestureRelevant APIsCode ExampleGrab GestureRelevant APIsCode ExamplePhone in Hand DetectionRelevant APIsCode Example Pinch GestureRelevant APIsCode Example Relevant APIs Code Example Palm Tap GestureRelevant APIsCode Example Relevant APIs Code Example Targeting GestureRelevant APIsCode Example Relevant APIs Code Example Grab GestureRelevant APIsCode Example Relevant APIs Code Example Phone in Hand DetectionRelevant APIsCode Example Relevant APIs Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/internet-access": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisitesAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites Spectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboardFetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites Spectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboard Spectacles FeaturesAPIsInternet AccessOn this pageCopy pageInternet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';});Was this page helpful?YesNoPreviousGesture ModuleNextKeyboard  Spectacles Features Spectacles Features APIs APIs Internet Access Internet Access On this page Copy page  Copy page     page Internet Access\nSpectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more.\nPrivacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nThe RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch.\nFetch\u200b\nSpectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference.\n\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nThis API is only available on Spectacles.\nUsage\u200b\nTo use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.\nTo send a request, invoke the fetch method.\nFor example:\nTypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response}\nFetch can also send Get, Post, Delete, and Put requests.\nBelow is a more detailed example that issues a Post request with headers and parses the response in JSON.\nTypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career'];\nIf you'd like even more detail, see here for a sample project that fully utilizes Fetch API.\nCompatibility\u200b\nThe Fetch API supports all methods in the standard, with the following exceptions:\n\n\nThe Request constructor does not support taking another Request as input; it only takes a URL.\n\n\nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n\n\nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n\n\nPerformHttpRequest\u200b\nThe performHttpRequest method can also be used to send HTTPS requests.\nPerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.\nTo use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule.\nFor example:\nTypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }});\nThe RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below.\n\nHeaders can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization:\nvar httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));});\nAccessing Remote Media\u200b\nTo download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example:\nTypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }});\nThe RemoteMediaModule can be used this way to load the following media types:\nMedia TypeMethodReturned ClassImageloadResourceAsImageTextureAsset.TextureVideoloadResourceAsVideoTextureAsset.TextureglTFloadResourceAsGltfAssetAsset.GltfAssetaudioloadResourceAsAudioTrackAssetAsset.AudioTrackAsset\nInternet Availability\u200b\nSpectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work.\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nThe isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available.\nThis function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time.\nThe onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity.\nFeatures as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.\nTypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); Internet Access Spectacles offers APIs to access the internet so you can access external APIs, open WebSocket connections, download media, and more. Privacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.   Privacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information. Privacy Note: Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information. The RemoteServiceModule enables developers to access the internet via HTTPS requests. There are two methods available to make these requests: fetch and performHttpRequest. For most cases we recommend using fetch. Fetch\u200b Spectacles offers the standardized Fetch API to make HTTPS requests on the open internet. This API is based on the MDN reference. Prerequisites\u200b Lens Studio v5.3.0 or later Spectacles OS v5.58.6621 or later This API is only available on Spectacles.   This API is only available on Spectacles. This API is only available on Spectacles. Usage\u200b To use the Fetch API add the RemoteServiceModule to your project and include it in your scripts as per the examples below. The Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404.   The Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404. The Fetch API will only work in the Preview window if the Device Type Override is set to Spectacles. Otherwise all requests will return 404. To send a request, invoke the fetch method. For example: TypeScriptJavaScript@componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response} TypeScript JavaScript @componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response} @componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }} @componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }} @componentexport class FetchExampleGET extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'GET',    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status == 200) {      let text = await response.text();      // Handle response    }  }} @component @ component  export class FetchExampleGET extends BaseScriptComponent {  export   class   FetchExampleGET   extends   BaseScriptComponent   {    @input remoteServiceModule: RemoteServiceModule;    @ input  remoteServiceModule :  RemoteServiceModule ;      // Method called when the script is awake    // Method called when the script is awake    async onAwake() {    async   onAwake ( )   {      let request = new Request('[YOUR URL]', {      let  request  =   new   Request ( '[YOUR URL]' ,   {        method: 'GET',       method :   'GET' ,      });      } ) ;      let response = await this.remoteServiceModule.fetch(request);      let  response  =   await   this . remoteServiceModule . fetch ( request ) ;      if (response.status == 200) {      if   ( response . status  ==   200 )   {        let text = await response.text();        let  text  =   await  response . text ( ) ;        // Handle response        // Handle response      }      }    }    }  }  }   //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response} //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response} //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'GET',});let response = await remoteServiceModule.fetch(request);if (response.status == 200) {  let text = await response.text();  // Handle response} //@input Asset.RemoteServiceModule remoteServiceModule //@input Asset.RemoteServiceModule remoteServiceModule  /** @type {RemoteServiceModule} */  /**  @type   { RemoteServiceModule }  */  var remoteServiceModule = script.remoteServiceModule;  var  remoteServiceModule  =  script . remoteServiceModule ;    let request = new Request('[YOUR URL]', {  let  request  =   new   Request ( '[YOUR URL]' ,   {    method: 'GET',    method :   'GET' ,  });  } ) ;    let response = await remoteServiceModule.fetch(request);  let  response  =   await  remoteServiceModule . fetch ( request ) ;  if (response.status == 200) {  if   ( response . status   ==   200 )   {    let text = await response.text();    let  text  =   await  response . text ( ) ;    // Handle response    // Handle response  }  }   Fetch can also send Get, Post, Delete, and Put requests. Below is a more detailed example that issues a Post request with headers and parses the response in JSON. TypeScriptJavaScript@componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career']; TypeScript JavaScript @componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career']; @componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }} @componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }} @componentexport class FetchExamplePOST extends BaseScriptComponent {  @input remoteServiceModule: RemoteServiceModule;  async onAwake() {    let request = new Request('[YOUR URL]', {      method: 'POST',      body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),      headers: {        'Content-Type': 'application/json',      },    });    let response = await this.remoteServiceModule.fetch(request);    if (response.status != 200) {      print('Failure: response not successful');      return;    }    let contentTypeHeader = response.headers.get('Content-Type');    if (!contentTypeHeader.includes('application/json')) {      print('Failure: wrong content type in response');      return;    }    let responseJson = await response.json();    let username = responseJson.json['user']['name'];    let career = responseJson.json['user']['career'];  }} @component @ component  export class FetchExamplePOST extends BaseScriptComponent {  export   class   FetchExamplePOST   extends   BaseScriptComponent   {    @input remoteServiceModule: RemoteServiceModule;    @ input  remoteServiceModule :  RemoteServiceModule ;      async onAwake() {    async   onAwake ( )   {      let request = new Request('[YOUR URL]', {      let  request  =   new   Request ( '[YOUR URL]' ,   {        method: 'POST',       method :   'POST' ,        body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),       body :   JSON . stringify ( {  user :   {  name :   'user' ,  career :   'developer'   }   } ) ,        headers: {       headers :   {          'Content-Type': 'application/json',          'Content-Type' :   'application/json' ,        },        } ,      });      } ) ;        let response = await this.remoteServiceModule.fetch(request);      let  response  =   await   this . remoteServiceModule . fetch ( request ) ;      if (response.status != 200) {      if   ( response . status  !=   200 )   {        print('Failure: response not successful');        print ( 'Failure: response not successful' ) ;        return;        return ;      }      }        let contentTypeHeader = response.headers.get('Content-Type');      let  contentTypeHeader  =  response . headers . get ( 'Content-Type' ) ;      if (!contentTypeHeader.includes('application/json')) {      if   ( ! contentTypeHeader . includes ( 'application/json' ) )   {        print('Failure: wrong content type in response');        print ( 'Failure: wrong content type in response' ) ;        return;        return ;      }      }        let responseJson = await response.json();      let  responseJson  =   await  response . json ( ) ;      let username = responseJson.json['user']['name'];      let  username  =  responseJson . json [ 'user' ] [ 'name' ] ;      let career = responseJson.json['user']['career'];      let  career  =  responseJson . json [ 'user' ] [ 'career' ] ;    }    }  }  }   //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career']; //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career']; //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let request = new Request('[YOUR URL]', {  method: 'POST',  body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),  headers: {    'Content-Type': 'application/json',  },});let response = await remoteServiceModule.fetch(request);if (response.status != 200) {  print('Failure: response not successful');  return;}let contentTypeHeader = response.headers.get('content-type');if (!contentTypeHeader.includes('application/json')) {  print('Failure: wrong content type in response');  return;}let responseJson = await response.json();let username = responseJson.json['user']['name'];let career = responseJson.json['user']['career']; //@input Asset.RemoteServiceModule remoteServiceModule //@input Asset.RemoteServiceModule remoteServiceModule  /** @type {RemoteServiceModule} */  /**  @type   { RemoteServiceModule }  */  var remoteServiceModule = script.remoteServiceModule;  var  remoteServiceModule  =  script . remoteServiceModule ;    let request = new Request('[YOUR URL]', {  let  request  =   new   Request ( '[YOUR URL]' ,   {    method: 'POST',    method :   'POST' ,    body: JSON.stringify({ user: { name: 'user', career: 'developer' } }),    body :   JSON . stringify ( {   user :   {   name :   'user' ,   career :   'developer'   }   } ) ,    headers: {    headers :   {      'Content-Type': 'application/json',      'Content-Type' :   'application/json' ,    },    } ,  });  } ) ;    let response = await remoteServiceModule.fetch(request);  let  response  =   await  remoteServiceModule . fetch ( request ) ;  if (response.status != 200) {  if   ( response . status   !=   200 )   {    print('Failure: response not successful');    print ( 'Failure: response not successful' ) ;    return;    return ;  }  }    let contentTypeHeader = response.headers.get('content-type');  let  contentTypeHeader  =  response . headers . get ( 'content-type' ) ;  if (!contentTypeHeader.includes('application/json')) {  if   ( ! contentTypeHeader . includes ( 'application/json' ) )   {    print('Failure: wrong content type in response');    print ( 'Failure: wrong content type in response' ) ;    return;    return ;  }  }    let responseJson = await response.json();  let  responseJson  =   await  response . json ( ) ;  let username = responseJson.json['user']['name'];  let  username  =  responseJson . json [ 'user' ] [ 'name' ] ;  let career = responseJson.json['user']['career'];  let  career  =  responseJson . json [ 'user' ] [ 'career' ] ;   If you'd like even more detail, see here for a sample project that fully utilizes Fetch API. Compatibility\u200b The Fetch API supports all methods in the standard, with the following exceptions: \nThe Request constructor does not support taking another Request as input; it only takes a URL.\n The Request constructor does not support taking another Request as input; it only takes a URL. \nRequest and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported.\n Request and Response bodies can be retrieved via bytes, text, and json. The body, blob, and arrayBuffer properties are not yet supported. \nOther properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below.\nRequestResponseHeadersRequest(input, options)\u2705Response()\u274cHeaders()\u2705body\u274cbody\u274cappend()\u2705bodyUsed\u2705bodyUsed\u2705delete()\u2705cache\u274cheaders\u2705entries()\u2705credentials\u274cok\u2705forEach()\u274cdestination\u274credirected\u274cget()\u2705headers\u2705status\u2705getSetCookie()\u274cintegrity\u274cstatusText\u2705has()\u2705isHistoryNavigation\u274ctype\u274ckeys()\u2705keepalive\u2705url\u2705set()\u2705method\u2705arrayBuffer()\u274cvalues()\u2705mode\u274cblob()\u274credirect\u2705bytes()\u2705referrer\u274cclone()\u274creferrerPolicy\u274cformData()\u274csignal\u274cjson()\u2705url\u2705text()\u2705arrayBuffer()\u274cblob()\u274cbytes()\u2705clone()\u274cformData()\u274cjson()\u2705text()\u2705\n Other properties related to web browser functionality are not supported. The full list of supported/unsupported properties is provided in the tables below. PerformHttpRequest\u200b The performHttpRequest method can also be used to send HTTPS requests. PerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead.   PerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead. PerformHttpRequest is a simpler API with less functionality, provided for compatibility reasons. We recommend using fetch instead. To use performHttpRequest, create a RemoteServiceHttpRequest, configure it, and send it via the RemoteServiceModule. For example: TypeScriptJavaScript@componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }}); TypeScript JavaScript @componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }}); @componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }} @componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }} @componentexport class GetCatFacts extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        print('Body: ' + response.body);      }    });  }} @component @ component  export class GetCatFacts extends BaseScriptComponent {  export   class   GetCatFacts   extends   BaseScriptComponent   {    @input    @ input    remoteServiceModule: RemoteServiceModule;   remoteServiceModule :  RemoteServiceModule ;      // Method called when the script is awake    // Method called when the script is awake    onAwake() {    onAwake ( )   {      // Create a new HTTP request      // Create a new HTTP request      let httpRequest = RemoteServiceHttpRequest.create();      let  httpRequest  =  RemoteServiceHttpRequest . create ( ) ;      httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request     httpRequest . url  =   'https://catfact.ninja/facts' ;   // Set the URL for the request      httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET     httpRequest . method  =  RemoteServiceHttpRequest . HttpRequestMethod . Get ;   // Set the HTTP method to GET        // Perform the HTTP request      // Perform the HTTP request      this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      this . remoteServiceModule . performHttpRequest ( httpRequest ,   ( response )   =>   {        if (response.statusCode === 200) {        if   ( response . statusCode  ===   200 )   {          // Check if the response status is 200 (OK)          // Check if the response status is 200 (OK)          print('Body: ' + response.body);          print ( 'Body: '   +  response . body ) ;        }        }      });      } ) ;    }    }  }  }   //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }}); //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }}); //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    print('Body: ' + response.body);  }}); //@input Asset.RemoteServiceModule remoteServiceModule //@input Asset.RemoteServiceModule remoteServiceModule  /** @type {RemoteServiceModule} */  /**  @type   { RemoteServiceModule }  */  var remoteServiceModule = script.remoteServiceModule;  var  remoteServiceModule  =  script . remoteServiceModule ;    // Create a new HTTP request  // Create a new HTTP request  let httpRequest = RemoteServiceHttpRequest.create();  let  httpRequest  =   RemoteServiceHttpRequest . create ( ) ;  httpRequest.url = 'https://catfact.ninja/facts'; // Set the URL for the request httpRequest . url   =   'https://catfact.ninja/facts' ;   // Set the URL for the request  httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET httpRequest . method   =   RemoteServiceHttpRequest . HttpRequestMethod . Get ;   // Set the HTTP method to GET    // Perform the HTTP request  // Perform the HTTP request  remoteServiceModule.performHttpRequest(httpRequest, (response) => { remoteServiceModule . performHttpRequest ( httpRequest ,   ( response )   =>   {    if (response.statusCode === 200) {    if   ( response . statusCode   ===   200 )   {      // Check if the response status is 200 (OK)      // Check if the response status is 200 (OK)      print('Body: ' + response.body);      print ( 'Body: '   +  response . body ) ;    }    }  });  } ) ;   The RemoteServiceHttpRequest can send Get, Post, Delete, and Put requests. The URL for the request must be specified, and be sure to set the apiSpecId to \u201chttp\u201d as shown below. Headers can be set via setHeader(name, value). Below is an example that utilizes request headers for authorization: var httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));}); var httpRequest = RemoteServiceHttpRequest.create();httpRequest.url = '[YOUR URL]';httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded');httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`);httpRequest.body = 'grant_type=client_credentials';script.remoteServiceModule.performHttpRequest(httpRequest, function (response) {  print('Status code: ' + response.statusCode);  print('Content type: ' + response.contentType);  print('Body: ' + response.body);  print('Headers: ' + response.headers);  print('Header(date): ' + response.getHeader('date'));  print('Header(contenttype): ' + response.getHeader('content-type'));}); var httpRequest = RemoteServiceHttpRequest.create(); var  httpRequest  =   RemoteServiceHttpRequest . create ( ) ;  httpRequest.url = '[YOUR URL]'; httpRequest . url   =   '[YOUR URL]' ;  httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Post; httpRequest . method   =   RemoteServiceHttpRequest . HttpRequestMethod . Post ;  httpRequest.setHeader('Content-Type', 'application/x-www-form-urlencoded'); httpRequest . setHeader ( 'Content-Type' ,   'application/x-www-form-urlencoded' ) ;  httpRequest.setHeader('Authorization', `Basic ${encodedCredentials}`); httpRequest . setHeader ( 'Authorization' ,   ` Basic  ${ encodedCredentials } ` ) ;  httpRequest.body = 'grant_type=client_credentials'; httpRequest . body   =   'grant_type=client_credentials' ;    script.remoteServiceModule.performHttpRequest(httpRequest, function (response) { script . remoteServiceModule . performHttpRequest ( httpRequest ,   function   ( response )   {    print('Status code: ' + response.statusCode);    print ( 'Status code: '   +  response . statusCode ) ;    print('Content type: ' + response.contentType);    print ( 'Content type: '   +  response . contentType ) ;    print('Body: ' + response.body);    print ( 'Body: '   +  response . body ) ;    print('Headers: ' + response.headers);    print ( 'Headers: '   +  response . headers ) ;    print('Header(date): ' + response.getHeader('date'));    print ( 'Header(date): '   +  response . getHeader ( 'date' ) ) ;    print('Header(contenttype): ' + response.getHeader('content-type'));    print ( 'Header(contenttype): '   +  response . getHeader ( 'content-type' ) ) ;  });  } ) ;   Accessing Remote Media\u200b To download media (Image, Video, glTF, and audio), utilize the makeResourceFromUrl method to create a Resource out of the URL, then pass that resource into the appropriate method in RemoteMediaModule. For example: TypeScriptJavaScript@componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }}); TypeScript JavaScript @componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }}); @componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }} @componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }} @componentexport class GetHttpImage extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Import the RemoteMediaModule  private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');  // Method called when the script is awake  onAwake() {    // Create a new HTTP request    let httpRequest = RemoteServiceHttpRequest.create();    httpRequest.url =      'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET    // Perform the HTTP request    this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      if (response.statusCode === 200) {        // Check if the response status is 200 (OK)        let textureResource = response.asResource(); // Convert the response to a resource        this.remoteMediaModule.loadResourceAsImageTexture(          textureResource,          (texture) => {            // Assign texture to a material            print(texture);          },          (error) => {            print('Error loading image texture: ' + error); // Print an error message if loading fails          }        );      }    });  }} @component @ component  export class GetHttpImage extends BaseScriptComponent {  export   class   GetHttpImage   extends   BaseScriptComponent   {    @input    @ input    remoteServiceModule: RemoteServiceModule;   remoteServiceModule :  RemoteServiceModule ;      // Import the RemoteMediaModule    // Import the RemoteMediaModule    private remoteMediaModule: RemoteMediaModule = require('LensStudio:RemoteMediaModule');    private  remoteMediaModule :  RemoteMediaModule  =   require ( 'LensStudio:RemoteMediaModule' ) ;      // Method called when the script is awake    // Method called when the script is awake    onAwake() {    onAwake ( )   {      // Create a new HTTP request      // Create a new HTTP request      let httpRequest = RemoteServiceHttpRequest.create();      let  httpRequest  =  RemoteServiceHttpRequest . create ( ) ;      httpRequest.url =     httpRequest . url  =        'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request        'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png' ;   // Set the URL for the request      httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET     httpRequest . method  =  RemoteServiceHttpRequest . HttpRequestMethod . Get ;   // Set the HTTP method to GET        // Perform the HTTP request      // Perform the HTTP request      this.remoteServiceModule.performHttpRequest(httpRequest, (response) => {      this . remoteServiceModule . performHttpRequest ( httpRequest ,   ( response )   =>   {        if (response.statusCode === 200) {        if   ( response . statusCode  ===   200 )   {          // Check if the response status is 200 (OK)          // Check if the response status is 200 (OK)          let textureResource = response.asResource(); // Convert the response to a resource          let  textureResource  =  response . asResource ( ) ;   // Convert the response to a resource          this.remoteMediaModule.loadResourceAsImageTexture(          this . remoteMediaModule . loadResourceAsImageTexture (            textureResource,           textureResource ,            (texture) => {            ( texture )   =>   {              // Assign texture to a material              // Assign texture to a material              print(texture);              print ( texture ) ;            },            } ,            (error) => {            ( error )   =>   {              print('Error loading image texture: ' + error); // Print an error message if loading fails              print ( 'Error loading image texture: '   +  error ) ;   // Print an error message if loading fails            }            }          );          ) ;        }        }      });      } ) ;    }    }  }  }   //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }}); //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }}); //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let remoteMediaModule = require('LensStudio:RemoteMediaModule');// Create a new HTTP requestlet httpRequest = RemoteServiceHttpRequest.create();httpRequest.url =  'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the requesthttpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET// Perform the HTTP requestremoteServiceModule.performHttpRequest(httpRequest, (response) => {  if (response.statusCode === 200) {    // Check if the response status is 200 (OK)    let textureResource = response.asResource(); // Convert the response to a resource    remoteMediaModule.loadResourceAsImageTexture(      textureResource,      (texture) => {        // Assign texture to a material mainPass baseTex        print(texture);      },      (error) => {        print('Error loading image texture: ' + error); // Print an error message if loading fails      }    );  }}); //@input Asset.RemoteServiceModule remoteServiceModule //@input Asset.RemoteServiceModule remoteServiceModule  /** @type {RemoteServiceModule} */  /**  @type   { RemoteServiceModule }  */  var remoteServiceModule = script.remoteServiceModule;  var  remoteServiceModule  =  script . remoteServiceModule ;    let remoteMediaModule = require('LensStudio:RemoteMediaModule');  let  remoteMediaModule  =   require ( 'LensStudio:RemoteMediaModule' ) ;    // Create a new HTTP request  // Create a new HTTP request  let httpRequest = RemoteServiceHttpRequest.create();  let  httpRequest  =   RemoteServiceHttpRequest . create ( ) ;  httpRequest.url = httpRequest . url   =    'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png'; // Set the URL for the request    'https://developers.snap.com/img/spectacles/spectacles-2024-hero.png' ;   // Set the URL for the request  httpRequest.method = RemoteServiceHttpRequest.HttpRequestMethod.Get; // Set the HTTP method to GET httpRequest . method   =   RemoteServiceHttpRequest . HttpRequestMethod . Get ;   // Set the HTTP method to GET    // Perform the HTTP request  // Perform the HTTP request  remoteServiceModule.performHttpRequest(httpRequest, (response) => { remoteServiceModule . performHttpRequest ( httpRequest ,   ( response )   =>   {    if (response.statusCode === 200) {    if   ( response . statusCode   ===   200 )   {      // Check if the response status is 200 (OK)      // Check if the response status is 200 (OK)      let textureResource = response.asResource(); // Convert the response to a resource      let  textureResource  =  response . asResource ( ) ;   // Convert the response to a resource      remoteMediaModule.loadResourceAsImageTexture(     remoteMediaModule . loadResourceAsImageTexture (        textureResource,       textureResource ,        (texture) => {        ( texture )   =>   {          // Assign texture to a material mainPass baseTex          // Assign texture to a material mainPass baseTex          print(texture);          print ( texture ) ;        },        } ,        (error) => {        ( error )   =>   {          print('Error loading image texture: ' + error); // Print an error message if loading fails          print ( 'Error loading image texture: '   +  error ) ;   // Print an error message if loading fails        }        }      );      ) ;    }    }  });  } ) ;   The RemoteMediaModule can be used this way to load the following media types: Internet Availability\u200b Spectacles allows you to detect and respond to changes of internet availability status. This can help you avoid frozen, empty, or broken experiences and clearly indicate that the internet is needed for some functionality to work. Prerequisites\u200b Lens Studio v5.7.0 or later Spectacles OS v5.60.x or later The isInternetAvailable method of DeviceInfoSystem checks the current status of the internet connection. It returns a boolean value:\nTrue if internet is available.\nFalse if internet is not available. This function is typically used to determine the initial state of the internet connection upon lens starts. It can also be used to determine internet availability at an arbitrary time. The onInternetStatusChanged is an event, retrieved from DeviceInfoSystem, that triggers whenever there is a change in the internet connection status. It allows the application to respond dynamically to changes in connectivity. Features as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection.   Features as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection. Features as Text-to-Speech, Speech-to-Text, Connected Lenses, Web View, Bitmoji Module, Location Cloudstorage, Map, Leaderboard, requires Internet Connection. TypeScriptJavaScript@componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); TypeScript JavaScript @componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }}// @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); @componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }} @componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }} @componentexport class NewScript extends BaseScriptComponent {  @input textObject: Text;  onAwake() {    this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      ? 'Internet is available'      : 'No internet';    global.deviceInfoSystem.onInternetStatusChanged.add((args) => {      this.textObject.text = args.isInternetAvailable        ? 'UPDATED: Internet is available'        : 'UPDATED: No internet';    });  }} @component @ component  export class NewScript extends BaseScriptComponent {  export   class   NewScript   extends   BaseScriptComponent   {    @input textObject: Text;    @ input  textObject :  Text ;      onAwake() {    onAwake ( )   {      this.textObject.text = global.deviceInfoSystem.isInternetAvailable()      this . textObject . text  =  global . deviceInfoSystem . isInternetAvailable ( )        ? 'Internet is available'        ?   'Internet is available'        : 'No internet';        :   'No internet' ;        global.deviceInfoSystem.onInternetStatusChanged.add((args) => {     global . deviceInfoSystem . onInternetStatusChanged . add ( ( args )   =>   {        this.textObject.text = args.isInternetAvailable        this . textObject . text  =  args . isInternetAvailable         ? 'UPDATED: Internet is available'          ?   'UPDATED: Internet is available'          : 'UPDATED: No internet';          :   'UPDATED: No internet' ;      });      } ) ;    }    }  }  }   // @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); // @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); // @input Component.Text textObjectscript.textObject.text = global.deviceInfoSystem.isInternetAvailable()  ? 'Internet is available'  : 'No internet';global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) {  script.textObject.text = eventData.isInternetAvailable    ? 'UPDATED: Internet is available'    : 'UPDATED: No internet';}); // @input Component.Text textObject // @input Component.Text textObject    script.textObject.text = global.deviceInfoSystem.isInternetAvailable() script . textObject . text   =  global . deviceInfoSystem . isInternetAvailable ( )    ? 'Internet is available'    ?   'Internet is available'    : 'No internet';    :   'No internet' ;    global.deviceInfoSystem.onInternet  StatusChanged.add(function (eventData) { global . deviceInfoSystem . onInternet    StatusChanged . add ( function   ( eventData )   {    script.textObject.text = eventData.isInternetAvailable   script . textObject . text   =  eventData . isInternetAvailable      ? 'UPDATED: Internet is available'      ?   'UPDATED: Internet is available'      : 'UPDATED: No internet';      :   'UPDATED: No internet' ;  });  } ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Gesture Module Next Keyboard FetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites FetchPrerequisitesUsageCompatibilityPerformHttpRequestAccessing Remote MediaInternet AvailabilityPrerequisites FetchPrerequisitesUsageCompatibility Prerequisites Usage Compatibility PerformHttpRequest Accessing Remote Media Internet AvailabilityPrerequisites Prerequisites AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/key-board": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample UsageAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Spectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboardOverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Spectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboard Spectacles FeaturesAPIsKeyboardOn this pageCopy pageKeyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);});Was this page helpful?YesNoPreviousInternet AccessNextLeaderboard  Spectacles Features Spectacles Features APIs APIs Keyboard Keyboard On this page Copy page  Copy page     page Keyboard\nOverview\u200b\nThe Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages.\nDeveloper Control\u200b\nDevelopers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone.\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.060 or later\nMobile Spectacles App Keyboard\u200b\n\nThe mobile Spectacles App supports keyboard functionality via the user's mobile device.\nWhen the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\"\nAR Keyboard\u200b\n\nThe AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user.\nUsers can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\"\nActivation Scenarios\u200b\nOpening the Keyboard\u200b\n\n\nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n\n\nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n\n\nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n\n\nClosing the Keyboard\u200b\nPressing the return key on either keyboard will deactivate the text input field and close the keyboard.\nExample Usage\u200b\nTypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); Keyboard Overview\u200b The Keyboard enables users to input any alphanumeric character, including special characters. It supports use cases such as web browsing, entering usernames/passwords, and sending messages. Developers have the ability to enable or disable the keyboard during a Lens session. The keyboard layout can be adjusted through the Keyboard Type Configuration, which includes options for Text, URL, Numpad, and Phone. Prerequisites\u200b Lens Studio v5.7.0 or later Spectacles OS v5.060 or later Mobile Spectacles App Keyboard\u200b The mobile Spectacles App supports keyboard functionality via the user's mobile device. When the mobile keyboard is active, users can close it to revert to the AR Keyboard. A system notification will indicate \"keyboard disconnected.\" AR Keyboard\u200b The AR Keyboard is a digital overlay displayed within the Lens experience on the Spectacles device. When activated, it animates directly in front of the user. Users can switch to the mobile Spectacles app keyboard from the AR Keyboard. A system notification will confirm \"keyboard connected.\" Activation Scenarios\u200b \nIf the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user.\n If the Spectacles app is not connected, the AR Keyboard will animate directly in front of the user. \nIf the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available.\n If the Spectacles app is connected but not open, the AR Keyboard will animate directly in front of the user, and a mobile notification will alert that the mobile keyboard is available. \nIf the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\"\n If the Spectacles app is both connected and open, the mobile keyboard will be activated, accompanied by a system notification stating \"keyboard connected.\" Pressing the return key on either keyboard will deactivate the text input field and close the keyboard. Example Usage\u200b TypeScriptJavaScriptrequire('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); TypeScript JavaScript require('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }}require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); require('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }} require('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }} require('LensStudio:TextInputModule');export class KeyboardExample extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      let options = new TextInputSystem.KeyboardOptions();      options.enablePreview = true;      options.keyboardType = TextInputSystem.KeyboardType.Text;      options.returnKeyType = TextInputSystem.ReturnKeyType.Return;      global.textInputSystem.requestKeyboard(options);    });  }} require('LensStudio:TextInputModule'); require ( 'LensStudio:TextInputModule' ) ;    export class KeyboardExample extends BaseScriptComponent {  export   class   KeyboardExample   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        let options = new TextInputSystem.KeyboardOptions();        let  options  =   new   TextInputSystem . KeyboardOptions ( ) ;        options.enablePreview = true;       options . enablePreview  =   true ;        options.keyboardType = TextInputSystem.KeyboardType.Text;       options . keyboardType  =  TextInputSystem . KeyboardType . Text ;        options.returnKeyType = TextInputSystem.ReturnKeyType.Return;       options . returnKeyType  =  TextInputSystem . ReturnKeyType . Return ;          global.textInputSystem.requestKeyboard(options);       global . textInputSystem . requestKeyboard ( options ) ;      });      } ) ;    }    }  }  }   require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); require('LensStudio:TextInputModule');script.createEvent('OnStartEvent').bind(function () {  var options = new TextInputSystem.KeyboardOptions();  options.enablePreview = true;  options.keyboardType = TextInputSystem.KeyboardType.Text;  options.returnKeyType = TextInputSystem.ReturnKeyType.Return;  global.textInputSystem.requestKeyboard(options);}); require('LensStudio:TextInputModule'); require ( 'LensStudio:TextInputModule' ) ;    script.createEvent('OnStartEvent').bind(function () { script . createEvent ( 'OnStartEvent' ) . bind ( function   ( )   {    var options = new TextInputSystem.KeyboardOptions();    var  options  =   new   TextInputSystem . KeyboardOptions ( ) ;    options.enablePreview = true;   options . enablePreview   =   true ;    options.keyboardType = TextInputSystem.KeyboardType.Text;   options . keyboardType   =   TextInputSystem . KeyboardType . Text ;    options.returnKeyType = TextInputSystem.ReturnKeyType.Return;   options . returnKeyType   =   TextInputSystem . ReturnKeyType . Return ;    global.textInputSystem.requestKeyboard(options);   global . textInputSystem . requestKeyboard ( options ) ;  });  } ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Internet Access Next Leaderboard OverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage OverviewPrerequisitesMobile Spectacles App KeyboardAR KeyboardActivation ScenariosExample Usage Overview Prerequisites Mobile Spectacles App Keyboard AR Keyboard Activation Scenarios Example Usage AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/leaderboard": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying LeaderboardsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Spectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocationUsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Spectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocation Spectacles FeaturesAPIsLeaderboardOn this pageCopy pageLeaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard();Was this page helpful?YesNoPreviousKeyboardNextLocation  Spectacles Features Spectacles Features APIs APIs Leaderboard Leaderboard On this page Copy page  Copy page     page Leaderboard\nSpectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide.\nPrivacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.\nFor Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.\nUsage\u200b\n\nPrerequisites\u200b\n\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.x or later\n\nCreating Leaderboards\u200b\nTo create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set:\n\nname: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned.\norderingType: Ascending or Descending.\nttlSeconds: The number of seconds this leaderboard should exist before being reset.\n\nOnce the Leaderboard is created, you can submit scores to it, or retrieve it for display.\nSubmitting Scores\u200b\nTo submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000.\nTypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard();\nDisplaying Leaderboards\u200b\nTo load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify:\n\nusersLimit: How many user entries to return.\nusersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends.\n\nThe leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries.\nTypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); Leaderboard Spectacles offers APIs to create Leaderboards so users of your Lens can track their progress and compare scores with friends and users worldwide. Privacy Note: Leaderboards protect user privacy and require each user to opt in to share scores.   Privacy Note: Leaderboards protect user privacy and require each user to opt in to share scores. Privacy Note: Leaderboards protect user privacy and require each user to opt in to share scores. For Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI.   For Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI. For Spectacles we only support the Leaderboard Module, not the Leaderboard Component that's used by mobile Snapchat. The Leaderboard Module provides access to raw Leaderboard data, but unlike the Leaderboard Component it does not provide any UI. Usage\u200b Prerequisites\u200b Lens Studio v5.7.0 or later Spectacles OS v5.60.x or later Creating Leaderboards\u200b To create a new Leaderboard add the LeaderboardModule to your project and include it in your scripts. The LeaderboardModule enables you to create or retrieve a Leaderboard through the getLeaderboard method, which takes a CreateOptions parameter where you can set: name: The name of the Leaderboard. Your Leaderboard is uniquely determined by the combination of your Lens ID and this leaderboard name. If a Leaderboard with this name does not exist, it will be created for this lens. If the Leaderboard with this name does exist, it will simply be returned. orderingType: Ascending or Descending. ttlSeconds: The number of seconds this leaderboard should exist before being reset. Once the Leaderboard is created, you can submit scores to it, or retrieve it for display. Submitting Scores\u200b To submit a score to the leaderboard, simply call the submitScore method, as shown in the example below, where we retrieve a Leaderboard and immediately submit a random score between 1 and 1000. TypeScriptJavaScript@componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard(); TypeScript JavaScript @componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard(); @componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }} @componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }} @componentexport class LeaderboardExample extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let randomInt = MathUtils.randomRange(1, 1000);        leaderboardInstance.submitScore(          randomInt,          this.submitScoreSuccessCallback,          (status) => {            print('[Leaderboard] Submit failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }  submitScoreSuccessCallback(currentUserInfo): void {    print('[Leaderboard] submitScore success callback');    if (!isNull(currentUserInfo)) {      print(        `[Leaderboard] Current User info: ${          currentUserInfo.snapchatUser.displayName            ? currentUserInfo.snapchatUser.displayName            : ''        } score: ${currentUserInfo.score}`      );    }  }} @component @ component  export class LeaderboardExample extends BaseScriptComponent {  export   class   LeaderboardExample   extends   BaseScriptComponent   {    private leaderboardModule = require('LensStudio:LeaderboardModule');    private  leaderboardModule  =   require ( 'LensStudio:LeaderboardModule' ) ;      onAwake() {    onAwake ( )   {      this.updateLeaderboard();      this . updateLeaderboard ( ) ;    }    }      updateLeaderboard(): void {    updateLeaderboard ( ) :   void   {      const leaderboardCreateOptions = Leaderboard.CreateOptions.create();      const  leaderboardCreateOptions  =  Leaderboard . CreateOptions . create ( ) ;      leaderboardCreateOptions.name = 'TEST_NAME';     leaderboardCreateOptions . name  =   'TEST_NAME' ;      leaderboardCreateOptions.ttlSeconds = 800000;     leaderboardCreateOptions . ttlSeconds  =   800000 ;      leaderboardCreateOptions.orderingType = 1;     leaderboardCreateOptions . orderingType  =   1 ;        this.leaderboardModule.getLeaderboard(      this . leaderboardModule . getLeaderboard (        leaderboardCreateOptions,       leaderboardCreateOptions ,        (leaderboardInstance) => {        ( leaderboardInstance )   =>   {          let randomInt = MathUtils.randomRange(1, 1000);          let  randomInt  =  MathUtils . randomRange ( 1 ,   1000 ) ;          leaderboardInstance.submitScore(         leaderboardInstance . submitScore (            randomInt,           randomInt ,            this.submitScoreSuccessCallback,            this . submitScoreSuccessCallback ,            (status) => {            ( status )   =>   {              print('[Leaderboard] Submit failed, status: ' + status);              print ( '[Leaderboard] Submit failed, status: '   +  status ) ;            }            }          );          ) ;        },        } ,        (status) => {        ( status )   =>   {          print('[Leaderboard] getLeaderboard failed, status: ' + status);          print ( '[Leaderboard] getLeaderboard failed, status: '   +  status ) ;        }        }      );      ) ;    }    }      submitScoreSuccessCallback(currentUserInfo): void {    submitScoreSuccessCallback ( currentUserInfo ) :   void   {      print('[Leaderboard] submitScore success callback');      print ( '[Leaderboard] submitScore success callback' ) ;      if (!isNull(currentUserInfo)) {      if   ( ! isNull ( currentUserInfo ) )   {        print(        print (          `[Leaderboard] Current User info: ${          ` [Leaderboard] Current User info:  ${            currentUserInfo.snapchatUser.displayName           currentUserInfo . snapchatUser . displayName             ? currentUserInfo.snapchatUser.displayName              ?  currentUserInfo . snapchatUser . displayName             : ''              :   ''          } score: ${currentUserInfo.score}`          }  score:  ${ currentUserInfo . score } `        );        ) ;      }      }    }    }  }  }   const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let randomInt = MathUtils.randomRange(1, 1000);      leaderboardInstance.submitScore(        randomInt,        submitScoreSuccessCallback,        (status) => {          print('[Leaderboard] Submit failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}function submitScoreSuccessCallback(currentUserInfo) {  print('[Leaderboard] submitScore success callback');  if (!isNull(currentUserInfo)) {    print(      `[Leaderboard] Current User info: ${        currentUserInfo.snapchatUser.displayName          ? currentUserInfo.snapchatUser.displayName          : ''      } score: ${currentUserInfo.score}`    );  }}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule'); const  leaderboardModule  =   require ( 'LensStudio:LeaderboardModule' ) ;    function updateLeaderboard() {  function   updateLeaderboard ( )   {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    const  leaderboardCreateOptions  =   Leaderboard . CreateOptions . create ( ) ;    leaderboardCreateOptions.name = 'TEST_NAME';   leaderboardCreateOptions . name   =   'TEST_NAME' ;    leaderboardCreateOptions.ttlSeconds = 800000;   leaderboardCreateOptions . ttlSeconds   =   800000 ;    leaderboardCreateOptions.orderingType = 1;   leaderboardCreateOptions . orderingType   =   1 ;      leaderboardModule.getLeaderboard(   leaderboardModule . getLeaderboard (      leaderboardCreateOptions,     leaderboardCreateOptions ,      (leaderboardInstance) => {      ( leaderboardInstance )   =>   {        let randomInt = MathUtils.randomRange(1, 1000);        let  randomInt  =   MathUtils . randomRange ( 1 ,   1000 ) ;        leaderboardInstance.submitScore(       leaderboardInstance . submitScore (          randomInt,         randomInt ,          submitScoreSuccessCallback,         submitScoreSuccessCallback ,          (status) => {          ( status )   =>   {            print('[Leaderboard] Submit failed, status: ' + status);            print ( '[Leaderboard] Submit failed, status: '   +  status ) ;          }          }        );        ) ;      },      } ,      (status) => {      ( status )   =>   {        print('[Leaderboard] getLeaderboard failed, status: ' + status);        print ( '[Leaderboard] getLeaderboard failed, status: '   +  status ) ;      }      }    );    ) ;  }  }    function submitScoreSuccessCallback(currentUserInfo) {  function   submitScoreSuccessCallback ( currentUserInfo )   {    print('[Leaderboard] submitScore success callback');    print ( '[Leaderboard] submitScore success callback' ) ;    if (!isNull(currentUserInfo)) {    if   ( ! isNull ( currentUserInfo ) )   {      print(      print (        `[Leaderboard] Current User info: ${        ` [Leaderboard] Current User info:  ${          currentUserInfo.snapchatUser.displayName         currentUserInfo . snapchatUser . displayName            ? currentUserInfo.snapchatUser.displayName            ?  currentUserInfo . snapchatUser . displayName            : ''            :   ''        } score: ${currentUserInfo.score}`        }  score:  ${ currentUserInfo . score } `      );      ) ;    }    }  }  }    updateLeaderboard();  updateLeaderboard ( ) ;   Displaying Leaderboards\u200b To load a Leaderboard's entries, call getLeaderboardInfo with RetrievalOptions. The RetrievalOptions allow you to specify: usersLimit: How many user entries to return. usersType: Whether to return the Global Leaderboard or the Friends Leaderboard. The Global Leaderboard returns scores of all users of the Lens, while the Friends Leaderboard only ranks and returns the scores of the current user's friends. The leaderboard entries are returned in the success callback as an array of UserRecord objects. Below is an example demonstrating how to print the leaderboard entries. TypeScriptJavaScript@componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); TypeScript JavaScript @componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }}const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); @componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }} @componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }} @componentexport class LeaderBoardInfo extends BaseScriptComponent {  private leaderboardModule = require('LensStudio:LeaderboardModule');  onAwake() {    this.updateLeaderboard();  }  updateLeaderboard(): void {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    leaderboardCreateOptions.name = 'TEST_NAME';    leaderboardCreateOptions.ttlSeconds = 800000;    leaderboardCreateOptions.orderingType = 1;    this.leaderboardModule.getLeaderboard(      leaderboardCreateOptions,      (leaderboardInstance) => {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        leaderboardRetrievalOptions.usersLimit = 10;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;        leaderboardInstance.getLeaderboardInfo(          leaderboardRetrievalOptions,          (otherRecords, currentUserRecord) => {            print('[Leaderboard] Global scores loaded');            print(              '[Leaderboard] Current user: ' +                currentUserRecord.snapchatUser.displayName +                ', Score: ' +                currentUserRecord.score            );            otherRecords.forEach((record) => {              if (                record &&                record.snapchatUser &&                record.snapchatUser.displayName              ) {                print(                  '[Leaderboard] User: ' +                    record.snapchatUser.displayName +                    ', Score: ' +                    record.score                );              }            });          },          (status) => {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);          }        );      },      (status) => {        print('[Leaderboard] getLeaderboard failed, status: ' + status);      }    );  }} @component @ component  export class LeaderBoardInfo extends BaseScriptComponent {  export   class   LeaderBoardInfo   extends   BaseScriptComponent   {    private leaderboardModule = require('LensStudio:LeaderboardModule');    private  leaderboardModule  =   require ( 'LensStudio:LeaderboardModule' ) ;      onAwake() {    onAwake ( )   {      this.updateLeaderboard();      this . updateLeaderboard ( ) ;    }    }      updateLeaderboard(): void {    updateLeaderboard ( ) :   void   {      const leaderboardCreateOptions = Leaderboard.CreateOptions.create();      const  leaderboardCreateOptions  =  Leaderboard . CreateOptions . create ( ) ;      leaderboardCreateOptions.name = 'TEST_NAME';     leaderboardCreateOptions . name  =   'TEST_NAME' ;      leaderboardCreateOptions.ttlSeconds = 800000;     leaderboardCreateOptions . ttlSeconds  =   800000 ;      leaderboardCreateOptions.orderingType = 1;     leaderboardCreateOptions . orderingType  =   1 ;        this.leaderboardModule.getLeaderboard(      this . leaderboardModule . getLeaderboard (        leaderboardCreateOptions,       leaderboardCreateOptions ,        (leaderboardInstance) => {        ( leaderboardInstance )   =>   {          let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();          let  leaderboardRetrievalOptions  =  Leaderboard . RetrievalOptions . create ( ) ;          leaderboardRetrievalOptions.usersLimit = 10;         leaderboardRetrievalOptions . usersLimit  =   10 ;          leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;         leaderboardRetrievalOptions . usersType  =  Leaderboard . UsersType . Global ;          leaderboardInstance.getLeaderboardInfo(         leaderboardInstance . getLeaderboardInfo (            leaderboardRetrievalOptions,           leaderboardRetrievalOptions ,            (otherRecords, currentUserRecord) => {            ( otherRecords ,  currentUserRecord )   =>   {              print('[Leaderboard] Global scores loaded');              print ( '[Leaderboard] Global scores loaded' ) ;              print(              print (                '[Leaderboard] Current user: ' +                '[Leaderboard] Current user: '   +                  currentUserRecord.snapchatUser.displayName +                 currentUserRecord . snapchatUser . displayName  +                  ', Score: ' +                  ', Score: '   +                  currentUserRecord.score                 currentUserRecord . score             );              ) ;                otherRecords.forEach((record) => {             otherRecords . forEach ( ( record )   =>   {                if (                if   (                  record &&                 record  &&                  record.snapchatUser &&                 record . snapchatUser  &&                  record.snapchatUser.displayName                 record . snapchatUser . displayName               ) {                )   {                  print(                  print (                    '[Leaderboard] User: ' +                    '[Leaderboard] User: '   +                      record.snapchatUser.displayName +                     record . snapchatUser . displayName  +                      ', Score: ' +                      ', Score: '   +                      record.score                     record . score                 );                  ) ;                }                }              });              } ) ;            },            } ,            (status) => {            ( status )   =>   {              print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);              print ( '[Leaderboard] getLeaderboardInfo failed, status: '   +  status ) ;            }            }          );          ) ;        },        } ,        (status) => {        ( status )   =>   {          print('[Leaderboard] getLeaderboard failed, status: ' + status);          print ( '[Leaderboard] getLeaderboard failed, status: '   +  status ) ;        }        }      );      ) ;    }    }  }  }   const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule');function updateLeaderboard() {  const leaderboardCreateOptions = Leaderboard.CreateOptions.create();  leaderboardCreateOptions.name = 'TEST_NAME';  leaderboardCreateOptions.ttlSeconds = 800000;  leaderboardCreateOptions.orderingType = 1;  leaderboardModule.getLeaderboard(    leaderboardCreateOptions,    (leaderboardInstance) => {      let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();      leaderboardRetrievalOptions.usersLimit = 10;      leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;      leaderboardInstance.getLeaderboardInfo(        leaderboardRetrievalOptions,        (otherRecords, currentUserRecord) => {          print('[Leaderboard] Global scores loaded');          print(            '[Leaderboard] Current user: ' +              currentUserRecord.snapchatUser.displayName +              ', Score: ' +              currentUserRecord.score          );          otherRecords.forEach((record) => {            if (              record &&              record.snapchatUser &&              record.snapchatUser.displayName            ) {              print(                '[Leaderboard] User: ' +                  record.snapchatUser.displayName +                  ', Score: ' +                  record.score              );            }          });        },        (status) => {          print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);        }      );    },    (status) => {      print('[Leaderboard] getLeaderboard failed, status: ' + status);    }  );}updateLeaderboard(); const leaderboardModule = require('LensStudio:LeaderboardModule'); const  leaderboardModule  =   require ( 'LensStudio:LeaderboardModule' ) ;    function updateLeaderboard() {  function   updateLeaderboard ( )   {    const leaderboardCreateOptions = Leaderboard.CreateOptions.create();    const  leaderboardCreateOptions  =   Leaderboard . CreateOptions . create ( ) ;    leaderboardCreateOptions.name = 'TEST_NAME';   leaderboardCreateOptions . name   =   'TEST_NAME' ;    leaderboardCreateOptions.ttlSeconds = 800000;   leaderboardCreateOptions . ttlSeconds   =   800000 ;    leaderboardCreateOptions.orderingType = 1;   leaderboardCreateOptions . orderingType   =   1 ;      leaderboardModule.getLeaderboard(   leaderboardModule . getLeaderboard (      leaderboardCreateOptions,     leaderboardCreateOptions ,      (leaderboardInstance) => {      ( leaderboardInstance )   =>   {        let leaderboardRetrievalOptions = Leaderboard.RetrievalOptions.create();        let  leaderboardRetrievalOptions  =   Leaderboard . RetrievalOptions . create ( ) ;        leaderboardRetrievalOptions.usersLimit = 10;       leaderboardRetrievalOptions . usersLimit   =   10 ;        leaderboardRetrievalOptions.usersType = Leaderboard.UsersType.Global;       leaderboardRetrievalOptions . usersType   =   Leaderboard . UsersType . Global ;          leaderboardInstance.getLeaderboardInfo(       leaderboardInstance . getLeaderboardInfo (          leaderboardRetrievalOptions,         leaderboardRetrievalOptions ,          (otherRecords, currentUserRecord) => {          ( otherRecords ,  currentUserRecord )   =>   {            print('[Leaderboard] Global scores loaded');            print ( '[Leaderboard] Global scores loaded' ) ;            print(            print (              '[Leaderboard] Current user: ' +              '[Leaderboard] Current user: '   +                currentUserRecord.snapchatUser.displayName +               currentUserRecord . snapchatUser . displayName   +                ', Score: ' +                ', Score: '   +                currentUserRecord.score               currentUserRecord . score            );            ) ;              otherRecords.forEach((record) => {           otherRecords . forEach ( ( record )   =>   {              if (              if   (                record &&               record  &&                record.snapchatUser &&               record . snapchatUser   &&                record.snapchatUser.displayName               record . snapchatUser . displayName              ) {              )   {                print(                print (                  '[Leaderboard] User: ' +                  '[Leaderboard] User: '   +                    record.snapchatUser.displayName +                   record . snapchatUser . displayName   +                    ', Score: ' +                    ', Score: '   +                    record.score                   record . score                );                ) ;              }              }            });            } ) ;          },          } ,          (status) => {          ( status )   =>   {            print('[Leaderboard] getLeaderboardInfo failed, status: ' + status);            print ( '[Leaderboard] getLeaderboardInfo failed, status: '   +  status ) ;          }          }        );        ) ;      },      } ,      (status) => {      ( status )   =>   {        print('[Leaderboard] getLeaderboard failed, status: ' + status);        print ( '[Leaderboard] getLeaderboard failed, status: '   +  status ) ;      }      }    );    ) ;  }  }    updateLeaderboard();  updateLeaderboard ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Keyboard Next Location UsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards UsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards UsagePrerequisitesCreating LeaderboardsSubmitting ScoresDisplaying Leaderboards Prerequisites Creating Leaderboards Submitting Scores Displaying Leaderboards AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/location": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeadingAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Spectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller ModuleOverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Spectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller Module Spectacles FeaturesAPIsLocationOn this pageCopy pageLocation\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\nWas this page helpful?YesNoPreviousLeaderboardNextMotion Controller Module  Spectacles Features Spectacles Features APIs APIs Location Location On this page Copy page  Copy page     page Location\nOverview\u200b\nSpectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API.\n\nTo use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later\nSetup Instructions\u200b\nTo access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation.\nrequire('LensStudio:RawLocationModule');\nLocation and Heading\u200b\nLocation can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate.\nMake sure the right accuracy for your use case is selected using GeoLocationAccuracy\nHeading accuracy improves as you walk around due to automatic calibration adjustments\nExample\u200b\nTypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();});\nLocation and Heading visualisation\u200b\nSpectacles provides one template to assist in getting started with location services.\nThe Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view.\n2D Map\u200b\nThe 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places.\nFor using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation.\nAR view\u200b\nThe AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template:\n\nNavigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position.\nLocation Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy.\n6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content.\nPins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service.\nAR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n\n\nThe specific implementation details can be found on the provided Outdoor Navigation template.\nKnown Limitations\u200b\nLocation\u200b\n\nIt may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection.\nNavigation accuracy mode performance might degrade at night time\n\nHeading\u200b\n\nAfter a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n\n Location Overview\u200b Spectacles grants access to the device\u2019s GPS coordinates, opening up a world of possibilities for location-based experiences with the Location API. To use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet.   To use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet. To use location services, users must be logged in and paired with their Snapchat account, and their location permission must be enabled. Users are also expected to be connected to the internet. Getting Started\u200b Prerequisites\u200b Lens Studio v5.7.0 or later\nSpectacles OS v5.60.000 or later Setup Instructions\u200b To access the location API on Spectacles, you need to declare your Lens permission to use the RawLocationModule. For more information on how to declare permissions, refer to the Permission Overview documentation. require('LensStudio:RawLocationModule'); require('LensStudio:RawLocationModule'); require('LensStudio:RawLocationModule'); require ( 'LensStudio:RawLocationModule' ) ;   Location and Heading\u200b Location can be retrieved through the Location Service API. You can retrieve the user's current position through getCurrentPosition, and their heading through onNorthAlignedOrientationUpdate. Make sure the right accuracy for your use case is selected using GeoLocationAccuracy Heading accuracy improves as you walk around due to automatic calibration adjustments   Heading accuracy improves as you walk around due to automatic calibration adjustments Heading accuracy improves as you walk around due to automatic calibration adjustments Example\u200b TypeScriptJavaScriptrequire('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();}); TypeScript JavaScript require('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }}require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();}); require('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }} require('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }} require('LensStudio:RawLocationModule');@componentexport class LocationExample extends BaseScriptComponent {  latitude: number;  longitude: number;  altitude: number;  horizontalAccuracy: number;  verticalAccuracy: number;  timestamp: Date;  locationSource: string;  private repeatUpdateUserLocation: DelayedCallbackEvent;  private locationService: LocationService;  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.createAndLogLocationAndHeading();    });    this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');    this.repeatUpdateUserLocation.bind(() => {      // Get users location.      this.locationService.getCurrentPosition(        function (geoPosition) {          //Check if location coordinates have been updated based on timestamp          if (            this.timestamp === undefined ||            this.timestamp.getTime() != geoPosition.timestamp.getTime()          ) {            this.latitude = geoPosition.latitude;            this.longitude = geoPosition.longitude;            this.horizontalAccuracy = geoPosition.horizontalAccuracy;            this.verticalAccuracy = geoPosition.verticalAccuracy;            print('long: ' + this.longitude);            print('lat: ' + this.latitude);            if (geoPosition.altitude != 0) {              this.altitude = geoPosition.altitude;              print('altitude: ' + this.altitude);            }            this.timestamp = geoPosition.timestamp;          }        },        function (error) {          print(error);        }      );      // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds      this.repeatUpdateUserLocation.reset(1.0);    });  }  createAndLogLocationAndHeading() {    // Create location handler    this.locationService = GeoLocation.createLocationService();    // Set the accuracy    this.locationService.accuracy = GeoLocationAccuracy.Navigation;    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      print('Heading orientation: ' + heading.toFixed(3));      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      print(        'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)      );    };    this.locationService.onNorthAlignedOrientationUpdate.add(      onOrientationUpdate    );    // Acquire next location immediately with zero delay    this.repeatUpdateUserLocation.reset(0.0);  }} require('LensStudio:RawLocationModule'); require ( 'LensStudio:RawLocationModule' ) ;    @component  @ component  export class LocationExample extends BaseScriptComponent {  export   class   LocationExample   extends   BaseScriptComponent   {    latitude: number;   latitude :   number ;    longitude: number;   longitude :   number ;    altitude: number;   altitude :   number ;    horizontalAccuracy: number;   horizontalAccuracy :   number ;    verticalAccuracy: number;   verticalAccuracy :   number ;    timestamp: Date;   timestamp :  Date ;    locationSource: string;   locationSource :   string ;      private repeatUpdateUserLocation: DelayedCallbackEvent;    private  repeatUpdateUserLocation :  DelayedCallbackEvent ;    private locationService: LocationService;    private  locationService :  LocationService ;    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.createAndLogLocationAndHeading();        this . createAndLogLocationAndHeading ( ) ;      });      } ) ;        this.repeatUpdateUserLocation = this.createEvent('DelayedCallbackEvent');      this . repeatUpdateUserLocation  =   this . createEvent ( 'DelayedCallbackEvent' ) ;      this.repeatUpdateUserLocation.bind(() => {      this . repeatUpdateUserLocation . bind ( ( )   =>   {        // Get users location.        // Get users location.        this.locationService.getCurrentPosition(        this . locationService . getCurrentPosition (          function (geoPosition) {          function   ( geoPosition )   {            //Check if location coordinates have been updated based on timestamp            //Check if location coordinates have been updated based on timestamp            if (            if   (              this.timestamp === undefined ||              this . timestamp  ===   undefined   ||              this.timestamp.getTime() != geoPosition.timestamp.getTime()              this . timestamp . getTime ( )   !=  geoPosition . timestamp . getTime ( )            ) {            )   {              this.latitude = geoPosition.latitude;              this . latitude  =  geoPosition . latitude ;              this.longitude = geoPosition.longitude;              this . longitude  =  geoPosition . longitude ;              this.horizontalAccuracy = geoPosition.horizontalAccuracy;              this . horizontalAccuracy  =  geoPosition . horizontalAccuracy ;              this.verticalAccuracy = geoPosition.verticalAccuracy;              this . verticalAccuracy  =  geoPosition . verticalAccuracy ;              print('long: ' + this.longitude);              print ( 'long: '   +   this . longitude ) ;              print('lat: ' + this.latitude);              print ( 'lat: '   +   this . latitude ) ;              if (geoPosition.altitude != 0) {              if   ( geoPosition . altitude  !=   0 )   {                this.altitude = geoPosition.altitude;                this . altitude  =  geoPosition . altitude ;                print('altitude: ' + this.altitude);                print ( 'altitude: '   +   this . altitude ) ;              }              }              this.timestamp = geoPosition.timestamp;              this . timestamp  =  geoPosition . timestamp ;            }            }          },          } ,          function (error) {          function   ( error )   {            print(error);            print ( error ) ;          }          }        );        ) ;        // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds        // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds        this.repeatUpdateUserLocation.reset(1.0);        this . repeatUpdateUserLocation . reset ( 1.0 ) ;      });      } ) ;    }    }    createAndLogLocationAndHeading() {    createAndLogLocationAndHeading ( )   {      // Create location handler      // Create location handler      this.locationService = GeoLocation.createLocationService();      this . locationService  =  GeoLocation . createLocationService ( ) ;        // Set the accuracy      // Set the accuracy      this.locationService.accuracy = GeoLocationAccuracy.Navigation;      this . locationService . accuracy  =  GeoLocationAccuracy . Navigation ;        // Acquire heading orientation updates      // Acquire heading orientation updates      var onOrientationUpdate = function (northAlignedOrientation) {      var   onOrientationUpdate   =   function   ( northAlignedOrientation )   {        //Providing 3DoF north aligned rotation in quaternion form        //Providing 3DoF north aligned rotation in quaternion form        let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);        let  heading  =  GeoLocation . getNorthAlignedHeading ( northAlignedOrientation ) ;        print('Heading orientation: ' + heading.toFixed(3));        print ( 'Heading orientation: '   +  heading . toFixed ( 3 ) ) ;        // Convert to a 2DoF rotation for plane rendering purposes        // Convert to a 2DoF rotation for plane rendering purposes        var rotation = (heading * Math.PI) / 180;        var  rotation  =   ( heading  *  Math . PI )   /   180 ;        print(        print (          'Screen transform rotation: ' + quat.fromEulerAngles(0, 0, rotation)          'Screen transform rotation: '   +  quat . fromEulerAngles ( 0 ,   0 ,  rotation )        );        ) ;      };      } ;      this.locationService.onNorthAlignedOrientationUpdate.add(      this . locationService . onNorthAlignedOrientationUpdate . add (        onOrientationUpdate       onOrientationUpdate     );      ) ;        // Acquire next location immediately with zero delay      // Acquire next location immediately with zero delay      this.repeatUpdateUserLocation.reset(0.0);      this . repeatUpdateUserLocation . reset ( 0.0 ) ;    }    }  }  }   require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();}); require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();}); require('LensStudio:RawLocationModule');let locationService = null;const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');repeatUpdateUserLocation.bind(() => {  // Get users location.  locationService.getCurrentPosition(    function (geoPosition) {      //Check if location coordinates have been updated based on timestamp      let geoPositionTimestampMs = geoPosition.timestamp.getTime();      if (timestampLastLocation != geoPositionTimestampMs) {        script.latitude = geoPosition.latitude;        script.longitude = geoPosition.longitude;        script.horizontalAccuracy = geoPosition.horizontalAccuracy;        script.verticalAccuracy = geoPosition.vertical;        print('lat: ' + geoPosition.latitude);        print('long: ' + geoPosition.longitude);        if (geoPosition.altitude != 0) {          script.altitude = geoPosition.altitude;          print('altitude: ' + geoPosition.altitude);        }        print('location source: ' + geoPosition.locationSource);        timestampLastLocation = geoPositionTimestampMs;      }    },    function (error) {      print(error);    }  );  // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds  repeatUpdateUserLocation.reset(1.0);});function createAndLogLocationAndHeading() {  // Create location handler  locationService = GeoLocation.createLocationService();  // Set the accuracy  locationService.accuracy = GeoLocationAccuracy.Navigation;  // Acquire heading orientation updates  var onOrientationUpdate = function (northAlignedOrientation) {    //Providing 3DoF north aligned rotation in quaternion form    let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);    print('Heading orientation: ' + heading.toFixed(3));    // Convert to a 2DoF rotation for plane rendering purposes    var rotation = (heading * Math.PI) / 180;    script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);  };  locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);  // Acquire next location immediately with zero delay  repeatUpdateUserLocation.reset(0.0);}script.createEvent('OnStartEvent').bind(() => {  createAndLogLocationAndHeading();}); require('LensStudio:RawLocationModule'); require ( 'LensStudio:RawLocationModule' ) ;    let locationService = null;  let  locationService  =   null ;    const repeatUpdateUserLocation = script.createEvent('DelayedCallbackEvent');  const  repeatUpdateUserLocation  =  script . createEvent ( 'DelayedCallbackEvent' ) ;  repeatUpdateUserLocation.bind(() => { repeatUpdateUserLocation . bind ( ( )   =>   {    // Get users location.    // Get users location.    locationService.getCurrentPosition(   locationService . getCurrentPosition (      function (geoPosition) {      function   ( geoPosition )   {        //Check if location coordinates have been updated based on timestamp        //Check if location coordinates have been updated based on timestamp        let geoPositionTimestampMs = geoPosition.timestamp.getTime();        let  geoPositionTimestampMs  =  geoPosition . timestamp . getTime ( ) ;        if (timestampLastLocation != geoPositionTimestampMs) {        if   ( timestampLastLocation  !=  geoPositionTimestampMs )   {          script.latitude = geoPosition.latitude;         script . latitude   =  geoPosition . latitude ;          script.longitude = geoPosition.longitude;         script . longitude   =  geoPosition . longitude ;          script.horizontalAccuracy = geoPosition.horizontalAccuracy;         script . horizontalAccuracy   =  geoPosition . horizontalAccuracy ;          script.verticalAccuracy = geoPosition.vertical;         script . verticalAccuracy   =  geoPosition . vertical ;          print('lat: ' + geoPosition.latitude);          print ( 'lat: '   +  geoPosition . latitude ) ;          print('long: ' + geoPosition.longitude);          print ( 'long: '   +  geoPosition . longitude ) ;          if (geoPosition.altitude != 0) {          if   ( geoPosition . altitude   !=   0 )   {            script.altitude = geoPosition.altitude;           script . altitude   =  geoPosition . altitude ;            print('altitude: ' + geoPosition.altitude);            print ( 'altitude: '   +  geoPosition . altitude ) ;          }          }          print('location source: ' + geoPosition.locationSource);          print ( 'location source: '   +  geoPosition . locationSource ) ;          timestampLastLocation = geoPositionTimestampMs;         timestampLastLocation  =  geoPositionTimestampMs ;        }        }      },      } ,      function (error) {      function   ( error )   {        print(error);        print ( error ) ;      }      }    );    ) ;    // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds    // Acquire next location update in 1 second, increase this value if required for AR visualisation purposes such as 0.5 or 0.1 seconds    repeatUpdateUserLocation.reset(1.0);   repeatUpdateUserLocation . reset ( 1.0 ) ;  });  } ) ;  function createAndLogLocationAndHeading() {  function   createAndLogLocationAndHeading ( )   {    // Create location handler    // Create location handler      locationService = GeoLocation.createLocationService();   locationService  =   GeoLocation . createLocationService ( ) ;      // Set the accuracy    // Set the accuracy    locationService.accuracy = GeoLocationAccuracy.Navigation;   locationService . accuracy   =   GeoLocationAccuracy . Navigation ;      // Acquire heading orientation updates    // Acquire heading orientation updates    var onOrientationUpdate = function (northAlignedOrientation) {    var   onOrientationUpdate   =   function   ( northAlignedOrientation )   {      //Providing 3DoF north aligned rotation in quaternion form      //Providing 3DoF north aligned rotation in quaternion form      let heading = GeoLocation.getNorthAlignedHeading(northAlignedOrientation);      let  heading  =   GeoLocation . getNorthAlignedHeading ( northAlignedOrientation ) ;      print('Heading orientation: ' + heading.toFixed(3));      print ( 'Heading orientation: '   +  heading . toFixed ( 3 ) ) ;      // Convert to a 2DoF rotation for plane rendering purposes      // Convert to a 2DoF rotation for plane rendering purposes      var rotation = (heading * Math.PI) / 180;      var  rotation  =   ( heading  *   Math . PI )   /   180 ;      script.screenTransform.rotation = quat.fromEulerAngles(0, 0, rotation);     script . screenTransform . rotation   =  quat . fromEulerAngles ( 0 ,   0 ,  rotation ) ;    };    } ;    locationService.onNorthAlignedOrientationUpdate.add(onOrientationUpdate);   locationService . onNorthAlignedOrientationUpdate . add ( onOrientationUpdate ) ;      // Acquire next location immediately with zero delay    // Acquire next location immediately with zero delay    repeatUpdateUserLocation.reset(0.0);   repeatUpdateUserLocation . reset ( 0.0 ) ;  }  }    script.createEvent('OnStartEvent').bind(() => { script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {    createAndLogLocationAndHeading();    createAndLogLocationAndHeading ( ) ;  });  } ) ;   Location and Heading visualisation\u200b Spectacles provides one template to assist in getting started with location services. The Outdoor Navigation Template serves as an example for providing directions using location and heading information. It describes how to visualise content in a 2D map or AR view. 2D Map\u200b The 2D map component provides functionality which includes zooming and scrolling of a 2D map, follow-me functionality, manual generation of pins, automatic points of interest generation using the Snap Places API, and simultaneous AR view content visualization. The Snap Places API allows developers to bring Snapchat map features into their lenses, enabling interaction with nearby places. For using the Snapchat Places API, Extended Permissions are required when combined with the Location Service for automatic pin generation. AR view\u200b The AR view provides a system for visualizing points of interest with AR functionality. Here is a breakdown of the components needed to enable this mode which are provided within the template: Navigation accuracy : Requires selecting Navigation accuracy level within the LocationService. This ensures that the AR features align accurately with the user's geographical position. Location Source : The AR view will benefit from utilizing a FUSED_LOCATION source, which combines various location signals for optimal accuracy and reliability. This is activated automatically when selecting Navigation accuracy. 6DoF Pose : Utilizes latitude, longitude, altitude, and north-aligned orientation to create a six degrees of freedom (6DoF) pose, allowing for precise rendering of AR content. Pins on 2D Map : Pins can be created manually by users or automatically populated using the Snap Places API. For the latter, this requires extended permissions and the use of an experimental API whenever combined with the Location Service. AR View Visuals :\n\nDisplays existing pins from the 2D map.\nProvides directional guidance via white arrows to point the user towards the points of interest.\nShows a red dot when a point of interest should be in view.\nDisplays the distance from the user to these points of interest.\n\n Displays existing pins from the 2D map. Provides directional guidance via white arrows to point the user towards the points of interest. Shows a red dot when a point of interest should be in view. Displays the distance from the user to these points of interest. The specific implementation details can be found on the provided Outdoor Navigation template. Known Limitations\u200b Location\u200b It may take a moment for the Lens to initialize and provide location data on the first run if there is no active internet connection. Navigation accuracy mode performance might degrade at night time Heading\u200b After a new SnapOS is installed, heading orientation can take a moment to initialise. This is due to a calibration process that occurs in the background which is invisible to the user. This calibration process can be accelerated using the steps below:\n\nJust after a SnapOS upgrade, wear your Spectacles with the display turned on.\nRotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions.\n\n Just after a SnapOS upgrade, wear your Spectacles with the display turned on. Rotate the device for about 20 seconds. This involves looking around, turning around, and looking up and down to cover several directions. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Leaderboard Next Motion Controller Module OverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading OverviewGetting StartedPrerequisitesSetup InstructionsLocation and HeadingExampleLocation and Heading visualisation2D MapAR viewKnown LimitationsLocationHeading Overview Getting StartedPrerequisitesSetup Instructions Prerequisites Setup Instructions Location and HeadingExample Example Location and Heading visualisation2D MapAR view 2D Map AR view Known LimitationsLocationHeading Location Heading AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/motion-controller": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic FeedbackAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Spectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial AnchorsOverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Spectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial Anchors Spectacles FeaturesAPIsMotion Controller ModuleOn this pageCopy pageMotion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description!Was this page helpful?YesNoPreviousLocationNextSpatial Anchors  Spectacles Features Spectacles Features APIs APIs Motion Controller Module Motion Controller Module On this page Copy page  Copy page     page Motion Controller Module\nOverview\u200b\n\nDevelopers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api.\nThis Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.\nMotion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device.\nExample\u200b\nLet\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions.\nPrerequisites\u200b\nPlease configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone.\nAdding a Motion Controller to Your Project\u200b\nYou can access the Motion Controller through the Motion Controller Module asset like this:\n//@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController();\nAlternatively, you can import it as a module using the require keyword:\nconst MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController();\nSimple Transform Controller\u200b\nCreate a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating.\n\nScale and move the child object as shown below.\n\nThen create a new TypeScript file in Asset Browser and paste the code:\nTypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);});\nSave the script and add it to the parent scene object we created above.\nJust like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle.\n\nAt this point you should see the object move and rotate with your phone!\nYou may find the complete Motion Controller Helper example in Lens Studio Asset Library!\nUsing Touch Events\u200b\nLet\u2019s expand our code by adding a couple lines :\n\nAdd another script input:\n\n   @input   childObject: SceneObject\n\nAdd an `onTouchEvent` function:\n\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;}\n\nAnd finally add an event callback in onAwake function\n\nthis.controller.onTouchEvent.add(this.onTouchEvent.bind(this))\nSend the lens to Spectacles and try tapping on the screen to toggle the object.\nThe Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.\nAdding Haptic Feedback\u200b\nMotion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it.\nFor this api please make sure you have Vibration and System Haptics enabled on your phone.\nLet\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled:\nonTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }}\nPlease check out the full Motion Controller API documentation for more code snippets and detailed functionality description! Motion Controller Module Overview\u200b Developers for Spectacles are able to build mobile controllers that are able to communicate to Spectacles that are linked to the mobile companion app Spectacles App. The following guide explains how to integrate a motion controller into a Lens Studio project and utilize its api. This Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously.   This Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously. This Motion Controller Module is designed to support a variety of 3rd party input devices in future. At the moment only the Spectacles App controller is supported and only one Motion Controller can be connected to Spectacles at a time. In the long future, we expect that there may be multiple Motion Controllers simultaneously. Motion Controller allows access to track world position and rotation of a mobile controller, track touch events and invoke haptic feedback on the mobile device. Example\u200b Let\u2019s build a simple example that would allow us to manipulate a 3D object in space and provide certain interactions. Prerequisites\u200b Please configure your Lens Studio project for Spectacles and connect your Spectacles device.\nAlso please have the Spectacles App installed on your phone. Adding a Motion Controller to Your Project\u200b You can access the Motion Controller through the Motion Controller Module asset like this: //@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController(); //@input Asset.MotionControllerModule motionControllerModuleconst motionController = script.motionControllerModule.getController(); //@input Asset.MotionControllerModule motionControllerModule //@input Asset.MotionControllerModule motionControllerModule const motionController = script.motionControllerModule.getController(); const motionController = script.motionControllerModule.getController();   Alternatively, you can import it as a module using the require keyword: const MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController(); const MotionControllerModule = require(\"LensStudio:MotionControllerModule\");const motionController = MotionControllerModule.getController(); const MotionControllerModule = require(\"LensStudio:MotionControllerModule\"); const MotionControllerModule = require(\"LensStudio:MotionControllerModule\"); const motionController = MotionControllerModule.getController(); const motionController = MotionControllerModule.getController();   Simple Transform Controller\u200b Create a new scene object and a new child object of your choice. This is the 3D object we\u2019ll be manipulating. Scale and move the child object as shown below. Then create a new TypeScript file in Asset Browser and paste the code: TypeScriptJavaScriptconst MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);}); TypeScript JavaScript const MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }}const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);}); const MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }} const MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }} const MotionControllerModule = require('LensStudio:MotionControllerModule');@componentexport class NewScript extends BaseScriptComponent {  private transform;  private controller;  onAwake() {    var options = MotionController.Options.create();    options.motionType = MotionController.MotionType.SixDoF;    this.controller = MotionControllerModule.getController(options);    this.transform = this.sceneObject.getTransform();    this.controller.onTransformEvent.add(this.updateTransform.bind(this));  }  updateTransform(position, rotation) {    this.transform.setWorldPosition(position);    this.transform.setWorldRotation(rotation);  }} const MotionControllerModule = require('LensStudio:MotionControllerModule'); const  MotionControllerModule  =   require ( 'LensStudio:MotionControllerModule' ) ;    @component  @ component  export class NewScript extends BaseScriptComponent {  export   class   NewScript   extends   BaseScriptComponent   {    private transform;    private  transform ;    private controller;    private  controller ;    onAwake() {    onAwake ( )   {      var options = MotionController.Options.create();      var  options  =  MotionController . Options . create ( ) ;      options.motionType = MotionController.MotionType.SixDoF;     options . motionType  =  MotionController . MotionType . SixDoF ;      this.controller = MotionControllerModule.getController(options);      this . controller  =  MotionControllerModule . getController ( options ) ;        this.transform = this.sceneObject.getTransform();      this . transform  =   this . sceneObject . getTransform ( ) ;      this.controller.onTransformEvent.add(this.updateTransform.bind(this));      this . controller . onTransformEvent . add ( this . updateTransform . bind ( this ) ) ;    }    }      updateTransform(position, rotation) {    updateTransform ( position ,  rotation )   {      this.transform.setWorldPosition(position);      this . transform . setWorldPosition ( position ) ;      this.transform.setWorldRotation(rotation);      this . transform . setWorldRotation ( rotation ) ;    }    }  }  }   const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);}); const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);}); const MotionControllerModule = require('LensStudio:MotionControllerModule');let options = MotionController.Options.create();options.motionType = MotionController.MotionType.SixDoF;const motionController = MotionControllerModule.getController(options);const sceneObject = script.getSceneObject();const transform = sceneObject.getTransform();motionController.onTransformEvent.add((worldPosition, worldRotation) => {  transform.setWorldPosition(worldPosition);  transform.setWorldRotation(worldRotation);}); const MotionControllerModule = require('LensStudio:MotionControllerModule'); const   MotionControllerModule   =   require ( 'LensStudio:MotionControllerModule' ) ;  let options = MotionController.Options.create();  let  options  =   MotionController . Options . create ( ) ;  options.motionType = MotionController.MotionType.SixDoF; options . motionType   =   MotionController . MotionType . SixDoF ;  const motionController = MotionControllerModule.getController(options);  const  motionController  =   MotionControllerModule . getController ( options ) ;    const sceneObject = script.getSceneObject();  const  sceneObject  =  script . getSceneObject ( ) ;  const transform = sceneObject.getTransform();  const  transform  =  sceneObject . getTransform ( ) ;    motionController.onTransformEvent.add((worldPosition, worldRotation) => { motionController . onTransformEvent . add ( ( worldPosition ,  worldRotation )   =>   {    transform.setWorldPosition(worldPosition);   transform . setWorldPosition ( worldPosition ) ;    transform.setWorldRotation(worldRotation);   transform . setWorldRotation ( worldRotation ) ;  });  } ) ;   Save the script and add it to the parent scene object we created above. Just like this the simplest example is ready. Click on the `Preview Lens` button and select `Send to Connected Spectacles`. Open the Spectacles App, go through the calibration step and enable the `Controller` toggle. At this point you should see the object move and rotate with your phone! You may find the complete Motion Controller Helper example in Lens Studio Asset Library!   You may find the complete Motion Controller Helper example in Lens Studio Asset Library! You may find the complete Motion Controller Helper example in Lens Studio Asset Library! Using Touch Events\u200b Let\u2019s expand our code by adding a couple lines : Add another script input:    @input   childObject: SceneObject    @input   childObject: SceneObject    @input     @ input     childObject: SceneObject    childObject :  SceneObject   Add an `onTouchEvent` function: onTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;} onTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;} onTouchEvent(normalizedPosition, touchId, timestampMs, phase) { onTouchEvent ( normalizedPosition ,  touchId ,  timestampMs ,  phase )   {      if (phase != MotionController.TouchPhase.Began) {      if   ( phase  !=  MotionController . TouchPhase . Began )   {          return          return      }      }      this.childObject.enabled = !this.childObject.enabled;      this . childObject . enabled  =   ! this . childObject . enabled ;  }  }   And finally add an event callback in onAwake function this.controller.onTouchEvent.add(this.onTouchEvent.bind(this)) this.controller.onTouchEvent.add(this.onTouchEvent.bind(this)) this.controller.onTouchEvent.add(this.onTouchEvent.bind(this)) this.controller.onTouchEvent.add(this.onTouchEvent.bind(this))   Send the lens to Spectacles and try tapping on the screen to toggle the object. The Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio.   The Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio. The Motion Controller also invokes usual script touch events such as TapEvent, TouchStartEvent, TouchMoveEvent and TouchEndEvent. Which means you can reutilize some of existing helper scripts and create touch interactions that also work in Lens Studio. Adding Haptic Feedback\u200b Motion Controller allows you to invoke the tactile response or physical sensations provided by a device when a user interacts with it. For this api please make sure you have Vibration and System Haptics enabled on your phone.   For this api please make sure you have Vibration and System Haptics enabled on your phone. For this api please make sure you have Vibration and System Haptics enabled on your phone. Let\u2019s expand an `onTouchEvent` function by adding vibration feedback whenever a 3D object is enabled: onTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }} onTouchEvent(normalizedPosition, touchId, timestampMs, phase) {    if (phase != MotionController.TouchPhase.Began) {        return    }    this.childObject.enabled = !this.childObject.enabled;    if (this.childObject.enabled) {        var request = MotionController.HapticRequest.create()        request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium        request.duration = 1.0        this.controller.invokeHaptic(request)    }} onTouchEvent(normalizedPosition, touchId, timestampMs, phase) { onTouchEvent ( normalizedPosition ,  touchId ,  timestampMs ,  phase )   {      if (phase != MotionController.TouchPhase.Began) {      if   ( phase  !=  MotionController . TouchPhase . Began )   {          return          return      }      }        this.childObject.enabled = !this.childObject.enabled;      this . childObject . enabled  =   ! this . childObject . enabled ;      if (this.childObject.enabled) {      if   ( this . childObject . enabled )   {          var request = MotionController.HapticRequest.create()          var  request  =  MotionController . HapticRequest . create ( )          request.hapticFeedback \\= MotionController.HapticFeedback.VibrationMedium         request . hapticFeedback \\ =  MotionController . HapticFeedback . VibrationMedium         request.duration = 1.0         request . duration  =   1.0          this.controller.invokeHaptic(request)          this . controller . invokeHaptic ( request )      }      }  }  }   Please check out the full Motion Controller API documentation for more code snippets and detailed functionality description! Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Location Next Spatial Anchors OverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback OverviewExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Overview ExamplePrerequisitesAdding a Motion Controller to Your ProjectSimple Transform ControllerUsing Touch EventsAdding Haptic Feedback Prerequisites Adding a Motion Controller to Your Project Simple Transform Controller Using Touch Events Adding Haptic Feedback AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/spatial-anchors": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponentAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Spectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial ImageOverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Spectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial Image Spectacles FeaturesAPIsSpatial AnchorsOn this pageCopy pageSpatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against.Was this page helpful?YesNoPreviousMotion Controller ModuleNextSpatial Image  Spectacles Features Spectacles Features APIs APIs Spatial Anchors Spatial Anchors On this page Copy page  Copy page     page Spatial Anchors\nOverview\u200b\n\nAnchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment.\nFor instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual.\nThe Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor.\nWorldAnchor is the only anchor type available today. Future releases will include more anchor types.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.6621 or later\n\nPackage Installation\u200b\n\nInstall and import the Spatial Anchors from the Asset Library.\nAfter installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nThis guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.\nAnchorModule Configuration\u200b\nThis guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components.\nThe AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area.\nAdd the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them:\n\nLocation Cloud Storage Module\nConnected Lens Module\n\n\nNow add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed:\n\nFinally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World.\n\nStarting an AnchorSession\u200b\nThe next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first.\nIn this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found!\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }}\nPlacing an Anchor\u200b\nThere are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor.\nExpanding from the example above:\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nIn the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab.\nScanning for Anchors\u200b\nAnchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user.\nYou can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.\nimport {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }}\nAdditional Concepts\u200b\nUpdating Anchors\u200b\nAnchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example:\nanchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);}\nFinally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor).\nCleaning Up\u200b\nWhen you're done, ensure you close the session to stop tracking:\nawait anchorSession.close();\nArea\u200b\nAn area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses.\nPersistence\u200b\nThe WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions.\nAPI Reference\u200b\nAnchorModule\u200b\nSignatureTypeDescriptionMETHODSopenSessionfunction (options: AnchorSessionOptions) \u2192 Promise<AnchorSession>Open an AnchorSession and begin scanning for anchors, given parameters in AnchorSessionOptions. The returned AnchorSession will manage the creation, recognition, and updating of anchors.\nAnchorSession\u200b\nThe AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor.\nSignatureTypeDescriptionMETHODSclosefunction () \u2192 Promise<void>End scanning. No further operations on this session.resetfunction () \u2192 Promise<void>Clear the session: e.g. forget previous anchors here. Resets the ability to track in the current area.updateAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Update the given anchor. Required to persist changes made to anchors (e.g. changing toWorldFromAnchor). Note: anchors are automatically saved upon creation, so this does not need to be called for new anchors to be persisted.deleteAnchorfunction (anchor: UserAnchor) \u2192 Promise<UserAnchor>Remove the given anchor. The anchor will no longer be loaded by scan.createWorldAnchorfunction (toWorldFromAnchor: mat4) \u2192 Promise<WorldAnchor>Create an anchor with the specified world pose. 'World' is the coordinate system of scene graph root, compatible with a child rendering camera positioned by DeviceTracking set to world.PROPERTIESareastringThe area to which anchors of this session are scoped.onAnchorNearbyevent \u2192 (anchor: Anchor)Callback invoked when scan has found a nearby anchor.\nAnchorSessionOptions\u200b\nSignatureTypeDescriptionMETHODScreate()Static functionPROPERTIESscanForWorldAnchorsboolTrue to scan for world anchors. Defaults to true.areastringIf supplied, scopes anchors returned to the specified area. Otherwise uses a default area.\nAnchor\u200b\nBase class for anchors.\nThe state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019]\nSignatureTypeDescriptionPROPERTIESidstringID of anchor is guaranteed to be unique within area. May be used by lenses to look up content they may have associated with the anchor.toWorldFromAnchormat4Pose of the anchor in world space. Only valid when state == Found.stateState enum { Initializing, Ready, Found, Lost, Error }Current tracking state.onFoundevent \u2192 ()Invoked when the anchor is found. At this point content can be associated with the anchor.onLostevent \u2192 ()Invoked when the anchor is lost; e.g. it is no longer reliably being tracked. Content should be removed.onReadyevent \u2192 ()We have all the information needed to track. Used, e.g., to end a \u2018loading\u2019 animation for this anchor. onError will never fire this session.onErrorevent \u2192 (error: Error)If fired, is not recoverable. onFound will never be called this lens session.\nWorldAnchor\u200b\nWorld anchors can be created by developers by specifying a pose for the anchor in world space.\nSignatureTypeDescriptionPROPERTIEStoWorldFromAnchormat4Pose of the anchor in world space.\nAnchorComponent\u200b\nComponent that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject.\nSignatureTypeDescriptionPROPERTIESanchorAnchorThe anchor that the parent SceneObject should track against. Spatial Anchors Overview\u200b Anchors in augmented reality enable the creation of immersive experiences by allowing users to seamlessly integrate virtual content with the real world. These anchors serve as fixed points that ensure digital elements remain in place as users move around their environment. For instance, in the realm of interior design, AR anchors can facilitate virtual furniture placement, enabling users to visualize how different pieces would look in their homes before making a purchase. Another practical application is in creating interactive instructional Lenses that overlay step-by-step guides directly on household appliances, making it easier for users to operate them without needing to consult a manual. The Anchors API exposes this functionality to developers. The AnchorModule is the entrypoint to the overall anchoring system, and the first anchor type that can be used is WorldAnchor. WorldAnchor is the only anchor type available today. Future releases will include more anchor types.   WorldAnchor is the only anchor type available today. Future releases will include more anchor types. WorldAnchor is the only anchor type available today. Future releases will include more anchor types. Getting Started\u200b Prerequisites\u200b Lens Studio v5.3.0 or later Spectacles OS v5.58.6621 or later Package Installation\u200b Install and import the Spatial Anchors from the Asset Library. After installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.   After installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation. After installing Spatial Anchors from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation. This guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package.   This guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package. This guide assumes that the scripts you create will be located in the same directory as the Spatial Anchors package. AnchorModule Configuration\u200b This guide will help you quickly integrate and manage augmented reality anchors using the AnchorModule, AnchorSession, and WorldAnchor components. The AnchorModule is the starting point for managing anchors. It allows you to create an AnchorSession within a specified area. Add the following Assets by clicking on the + sign in Asset Browser, searching for these components, and selecting them: Location Cloud Storage Module Connected Lens Module Now add AnchorsModule to the scene. Inside the Anchors package in Asset Browser, find AnchorModule and drag it into your Scene. Setup the AnchorModule by selecting it in your Scene and setting its dependencies using the modules we just installed: Finally, ensure your camera is properly set up. Select your camera in the Scene Hierarchy and add Device Tracking to it, and set the Device Tracking mode to World. Starting an AnchorSession\u200b The next step is to obtain an AnchorSession, which enables you to scan for and place anchors. Create a new script that takes AnchorModule as an input. Ensure your script appears after AnchorModule in the Scene Hierarchy to ensure AnchorModule is initialized first. In this example, we set up the onAnchorNearby callback to be invoked whenever a new Anchor is found. Note that we haven\u2019t placed any anchors yet, so none will be found! import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }} import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorModule } from './Spatial Anchors/AnchorModule';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }} import { import   {    AnchorSession,   AnchorSession ,    AnchorSessionOptions,   AnchorSessionOptions ,  } from './Spatial Anchors/AnchorSession';  }   from   './Spatial Anchors/AnchorSession' ;    import { Anchor } from './Spatial Anchors/Anchor';  import   {  Anchor  }   from   './Spatial Anchors/Anchor' ;  import { AnchorModule } from './Spatial Anchors/AnchorModule';  import   {  AnchorModule  }   from   './Spatial Anchors/AnchorModule' ;    @component  @ component  export class AnchorPlacementController extends BaseScriptComponent {  export   class   AnchorPlacementController   extends   BaseScriptComponent   {    @input anchorModule: AnchorModule;    @ input  anchorModule :  AnchorModule ;      private anchorSession?: AnchorSession;    private  anchorSession ? :  AnchorSession ;      async onAwake() {    async   onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      async onStart() {    async   onStart ( )   {      // Set up the AnchorSession options to scan for World Anchors      // Set up the AnchorSession options to scan for World Anchors      const anchorSessionOptions = new AnchorSessionOptions();      const  anchorSessionOptions  =   new   AnchorSessionOptions ( ) ;      anchorSessionOptions.scanForWorldAnchors = true;     anchorSessionOptions . scanForWorldAnchors  =   true ;        // Start scanning for anchors      // Start scanning for anchors      this.anchorSession =      this . anchorSession  =        await this.anchorModule.openSession(anchorSessionOptions);        await   this . anchorModule . openSession ( anchorSessionOptions ) ;        // Listen for anchors      // Listen for anchors      this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));      this . anchorSession . onAnchorNearby . add ( this . onAnchorNearby . bind ( this ) ) ;    }    }      public onAnchorNearby(anchor: Anchor) {    public   onAnchorNearby ( anchor :  Anchor )   {      // Invoked when a new Anchor is found      // Invoked when a new Anchor is found    }    }  }  }   Placing an Anchor\u200b There are many ways to place an anchor. We\u2019ll use a simple example where we enable the user to anchor a 3D cube in front of them by pinching a button. To create the button first install the SpectaclesInteractionKit and add a PinchButton to your scene. Wire up the PinchButton such that when it\u2019s pinched, a WorldAnchor is created and a cube is attached to the new anchor. Expanding from the example above: import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }} import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    // Invoked when a new Anchor is found  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }} import { import   {    AnchorSession,   AnchorSession ,    AnchorSessionOptions,   AnchorSessionOptions ,  } from './Spatial Anchors/AnchorSession';  }   from   './Spatial Anchors/AnchorSession' ;    import { Anchor } from './Spatial Anchors/Anchor';  import   {  Anchor  }   from   './Spatial Anchors/Anchor' ;  import { AnchorComponent } from './Spatial Anchors/AnchorComponent';  import   {  AnchorComponent  }   from   './Spatial Anchors/AnchorComponent' ;  import { AnchorModule } from './Spatial Anchors/AnchorModule';  import   {  AnchorModule  }   from   './Spatial Anchors/AnchorModule' ;  import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';  import   {  PinchButton  }   from   './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton' ;    @component  @ component  export class AnchorPlacementController extends BaseScriptComponent {  export   class   AnchorPlacementController   extends   BaseScriptComponent   {    @input anchorModule: AnchorModule;    @ input  anchorModule :  AnchorModule ;    @input createAnchorButton: PinchButton;    @ input  createAnchorButton :  PinchButton ;      @input camera: SceneObject;    @ input  camera :  SceneObject ;    @input prefab: ObjectPrefab;    @ input  prefab :  ObjectPrefab ;      private anchorSession?: AnchorSession;    private  anchorSession ? :  AnchorSession ;      async onAwake() {    async   onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      async onStart() {    async   onStart ( )   {      this.createAnchorButton.onButtonPinched.add(() => {      this . createAnchorButton . onButtonPinched . add ( ( )   =>   {        this.createAnchor();        this . createAnchor ( ) ;      });      } ) ;        // Set up the AnchorSession options to scan for World Anchors      // Set up the AnchorSession options to scan for World Anchors      const anchorSessionOptions = new AnchorSessionOptions();      const  anchorSessionOptions  =   new   AnchorSessionOptions ( ) ;      anchorSessionOptions.scanForWorldAnchors = true;     anchorSessionOptions . scanForWorldAnchors  =   true ;        // Start scanning for anchors      // Start scanning for anchors      this.anchorSession =      this . anchorSession  =        await this.anchorModule.openSession(anchorSessionOptions);        await   this . anchorModule . openSession ( anchorSessionOptions ) ;        // Listen for nearby anchors      // Listen for nearby anchors      this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));      this . anchorSession . onAnchorNearby . add ( this . onAnchorNearby . bind ( this ) ) ;    }    }      public onAnchorNearby(anchor: Anchor) {    public   onAnchorNearby ( anchor :  Anchor )   {      // Invoked when a new Anchor is found      // Invoked when a new Anchor is found    }    }      private async createAnchor() {    private   async   createAnchor ( )   {      // Compute the anchor position 5 units in front of user      // Compute the anchor position 5 units in front of user      let toWorldFromDevice = this.camera.getTransform().getWorldTransform();      let  toWorldFromDevice  =   this . camera . getTransform ( ) . getWorldTransform ( ) ;      let anchorPosition = toWorldFromDevice.mult(      let  anchorPosition  =  toWorldFromDevice . mult (        mat4.fromTranslation(new vec3(0, 0, -5))       mat4 . fromTranslation ( new   vec3 ( 0 ,   0 ,   - 5 ) )      );      ) ;        // Create the anchor      // Create the anchor      let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);      let  anchor  =   await   this . anchorSession . createWorldAnchor ( anchorPosition ) ;        // Create the object and attach it to the anchor      // Create the object and attach it to the anchor      this.attachNewObjectToAnchor(anchor);      this . attachNewObjectToAnchor ( anchor ) ;        // Save the anchor so it's loaded in future sessions      // Save the anchor so it's loaded in future sessions      try {      try   {        this.anchorSession.saveAnchor(anchor);        this . anchorSession . saveAnchor ( anchor ) ;      } catch (error) {      }   catch   ( error )   {        print('Error saving anchor: ' + error);        print ( 'Error saving anchor: '   +  error ) ;      }      }    }    }      private attachNewObjectToAnchor(anchor: Anchor) {    private   attachNewObjectToAnchor ( anchor :  Anchor )   {      // Create a new object from the prefab      // Create a new object from the prefab      let object: SceneObject = this.prefab.instantiate(this.getSceneObject());      let  object :  SceneObject  =   this . prefab . instantiate ( this . getSceneObject ( ) ) ;      object.setParent(this.getSceneObject());     object . setParent ( this . getSceneObject ( ) ) ;        // Associate the anchor with the object by adding an AnchorComponent to the      // Associate the anchor with the object by adding an AnchorComponent to the      // object and setting the anchor in the AnchorComponent.      // object and setting the anchor in the AnchorComponent.      let anchorComponent = object.createComponent(      let  anchorComponent  =  object . createComponent (        AnchorComponent.getTypeName()       AnchorComponent . getTypeName ( )      ) as AnchorComponent;      )   as  AnchorComponent ;      anchorComponent.anchor = anchor;     anchorComponent . anchor  =  anchor ;    }    }  }  }   In the code above, when the button is pinched a new anchor is created in front of the user (via createAnchor); then, an object is created and attached to said anchor. The object here can be any SceneObject; here we use an injected prefab. Scanning for Anchors\u200b Anchors can be saved so that when we return to a Lens those anchors are found again. The anchors will be recognized by the onAnchorNearby method. In the example above, we saved the anchor by calling this.anchorSession.saveAnchor(anchor). Because we attached an onAnchorNearby callback to our AnchorSession, that saved anchor will be recovered the next time we run the code and Spectacles successfully localizes the user. You can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects.   You can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects. You can save anchors, but we do not save the content associated with anchors. This means that if we want to recover the objects that we had associated with an anchor in a previous session, we have to recreate those objects. import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }} import {  AnchorSession,  AnchorSessionOptions,} from './Spatial Anchors/AnchorSession';import { Anchor } from './Spatial Anchors/Anchor';import { AnchorComponent } from './Spatial Anchors/AnchorComponent';import { AnchorModule } from './Spatial Anchors/AnchorModule';import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';@componentexport class AnchorPlacementController extends BaseScriptComponent {  @input anchorModule: AnchorModule;  @input createAnchorButton: PinchButton;  @input camera: SceneObject;  @input prefab: ObjectPrefab;  private anchorSession?: AnchorSession;  async onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  async onStart() {    this.createAnchorButton.onButtonPinched.add(() => {      this.createAnchor();    });    // Set up the AnchorSession options to scan for World Anchors    const anchorSessionOptions = new AnchorSessionOptions();    anchorSessionOptions.scanForWorldAnchors = true;    // Start scanning for anchors    this.anchorSession =      await this.anchorModule.openSession(anchorSessionOptions);    // Listen for nearby anchors    this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));  }  public onAnchorNearby(anchor: Anchor) {    print('Anchor found: ' + anchor.id);    this.attachNewObjectToAnchor(anchor);  }  private async createAnchor() {    // Compute the anchor position 5 units in front of user    let toWorldFromDevice = this.camera.getTransform().getWorldTransform();    let anchorPosition = toWorldFromDevice.mult(      mat4.fromTranslation(new vec3(0, 0, -5))    );    // Create the anchor    let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);    // Create the object and attach it to the anchor    this.attachNewObjectToAnchor(anchor);    // Save the anchor so it's loaded in future sessions    try {      this.anchorSession.saveAnchor(anchor);    } catch (error) {      print('Error saving anchor: ' + error);    }  }  private attachNewObjectToAnchor(anchor: Anchor) {    // Create a new object from the prefab    let object: SceneObject = this.prefab.instantiate(this.getSceneObject());    object.setParent(this.getSceneObject());    // Associate the anchor with the object by adding an AnchorComponent to the    // object and setting the anchor in the AnchorComponent.    let anchorComponent = object.createComponent(      AnchorComponent.getTypeName()    ) as AnchorComponent;    anchorComponent.anchor = anchor;  }} import { import   {    AnchorSession,   AnchorSession ,    AnchorSessionOptions,   AnchorSessionOptions ,  } from './Spatial Anchors/AnchorSession';  }   from   './Spatial Anchors/AnchorSession' ;    import { Anchor } from './Spatial Anchors/Anchor';  import   {  Anchor  }   from   './Spatial Anchors/Anchor' ;  import { AnchorComponent } from './Spatial Anchors/AnchorComponent';  import   {  AnchorComponent  }   from   './Spatial Anchors/AnchorComponent' ;  import { AnchorModule } from './Spatial Anchors/AnchorModule';  import   {  AnchorModule  }   from   './Spatial Anchors/AnchorModule' ;  import { PinchButton } from './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton';  import   {  PinchButton  }   from   './SpectaclesInteractionKit/Components/UI/PinchButton/PinchButton' ;    @component  @ component  export class AnchorPlacementController extends BaseScriptComponent {  export   class   AnchorPlacementController   extends   BaseScriptComponent   {    @input anchorModule: AnchorModule;    @ input  anchorModule :  AnchorModule ;    @input createAnchorButton: PinchButton;    @ input  createAnchorButton :  PinchButton ;      @input camera: SceneObject;    @ input  camera :  SceneObject ;    @input prefab: ObjectPrefab;    @ input  prefab :  ObjectPrefab ;      private anchorSession?: AnchorSession;    private  anchorSession ? :  AnchorSession ;      async onAwake() {    async   onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      async onStart() {    async   onStart ( )   {      this.createAnchorButton.onButtonPinched.add(() => {      this . createAnchorButton . onButtonPinched . add ( ( )   =>   {        this.createAnchor();        this . createAnchor ( ) ;      });      } ) ;        // Set up the AnchorSession options to scan for World Anchors      // Set up the AnchorSession options to scan for World Anchors      const anchorSessionOptions = new AnchorSessionOptions();      const  anchorSessionOptions  =   new   AnchorSessionOptions ( ) ;      anchorSessionOptions.scanForWorldAnchors = true;     anchorSessionOptions . scanForWorldAnchors  =   true ;        // Start scanning for anchors      // Start scanning for anchors      this.anchorSession =      this . anchorSession  =        await this.anchorModule.openSession(anchorSessionOptions);        await   this . anchorModule . openSession ( anchorSessionOptions ) ;        // Listen for nearby anchors      // Listen for nearby anchors      this.anchorSession.onAnchorNearby.add(this.onAnchorNearby.bind(this));      this . anchorSession . onAnchorNearby . add ( this . onAnchorNearby . bind ( this ) ) ;    }    }      public onAnchorNearby(anchor: Anchor) {    public   onAnchorNearby ( anchor :  Anchor )   {      print('Anchor found: ' + anchor.id);      print ( 'Anchor found: '   +  anchor . id ) ;      this.attachNewObjectToAnchor(anchor);      this . attachNewObjectToAnchor ( anchor ) ;    }    }      private async createAnchor() {    private   async   createAnchor ( )   {      // Compute the anchor position 5 units in front of user      // Compute the anchor position 5 units in front of user      let toWorldFromDevice = this.camera.getTransform().getWorldTransform();      let  toWorldFromDevice  =   this . camera . getTransform ( ) . getWorldTransform ( ) ;      let anchorPosition = toWorldFromDevice.mult(      let  anchorPosition  =  toWorldFromDevice . mult (        mat4.fromTranslation(new vec3(0, 0, -5))       mat4 . fromTranslation ( new   vec3 ( 0 ,   0 ,   - 5 ) )      );      ) ;        // Create the anchor      // Create the anchor      let anchor = await this.anchorSession.createWorldAnchor(anchorPosition);      let  anchor  =   await   this . anchorSession . createWorldAnchor ( anchorPosition ) ;        // Create the object and attach it to the anchor      // Create the object and attach it to the anchor      this.attachNewObjectToAnchor(anchor);      this . attachNewObjectToAnchor ( anchor ) ;        // Save the anchor so it's loaded in future sessions      // Save the anchor so it's loaded in future sessions      try {      try   {        this.anchorSession.saveAnchor(anchor);        this . anchorSession . saveAnchor ( anchor ) ;      } catch (error) {      }   catch   ( error )   {        print('Error saving anchor: ' + error);        print ( 'Error saving anchor: '   +  error ) ;      }      }    }    }      private attachNewObjectToAnchor(anchor: Anchor) {    private   attachNewObjectToAnchor ( anchor :  Anchor )   {      // Create a new object from the prefab      // Create a new object from the prefab      let object: SceneObject = this.prefab.instantiate(this.getSceneObject());      let  object :  SceneObject  =   this . prefab . instantiate ( this . getSceneObject ( ) ) ;      object.setParent(this.getSceneObject());     object . setParent ( this . getSceneObject ( ) ) ;        // Associate the anchor with the object by adding an AnchorComponent to the      // Associate the anchor with the object by adding an AnchorComponent to the      // object and setting the anchor in the AnchorComponent.      // object and setting the anchor in the AnchorComponent.      let anchorComponent = object.createComponent(      let  anchorComponent  =  object . createComponent (        AnchorComponent.getTypeName()       AnchorComponent . getTypeName ( )      ) as AnchorComponent;      )   as  AnchorComponent ;      anchorComponent.anchor = anchor;     anchorComponent . anchor  =  anchor ;    }    }  }  }   Additional Concepts\u200b Updating Anchors\u200b Anchors can be updated or deleted. To change the pose (position and rotation) of a WorldAnchor, for example: anchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);} anchor.toWorldFromAnchor = pose; // Set to some new mat4 posetry {  this.anchorSession.saveAnchor(anchor);} catch (error) {  print('Error saving anchor: ' + error);} anchor.toWorldFromAnchor = pose; // Set to some new mat4 pose anchor . toWorldFromAnchor  =  pose ;   // Set to some new mat4 pose  try {  try   {    this.anchorSession.saveAnchor(anchor);    this . anchorSession . saveAnchor ( anchor ) ;  } catch (error) {  }   catch   ( error )   {    print('Error saving anchor: ' + error);    print ( 'Error saving anchor: '   +  error ) ;  }  }   Finally, deletion is accomplished by invoking anchorSession.deleteAnchor(anchor). Cleaning Up\u200b When you're done, ensure you close the session to stop tracking: await anchorSession.close(); await anchorSession.close(); await anchorSession.close(); await  anchorSession . close ( ) ;   Area\u200b An area is the scope for persistent anchors using a user-supplied string, such as \"living-room.\" When users create anchors within this area, those anchors will automatically be restored the next time they return. Each area is specific to a particular lens, ensuring no interaction occurs between areas with the same name across different lenses. Persistence\u200b The WorldAnchor has the capability to persist across sessions. When users create these anchors, they are saved automatically. You can update these anchors using updateAnchor or remove them with removeAnchor in the AnchorModule. It's important to remember that while anchors persist, the associated content does not. Include logic to re-associate content with the anchor when it is restored. For instance, if a user attaches virtual weather information to an anchor at their front door, ensure that this information is re-attached when the anchor is restored in future sessions. API Reference\u200b AnchorModule\u200b AnchorSession\u200b The AnchorSession fires events upon the recognition of anchors. When an anchor is recognized, it is handed to you, allowing you to associate it with a SceneObject using the AnchorComponent. Anchors managed by the AnchorModule are divided into two categories: those recognized and provided by the system (SystemAnchor) and those modifiable by the user (UserAnchor). In this release, one anchor type is introduced: WorldAnchor, which derives from UserAnchor. AnchorSessionOptions\u200b Anchor\u200b Base class for anchors. The state moves from <initializing>\n\u2192 (ready / error) [once per lens session]\n\u2192 (found / not found) [potentially many times per \u2018ready\u2019] WorldAnchor\u200b World anchors can be created by developers by specifying a pose for the anchor in world space. AnchorComponent\u200b Component that ties virtual content (attached to a SceneObject) to the physical world (via an Anchor`). More technically, this is a Component that applies a pose supplied from an Anchor to a SceneObject. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Motion Controller Module Next Spatial Image OverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent OverviewGetting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for AnchorsAdditional ConceptsUpdating AnchorsCleaning UpAreaPersistenceAPI ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent Overview Getting StartedPrerequisitesPackage InstallationAnchorModule ConfigurationStarting an AnchorSessionPlacing an AnchorScanning for Anchors Prerequisites Package Installation AnchorModule Configuration Starting an AnchorSession Placing an Anchor Scanning for Anchors Additional ConceptsUpdating AnchorsCleaning UpAreaPersistence Updating Anchors Cleaning Up Area Persistence API ReferenceAnchorModuleAnchorSessionAnchorSessionOptionsAnchorWorldAnchorAnchorComponent AnchorModule AnchorSession AnchorSessionOptions Anchor WorldAnchor AnchorComponent AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/spatial-image": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshootingAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Spectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocketOverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Spectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocket Spectacles FeaturesAPIsSpatial ImageOn this pageCopy pageSpatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection.Was this page helpful?YesNoPreviousSpatial AnchorsNextWebSocket  Spectacles Features Spectacles Features APIs APIs Spatial Image Spatial Image On this page Copy page  Copy page     page Spatial Image\nOverview\u200b\n\nUploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene.\nOnly one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio 5.3.0 or above\nSpectacles OS v5.58.6621 or later\n[Optional] Spectacles Interaction Kit\n\nInitial Setup\u200b\nThere are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library.\nSample Project\u200b\nThis sample project is available on Lens Studio Home Page.\n\nCustom Component Installation\u200b\nSetup your project so that it is built for Spectacles and your simulation environment is set to Spectacles\nOnce you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section.\n\nSample Project Scene Setup\u200b\n\nThe Spatial Image Gallery sample project includes three main scene objects:\n\nSpectaclesInteractionKit: Drives the interactions.\nSikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image.\nSpatialGallery: Enables browsing and navigating through multiple images.\n\nSample Project Scripts\u200b\nThis sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference\nSpatial Image Angle Validator\u200b\nThe spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed:\n\nThe first script tracks the camera's position relative to the Spatial Image.\nThe second script reduces the depth scale, effectively flattening the image back to its original texture.\n\nOther scripts can track these transitions by registering callbacks via the provided functions.\n  /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }\nThe boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle.\n\nvalidZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely.\nvalidZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects.\nvalidZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones.\n\n\nSpatial Image Depth Animator\u200b\nSpatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect.\nThe setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array.\nif (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower);\n\nThe maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.\nSpatial Gallery\u200b\nWith the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.\n  private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }\nSpatial Image Swapper\u200b\nSpatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set.\nSpatial Image Frame\u200b\nThe Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image.\nThe key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image.\nAdditionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated.\nA small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.\n  /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }\nWhen an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.\n  private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }\n\nAPI Reference\u200b\nLoadingIndicator\u200b\nThis script fills a loading indicator to represent progress while a task is completed.\nSignatureTypeDescriptionMETHODSresetfunction () \u2192 voidResets the progression to 0.PROPERTIEScheckProgressingdelegate () \u2192 booleanAllows custom start and stop functions to be added to the indicator.\nSpatialGallery\u200b\nProvides a somewhat complex example of use of the spatial image components.\nSignatureTypeDescriptionMETHODSleftPressedfunction () \u2192 voidMoves the gallery's focus to the next image.rightPressedfunction () \u2192 voidMoves the gallery's focus to the previous image.PROPERTIESframeSpatialImageFrameThe SIK container frame that holds the image.imageSpatialImageThe spatial image custom component.loadingIndicatorLoadingIndicatorThe loading indicator to tell that the image is being spatialized.galleryTexture[]The set of images that make up the gallery.shufflebooleamIf true the order of the gallery will be shuffled on initialization.\nSpatialImageAngleValidator\u200b\nTracks the users point of view and emits events on whether they are viewing from a valid angle or not.\nSignatureTypeDescriptionMETHODSsetValidZoneFocalfunction (focal: number) \u2192 voidSets the focal point of the valid zone. This allows the user to move their head around in front of the image without it being considered an extreme angle.setValidZoneAnglefunction (angle: number) \u2192 voidSets the angle, in degrees, at which the angle is considered valid.addOnValidityCallbackfunction( callback (entered: boolean) => void)Add a callback to onValidityCallbacks, to be called when the image is fully loaded.removeOnValidityCallbackfunction( callback (entered: boolean) => void)Remove a callback from the onValidityCallbacks.PROPERTIESvalidZoneFocalnumberA focal point, set behind the image, where the angle is measured from.validZoneAnglenumberThe angular range, in degrees, where no flattening is applied.validZoneThresholdnumberThe threshold, in degrees, which must be exceeded when moving between the dead zone.\nSpatialImageDepthAnimator\u200b\nControls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles.\nSignatureTypeDescriptionMETHODSsetBaseDepthScalefunction (depth: number) \u2192 voidSets the maximum depth scale for the image.PROPERTIESanimateSpeednumberThe speed at which the depth value is changed.\nSpatialImageFrame\u200b\nReorders rendering order to ensure the image looks correct.\nSignatureTypeDescriptionMETHODSsetSpatializedfunction (value: boolean) \u2192 voidToggle call to switch between specialized and flat images.setImagefunction (image: Texture, swapWhenSpatialized: boolean = false) \u2192 voidUpdates both spatialized and flat images to the passed texture.\nSpatialImageSwapper\u200b\nResponsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers.\nSignatureTypeDescriptionMETHODSsetImagefunction (image: Texture) \u2192 voidSets the texture of the flat version of the image.setSpatializedfunction (spatialized: boolean) \u2192 voidIf true, the spatialized image will be displayed and the depth animated in.\nTroubleshooting\u200b\nWhy is the spatial image's background cut off?\u200b\nTry increasing the far clipping plane of the camera.\nWhy doesn't the spatial image up in the lens studio preview?\u200b\nMake sure the preview device is set to Spectacles and check internet connection. Spatial Image Overview\u200b Uploads a normal photo and gets back a 3D spatialised mesh that can be observed within a scene. Only one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns.   Only one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns. Only one image is able to be spatialized at a time. The Spatial Component will work to prioritize images that are in the scene and active. Having many spatialization requests at the same time can result in delays to returns. Getting Started\u200b Prerequisites\u200b Lens Studio 5.3.0 or above Spectacles OS v5.58.6621 or later [Optional] Spectacles Interaction Kit Initial Setup\u200b There are two ways to get you started building with Spatial Image. One, you can use the sample project that is provided in Lens Studio or you can install the custom component through the Asset Library. This sample project is available on Lens Studio Home Page. Setup your project so that it is built for Spectacles and your simulation environment is set to Spectacles Once you have your environment setup, install the Spatial Image custom component through the Asset Library section under the Spectacles section. Sample Project Scene Setup\u200b The Spatial Image Gallery sample project includes three main scene objects: SpectaclesInteractionKit: Drives the interactions. SikSpatialImageFrame: Extends the container frame and allows manipulation of the spatial image. SpatialGallery: Enables browsing and navigating through multiple images. Sample Project Scripts\u200b This sample project includes several scripts. Below is an overview of their main functionalities and interactions. For more details, open the scripts in your preferred IDE or refer to API Reference Spatial Image Angle Validator\u200b The spatial image angle validator monitors the camera's position and emits events to determine if the viewing angle is optimal. In this sample project, these events are utilized by the SpatialImageDepthAnimator. Viewing images from extreme angles can reveal defects, which is generally undesirable. To address this, two scripts are employed: The first script tracks the camera's position relative to the Spatial Image. The second script reduces the depth scale, effectively flattening the image back to its original texture. Other scripts can track these transitions by registering callbacks via the provided functions.   /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }   /**   * Add a callback to onValidityCallbacks, to be called when the image is fully loaded   * @param callback - the callback to add   */  public addOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks.push(callback)  }  /**   * Remove a callback from the onValidityCallbacks.   * @param callback - the callback to remove   */  public removeOnValidityCallback(callback: (entered: boolean) => void): void {    this.onValidityCallbacks = this.onValidityCallbacks.filter(      (cb) => cb !== callback    )  }   /**    /**    * Add a callback to onValidityCallbacks, to be called when the image is fully loaded    * Add a callback to onValidityCallbacks, to be called when the image is fully loaded    * @param callback - the callback to add    *  @param   callback  - the callback to add    */    */    public addOnValidityCallback(callback: (entered: boolean) => void): void {    public   addOnValidityCallback ( callback :   ( entered :  boolean )   =>   void ) :   void   {      this.onValidityCallbacks.push(callback)      this . onValidityCallbacks . push ( callback )    }    }      /**    /**    * Remove a callback from the onValidityCallbacks.    * Remove a callback from the onValidityCallbacks.    * @param callback - the callback to remove    *  @param   callback  - the callback to remove    */    */    public removeOnValidityCallback(callback: (entered: boolean) => void): void {    public   removeOnValidityCallback ( callback :   ( entered :  boolean )   =>   void ) :   void   {      this.onValidityCallbacks = this.onValidityCallbacks.filter(      this . onValidityCallbacks   =   this . onValidityCallbacks . filter (        (cb) => cb !== callback        ( cb )   =>  cb  !==  callback     )      )    }    }   The boundaries of the zone are controlled by two variables in the inspector: validZoneFocal and validZoneAngle. validZoneFocal: Controls the distance of a point projected behind the image, which is used to compute the angle between the user's camera and the forward direction of the image. This allows the user to approach the image closely. validZoneAngle: Specifies the range of the valid zone in degrees. The default values are validZoneFocal set to 2 and validZoneAngle set to 25, but these can be adjusted to create different viewing effects. validZoneThreshold: Defines a small angle offset to prevent the user from frequently switching between valid and invalid zones. Spatial Image Depth Animator\u200b Spatial Image Depth Animator adjusts the \"depth scale\" of the spatialized image, setting the desired depth. It works with the angle validator to reduce depth to a minimum when the viewing angle is invalid. When a new image is applied, it animates from a minimum depth scale to a user-specified maximum to showcase the effect. The setMaxDepthScale function transitions the image between a flattened state and a fully spatialized state using an easing function. This makes the depth change feel more natural. You can create new visual effects by replacing the \"ease-in-out-sine\" function with other functions from the easing functions array. if (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower); if (Math.abs(distance) > 0.01) {  this.depthFlattenFollower =    this.depthFlattenFollower +    Math.sign(distance) * getDeltaTime() * this.animateSpeed;}const easedAngle = easingFunctions['ease-in-out-sine'](  this.depthFlattenFollower); if (Math.abs(distance) > 0.01) { if   ( Math . abs ( distance )   >   0.01 )   {    this.depthFlattenFollower =    this . depthFlattenFollower   =      this.depthFlattenFollower +      this . depthFlattenFollower   +      Math.sign(distance) * getDeltaTime() * this.animateSpeed;      Math . sign ( distance )   *   getDeltaTime ( )   *   this . animateSpeed ;  }  }    const easedAngle = easingFunctions['ease-in-out-sine'](  const  easedAngle  =  easingFunctions [ 'ease-in-out-sine' ] (    this.depthFlattenFollower    this . depthFlattenFollower  );  ) ;   The maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is.   The maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is. The maximum depth scale for an image can be edited with the function SpatialImageDepthAnimator.setBaseDepthScale. This can be useful for changing how \"3D\" the image is. Spatial Gallery\u200b With the gallery scene set up, you can open the SpatialGallery script to understand how this example works. Spatial Gallery demonstrates a basic use of the spatializer. Images, represented as Texture, are organized in a list and spatialized using SpatialImageFrame.setImage, which is called in the setIndex function.   private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }   private setIndex(newIndex: number) {    this.index = newIndex    this.frame.setImage(this.gallery[this.index], true)  }   private setIndex(newIndex: number) {    private   setIndex ( newIndex :  number )   {      this.index = newIndex      this . index   =  newIndex     this.frame.setImage(this.gallery[this.index], true)      this . frame . setImage ( this . gallery [ this . index ] ,   true )    }    }   Spatial Image Swapper\u200b Spatial Image Swapper manages the transition between a flat image and a spatialized image. It references both the flat image and the spatializer, ensuring the flat image is replaced with the spatialized version. Additionally, it updates the scale of the flat image to render correctly within the frame when the texture is set. Spatial Image Frame\u200b The Spatial Image Frame acts as the manager, it handles requests to spatialize images, updates the flat image, swaps the images when the spatialized version is ready, and ensures the SIK frame renders correctly without clipping or interfering with either image. The key function here is setImage, which integrates the display components. It first adjusts the SIK container size to match the aspect ratio of the provided image. Additionally, you can subscribe to the spatial image's onLoaded callback. This ensures that once the image is spatialized, the flat image is hidden, leaving only the 3D image visible. The image texture is passed to both the spatializer and the swapper to keep everything updated. A small timeout is included at the bottom to ensure the first image displayed has the correct aspect ratio. This allows the frame to initialize properly before reading from it.   /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }   /**   * Updates both spatialized and flat images to the passed texture.   */  public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    // update the size of the container to match the dimensions of the new image.    const height: number = this.container.innerSize.y    const newWidth: number = height * (image.getWidth() / image.getHeight())    this.updateContainerSize(new vec2(newWidth, height))    // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.    if (swapWhenSpatialized) {      const setSpatialCallback = () => {        this.setSpatialized(true)        this.spatializer.onLoaded.remove(setSpatialCallback)      }      this.spatializer.onLoaded.add(setSpatialCallback)    }    // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.    this.spatialImageSwapper.setImage(image)    this.spatialImageSwapper.setSpatialized(false)    this.spatializer.setImage(image)    // A work around to the initialization of the scene    setTimeout(() => {      this.updateContainerSize(new vec2(newWidth, height))    }, 100)  }   /**    /**    * Updates both spatialized and flat images to the passed texture.    * Updates both spatialized and flat images to the passed texture.    */    */    public setImage(image: Texture, swapWhenSpatialized: boolean = false): void {    public   setImage ( image :   Texture ,   swapWhenSpatialized :  boolean  =   false ) :   void   {      // update the size of the container to match the dimensions of the new image.      // update the size of the container to match the dimensions of the new image.      const height: number = this.container.innerSize.y      const   height :  number  =   this . container . innerSize . y      const newWidth: number = height * (image.getWidth() / image.getHeight())      const   newWidth :  number  =  height  *   ( image . getWidth ( )   /  image . getHeight ( ) )      this.updateContainerSize(new vec2(newWidth, height))      this . updateContainerSize ( new   vec2 ( newWidth ,  height ) )        // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.      // if this argument is true, then when the \"onLoaded\" event is actuated, this component should update to display the spatialized image.      if (swapWhenSpatialized) {      if   ( swapWhenSpatialized )   {        const setSpatialCallback = () => {        const   setSpatialCallback   =   ( )   =>   {          this.setSpatialized(true)          this . setSpatialized ( true )          this.spatializer.onLoaded.remove(setSpatialCallback)          this . spatializer . onLoaded . remove ( setSpatialCallback )        }        }        this.spatializer.onLoaded.add(setSpatialCallback)        this . spatializer . onLoaded . add ( setSpatialCallback )      }      }        // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.      // The swapper is passed a reference to the new flat image and set to be unspatialized until the spatialization result comes through.      this.spatialImageSwapper.setImage(image)      this . spatialImageSwapper . setImage ( image )      this.spatialImageSwapper.setSpatialized(false)      this . spatialImageSwapper . setSpatialized ( false )      this.spatializer.setImage(image)      this . spatializer . setImage ( image )        // A work around to the initialization of the scene      // A work around to the initialization of the scene      setTimeout(() => {      setTimeout ( ( )   =>   {        this.updateContainerSize(new vec2(newWidth, height))        this . updateContainerSize ( new   vec2 ( newWidth ,  height ) )      }, 100)      } ,   100 )    }    }   When an image is \"picked up\" with pinching, users are able to move the image closer or further away from them quite rapidly. Doing this can produce a strange effect on the observer as their window into the spatialized world appears to warp in depth. To counter this, the depth animator adjusts the frame offset of the spatialized image.   private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }   private setFocalPoint() {    const cameraPosition = this.camera.getTransform().getWorldPosition()    const imagePos = this.spatializer.getTransform().getWorldPosition()    const distance = cameraPosition.distance(imagePos)    this.spatializer.setFrameOffset(-distance)  }   private setFocalPoint() {    private   setFocalPoint ( )   {      const cameraPosition = this.camera.getTransform().getWorldPosition()      const  cameraPosition  =   this . camera . getTransform ( ) . getWorldPosition ( )      const imagePos = this.spatializer.getTransform().getWorldPosition()      const  imagePos  =   this . spatializer . getTransform ( ) . getWorldPosition ( )      const distance = cameraPosition.distance(imagePos)      const  distance  =  cameraPosition . distance ( imagePos )      this.spatializer.setFrameOffset(-distance)      this . spatializer . setFrameOffset ( - distance )    }    }   API Reference\u200b LoadingIndicator\u200b This script fills a loading indicator to represent progress while a task is completed. SpatialGallery\u200b Provides a somewhat complex example of use of the spatial image components. SpatialImageAngleValidator\u200b Tracks the users point of view and emits events on whether they are viewing from a valid angle or not. SpatialImageDepthAnimator\u200b Controls the depth scale of the spatial image to reflect the entry of new images as well as ensure it's viewed only from correct angles. SpatialImageFrame\u200b Reorders rendering order to ensure the image looks correct. SpatialImageSwapper\u200b Responsible to change the active scene object between a flat version and the spatialized version when the onLoaded event triggers. Troubleshooting\u200b Try increasing the far clipping plane of the camera. Make sure the preview device is set to Spectacles and check internet connection. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Spatial Anchors Next WebSocket OverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting OverviewGetting StartedPrerequisitesInitial SetupSample Project Scene SetupSample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image FrameAPI ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapperTroubleshooting Overview Getting StartedPrerequisitesInitial Setup Prerequisites Initial Setup Sample Project Scene Setup Sample Project ScriptsSpatial Image Angle ValidatorSpatial Image Depth AnimatorSpatial GallerySpatial Image SwapperSpatial Image Frame Spatial Image Angle Validator Spatial Image Depth Animator Spatial Gallery Spatial Image Swapper Spatial Image Frame API ReferenceLoadingIndicatorSpatialGallerySpatialImageAngleValidatorSpatialImageDepthAnimatorSpatialImageFrameSpatialImageSwapper LoadingIndicator SpatialGallery SpatialImageAngleValidator SpatialImageDepthAnimator SpatialImageFrame SpatialImageSwapper Troubleshooting AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/web-socket": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown LimitationsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Spectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb ViewOverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Spectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb View Spectacles FeaturesAPIsWebSocketOn this pageCopy pageWebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\nWas this page helpful?YesNoPreviousSpatial ImageNextWeb View  Spectacles Features Spectacles Features APIs APIs WebSocket WebSocket On this page Copy page  Copy page     page WebSocket\nOverview\u200b\nSpectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference.\nAccessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.\nGetting Started\u200b\nPrerequisites\u200b\nLens Studio v5.4.0 or later\nSpectacles OS v5.059 or later\nThis API is only available on Spectacles.\nSetup Instructions\u200b\nTo use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below.\nThe WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.\nCore Component\u200b\nThe RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url.\nThe WebSocket will only connect to URLs with a wss prefix.\nYou can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions:\nTypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};\nKnown Limitations\u200b\nThe WebSocket API supports all methods in the standard, with the following exceptions:\n\n\nThe Blob type does not yet support ArrayBuffer or Stream.\n\n\nWebSocket.binaryType does not support arrayBuffer.\n\n\nThe extensions, protocol and bufferedAmount properties are not supported.\n\n WebSocket Overview\u200b Spectacles offers the standardized WebSocket API to connect to real-time streams on the internet. This API is based on the MDN reference. Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information.   Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information. Accessing the internet in a Lens will disable access to privacy-sensitive user information in that Lens, such as the camera frame, location, and audio. For testing and experimental purposes however, extended permissions are available to access both the camera frame and the open internet at the same time. Note that lenses built this way may not be released publicly. Please see Extended Permissions doc for more information. Getting Started\u200b Prerequisites\u200b Lens Studio v5.4.0 or later\nSpectacles OS v5.059 or later This API is only available on Spectacles.   This API is only available on Spectacles. This API is only available on Spectacles. Setup Instructions\u200b To use the WebSocket API add the RemoteServiceModule to your project and include it in your scripts as per the examples below. The WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles.   The WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles. The WebSocket API will only work in the Preview window if the Device Type Override is set to Spectacles. Core Component\u200b The RemoteServiceModule exposes the WebSocket API. To connect to a WebSocket server, use the createWebSocket command with a wss url. The WebSocket will only connect to URLs with a wss prefix.   The WebSocket will only connect to URLs with a wss prefix. The WebSocket will only connect to URLs with a wss prefix. You can utilize the returned WebSocket object to manage the connection. For instance, you can monitor for a successful connection through the onopen property, handle incoming messages using the onmessage property, and send messages with the send method. The example below demonstrates how to perform all these actions: TypeScriptJavaScript@componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");}; TypeScript JavaScript @componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }}//@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");}; @componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }} @componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }} @componentexport class WebSocketExample extends BaseScriptComponent {  @input  remoteServiceModule: RemoteServiceModule;  // Method called when the script is awake  async onAwake() {    let socket: WebSocket =      this.remoteServiceModule.createWebSocket('wss://<some-url>');    socket.binaryType = 'blob';    // Listen for the open event    socket.onopen = (event: WebSocketEvent) => {      // Socket has opened, send a message back to the server      socket.send('Message 1');      // Try sending a binary message      // (the bytes below spell 'Message 2')      const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const bytes = new Uint8Array(message);      socket.send(bytes);    };    // Listen for messages    socket.onmessage = async (event: WebSocketMessageEvent) => {      if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        const bytes = await event.data.bytes();        const text = await event.data.text();        print('Received binary message, printing as text: ' + text);      } else {        // Text frame        const text: string = event.data;        print('Received text message: ' + text);      }    };    socket.onclose = (event: WebSocketCloseEvent) => {      if (event.wasClean) {        print('Socket closed cleanly');      } else {        print('Socket closed with error, code: ' + event.code);      }    };    socket.onerror = (event: WebSocketErrorEvent) => {      print('Socket error');    };  }} @component @ component  export class WebSocketExample extends BaseScriptComponent {  export   class   WebSocketExample   extends   BaseScriptComponent   {    @input    @ input    remoteServiceModule: RemoteServiceModule;   remoteServiceModule :  RemoteServiceModule ;      // Method called when the script is awake    // Method called when the script is awake    async onAwake() {    async   onAwake ( )   {      let socket: WebSocket =      let  socket :  WebSocket  =        this.remoteServiceModule.createWebSocket('wss://<some-url>');        this . remoteServiceModule . createWebSocket ( 'wss://<some-url>' ) ;      socket.binaryType = 'blob';     socket . binaryType  =   'blob' ;        // Listen for the open event      // Listen for the open event      socket.onopen = (event: WebSocketEvent) => {     socket . onopen   =   ( event :  WebSocketEvent )   =>   {        // Socket has opened, send a message back to the server        // Socket has opened, send a message back to the server        socket.send('Message 1');       socket . send ( 'Message 1' ) ;          // Try sending a binary message        // Try sending a binary message        // (the bytes below spell 'Message 2')        // (the bytes below spell 'Message 2')        const message: number[] = [77, 101, 115, 115, 97, 103, 101, 32, 50];        const  message :   number [ ]   =   [ 77 ,   101 ,   115 ,   115 ,   97 ,   103 ,   101 ,   32 ,   50 ] ;        const bytes = new Uint8Array(message);        const  bytes  =   new   Uint8Array ( message ) ;        socket.send(bytes);       socket . send ( bytes ) ;      };      } ;        // Listen for messages      // Listen for messages      socket.onmessage = async (event: WebSocketMessageEvent) => {     socket . onmessage   =   async   ( event :  WebSocketMessageEvent )   =>   {        if (event.data instanceof Blob) {        if   ( event . data  instanceof   Blob )   {          // Binary frame, can be retrieved as either Uint8Array or string          // Binary frame, can be retrieved as either Uint8Array or string          const bytes = await event.data.bytes();          const  bytes  =   await  event . data . bytes ( ) ;          const text = await event.data.text();          const  text  =   await  event . data . text ( ) ;            print('Received binary message, printing as text: ' + text);          print ( 'Received binary message, printing as text: '   +  text ) ;        } else {        }   else   {          // Text frame          // Text frame          const text: string = event.data;          const  text :   string   =  event . data ;          print('Received text message: ' + text);          print ( 'Received text message: '   +  text ) ;        }        }      };      } ;      socket.onclose = (event: WebSocketCloseEvent) => {     socket . onclose   =   ( event :  WebSocketCloseEvent )   =>   {        if (event.wasClean) {        if   ( event . wasClean )   {          print('Socket closed cleanly');          print ( 'Socket closed cleanly' ) ;        } else {        }   else   {          print('Socket closed with error, code: ' + event.code);          print ( 'Socket closed with error, code: '   +  event . code ) ;        }        }      };      } ;        socket.onerror = (event: WebSocketErrorEvent) => {     socket . onerror   =   ( event :  WebSocketErrorEvent )   =>   {        print('Socket error');        print ( 'Socket error' ) ;      };      } ;    }    }  }  }   //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");}; //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");}; //@input Asset.RemoteServiceModule remoteServiceModule/** @type {RemoteServiceModule} */var remoteServiceModule = script.remoteServiceModule;let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");socket.binaryType = \"blob\";// Listen for the open eventsocket.onopen = (event) => {    // Socket has opened, send a message back to the server    socket.send(\"Message 1\");    // Try sending a binary message    // (the bytes below spell 'Message 2')    const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];    const bytes = new Uint8Array(message);    socket.send(bytes);};// Listen for messagessocket.onmessage = async (event) => {    if (event.data instanceof Blob) {        // Binary frame, can be retrieved as either Uint8Array or string        let bytes = await event.data.bytes();        let text = await event.data.text();        print(\"Received binary message, printing as text: \" + text);    } else {        // Text frame        let text = event.data;        print(\"Received text message: \" + text);    }});socket.onclose = (event) => {    if (event.wasClean) {        print(\"Socket closed cleanly\");    } else {        print(\"Socket closed with error, code: \" + event.code);    }};socket.onerror = (event) => {    print(\"Socket error\");};   //@input Asset.RemoteServiceModule remoteServiceModule  //@input Asset.RemoteServiceModule remoteServiceModule  /** @type {RemoteServiceModule} */  /**  @type   { RemoteServiceModule }  */  var remoteServiceModule = script.remoteServiceModule;  var  remoteServiceModule  =  script . remoteServiceModule ;    let socket = remoteServiceModule.createWebSocket(\"wss://<some-url>\");  let  socket  =  remoteServiceModule . createWebSocket ( \"wss://<some-url>\" ) ;  socket.binaryType = \"blob\"; socket . binaryType   =   \"blob\" ;    // Listen for the open event  // Listen for the open event  socket.onopen = (event) => { socket . onopen   =   ( event )   =>   {      // Socket has opened, send a message back to the server      // Socket has opened, send a message back to the server      socket.send(\"Message 1\");     socket . send ( \"Message 1\" ) ;        // Try sending a binary message      // Try sending a binary message      // (the bytes below spell 'Message 2')      // (the bytes below spell 'Message 2')      const message = [77, 101, 115, 115, 97, 103, 101, 32, 50];      const  message  =   [ 77 ,   101 ,   115 ,   115 ,   97 ,   103 ,   101 ,   32 ,   50 ] ;      const bytes = new Uint8Array(message);      const  bytes  =   new   Uint8Array ( message ) ;      socket.send(bytes);     socket . send ( bytes ) ;  };  } ;    // Listen for messages  // Listen for messages  socket.onmessage = async (event) => { socket . onmessage   =   async   ( event )   =>   {      if (event.data instanceof Blob) {      if   ( event . data   instanceof   Blob )   {          // Binary frame, can be retrieved as either Uint8Array or string          // Binary frame, can be retrieved as either Uint8Array or string          let bytes = await event.data.bytes();          let  bytes  =   await  event . data . bytes ( ) ;          let text = await event.data.text();          let  text  =   await  event . data . text ( ) ;            print(\"Received binary message, printing as text: \" + text);          print ( \"Received binary message, printing as text: \"   +  text ) ;      } else {      }   else   {          // Text frame          // Text frame          let text = event.data;          let  text  =  event . data ;          print(\"Received text message: \" + text);          print ( \"Received text message: \"   +  text ) ;      }      }  });  } ) ;    socket.onclose = (event) => { socket . onclose   =   ( event )   =>   {      if (event.wasClean) {      if   ( event . wasClean )   {          print(\"Socket closed cleanly\");          print ( \"Socket closed cleanly\" ) ;      } else {      }   else   {          print(\"Socket closed with error, code: \" + event.code);          print ( \"Socket closed with error, code: \"   +  event . code ) ;      }      }  };  } ;    socket.onerror = (event) => { socket . onerror   =   ( event )   =>   {      print(\"Socket error\");      print ( \"Socket error\" ) ;  };  } ;      Known Limitations\u200b The WebSocket API supports all methods in the standard, with the following exceptions: \nThe Blob type does not yet support ArrayBuffer or Stream.\n The Blob type does not yet support ArrayBuffer or Stream. \nWebSocket.binaryType does not support arrayBuffer.\n WebSocket.binaryType does not support arrayBuffer. \nThe extensions, protocol and bufferedAmount properties are not supported.\n The extensions, protocol and bufferedAmount properties are not supported. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Spatial Image Next Web View OverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations OverviewGetting StartedPrerequisitesSetup InstructionsCore ComponentKnown Limitations Overview Getting StartedPrerequisitesSetup Instructions Prerequisites Setup Instructions Core Component Known Limitations AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/web-view": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render ScaleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Spectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query ModuleOverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Spectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query Module Spectacles FeaturesAPIsWeb ViewOn this pageCopy pageWeb View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\nWas this page helpful?YesNoPreviousWebSocketNextWorld Query Module  Spectacles Features Spectacles Features APIs APIs Web View Web View On this page Copy page  Copy page     page Web View\nOverview\u200b\n\nThe WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input.\nWebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible.\nAt this time only HTTPS content is supported.\nCamera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.\nUse Cases\u200b\nSome common use cases for the WebView script could be:\n\nLinking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n\nAdd videos from websites such as YouTube\nOpen an existing Web Application\n\nAt this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.\nA Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.\nAt this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.\nGetting Started\u200b\nPrerequisites\u200b\n\nLens Studio v5.3.0 or later\nSpectacles OS v5.58.621 or later\n\nInitial Setup\u200b\nIt is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script.\nWebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information.\nCreate a new Spectacles Lens Project\u200b\n\nSelect the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed.\nHaving the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.\nProject Settings\u200b\n\nWebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs.\n\nInstall WebView Package\u200b\n\nInstall and import the WebView from the Asset Library.\nAfter installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.\nPackage Setup\u200b\nAfter installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing.\n\nSetup Scene Hierarchy\u200b\n\n\nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n\n\nOpen Scene Object in Inspector view.\n\n\nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n\n\nUnder the WebView settings the following can be changed:\n\n\nURL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime.\nResolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization.\nUser-Agent: If a custom user-agent is required it can be set here or at runtime through the script api.\nInclude Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting.\n\nWebView Render Scale\u200b\nEven though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene.\nThe WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane.\n\n\nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n\n\nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n\n\nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n\n Web View Overview\u200b The WebView script allows creators to host web content within their lens. Combined with Spectacles Interaction Kit (SIK) users will be able to fully interact with the web content through a touch interface as well as mobile keyboard for text input. WebView\u2019s are a way to embed web content and not a fully fledged browser. This means that while most web capabilities are supported there are cases where compatibility might not be possible. At this time only HTTPS content is supported.   At this time only HTTPS content is supported. At this time only HTTPS content is supported. Camera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported.   Camera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported. Camera access is not supported for web content.\nMicrophone access is not supported for web content.\nIf you open a website that requests camera or microphone access it will not be supported. Use Cases\u200b Some common use cases for the WebView script could be: Linking to existing content for your Lens\n\nWhat\u2019s new page for your lens\nContact us page\nEmbedded videos\nHelp documentation\n\n What\u2019s new page for your lens Contact us page Embedded videos Help documentation Add videos from websites such as YouTube Open an existing Web Application At this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself.   At this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself. At this time all web content must be hosted externally. WebView script does not allow for loading HTML content from the Lens itself. A Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot.   A Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot. A Lens can create multiple WebView\u2019s but the platform may suspend a webview to free up resources. A suspended WebView can be resumed by the user by interacting with it. This will cause a page reload. When a WebView is suspended, the last rendered version of the page is displayed as a snapshot. At this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio.   At this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio. At this time, WebView is only supported on the Spectacles Device. It is not supported on Snapchat or Lens Studio. Getting Started\u200b Prerequisites\u200b Lens Studio v5.3.0 or later Spectacles OS v5.58.621 or later Initial Setup\u200b It is strongly recommended that creators start with the WebView from the Asset Library. This handles integration with the Spectacles Interaction Kit as well as ensuring that the WebView life cycle events are correctly handled. This guide will cover usage with SIK + WebView script. WebView has a dependency on SIK but SIK is not required to be in the Scene Hierarchy if no input is necessary in which case a simplified version of the WebView script could be used instead. Please refer to the API documentation for more information. Select the Spectacles template from the new project window. This will ensure that the project has a default Spectacles Interaction Kit installed. Having the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples.   Having the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples. Having the Spectacles Interaction Kit Examples is optional. If you are already familiar with SIK you can remove the examples. WebView is still an experimental feature and is subject to change. This means that the experimental flag is necessary in order to access the APIs. Install WebView Package\u200b Install and import the WebView from the Asset Library. After installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation.   After installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation. After installing WebView from the Asset Library, you will need to import it into your project. For more information, please refer to the documentation. Package Setup\u200b After installing the WebView package, you need to perform a few additional steps. Right-click on the WebView package and select Unpack for Editing. Setup Scene Hierarchy\u200b \nCreate a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit.\n Create a new empty Scene Object. Make sure the Scene Object is under the hiearchy of Spectacles Interaction Kit. \nOpen Scene Object in Inspector view.\n Open Scene Object in Inspector view. \nDrag and drop the WebView Script from the Asset Browser to SceneObject.\n\n Drag and drop the WebView Script from the Asset Browser to SceneObject. Under the WebView settings the following can be changed: URL: If the desired URL is known or fixed, it can be specified here. Otherwise it can be left blank and set through scripts at runtime. Resolution: The desired web page resolution in pixels. This is separate from the Spectacles display resolution. This must be set before initialization. The maximum resolution supported is 2048x2048. This can not be changed after initialization. User-Agent: If a custom user-agent is required it can be set here or at runtime through the script api. Include Poke: WebView relies on indirect targeting using hand tracking and cursor. It can also support direct targeting using finger tracking when this setting is enabled. This does not disable indirect targeting. WebView Render Scale\u200b Even though the specified resolution is used to render the Web Content to a texture. The output size must still be configured in the Scene Hierarchy. In the WebView script a RenderMeshVisual is used to render the Texture onto. This means that the Transform Scale can be used to adjust the visual size in the scene. The WebPage Resolution aspect ratio must match the Scale aspect ratio in the XY plane. \nIn the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio.\n\n In the following example, the WebPage Resolution is set to 600x800, which is a 0.75 ratio. \nNext set the scale to match this ratio. In the above example 60x80 is used for the XY plane.\nA unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.\n Next set the scale to match this ratio. In the above example 60x80 is used for the XY plane. A unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space.   A unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space. A unit in Lens Studio results in cm. This results in 60cm x 80cm WebView in AR space. \nIn most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly.\n In most cases using values based on the 100cm scale will give a comfortable render size. As long as the aspect ratio is correct, any magnitude of scale will function correctly. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous WebSocket Next World Query Module OverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale OverviewUse CasesGetting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale OverviewUse Cases Use Cases Getting StartedPrerequisitesInitial SetupInstall WebView PackagePackage SetupSetup Scene HierarchyWebView Render Scale Prerequisites Initial Setup Install WebView Package Package Setup Setup Scene Hierarchy WebView Render Scale AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/apis/world-query": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query ModuleFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIsCamera ModuleCustom LocationsGesture ModuleInternet AccessKeyboardLeaderboardLocationMotion Controller ModuleSpatial AnchorsSpatial ImageWebSocketWeb ViewWorld Query Module APIs Camera Module Custom Locations Gesture Module Internet Access Keyboard Leaderboard Location Motion Controller Module Spatial Anchors Spatial Image WebSocket Web View World Query Module Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIs Spectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeaturesOverviewExampleAdditional ResourcesRelevant APIs Spectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeatures Spectacles FeaturesAPIsWorld Query ModuleOn this pageCopy pageWorld Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\nWas this page helpful?YesNoPreviousWeb ViewNextFeatures  Spectacles Features Spectacles Features APIs APIs World Query Module World Query Module On this page Copy page  Copy page     page World Query Module\nOverview\u200b\nWhen developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight.\nExisting APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits.\n\nDue to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.\nExample\u200b\nFollow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API.\n\nConfigure your project for Spectacles and enable Interactive Preview.\nImport and initialize Spectacles Interaction Kit.\nCreate a new TypeScript file and include the following code:\n\n// import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }}\n\n\nSave the script and add it to the scene.\n\n\nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n\n\nTest this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object).\nThis example demonstrates the simplest method for spawning objects on a surface.\nCheck out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!\nAdditional Resources\u200b\nRelevant APIs\u200b\n\nWorldQueryModule\nHitTestSession\nHitTestSessionOptions\nWorldQueryHitTestResult\n World Query Module Overview\u200b When developing lenses for Spectacles, you may need to place an object on a surface instantaneously. This method aims to be accurate and lightweight. Existing APIs such as hitTestWorldMesh, Physics.Probe.raycast, or DepthTexture.sampleDepthAtPoint provide accurate results but are often computationally heavy or not designed for wearable devices. The World Query Hit Test from the World Query Module performs a hit test for real surfaces, sampling the depth and normal at a specific location. If the hit test is executed, the internal algorithm computes a depth map for the current view and tries to intersect the ray with the depth to determine the point and surface normal. If the ray is outside the field of view of the depth map, the hit test fails and returns null. The API provides additional options to smooth out results across multiple hits. Due to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly.   Due to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly. Due to the low, 5Hz update rate of the underlying depth data, the hit test works only for static or slowly moving objects. Consider use cases accordingly. Example\u200b Follow these steps to sample depth and normal of a ray to place an object on real-world geometry using the WorldQueryHit API. Configure your project for Spectacles and enable Interactive Preview. Import and initialize Spectacles Interaction Kit. Create a new TypeScript file and include the following code: // import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }} // import required modulesconst WorldQueryModule = require('LensStudio:WorldQueryModule');const SIK = require('SpectaclesInteractionKit/SIK').SIK;const InteractorTriggerType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;const InteractorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;const EPSILON = 0.01;@componentexport class NewScript extends BaseScriptComponent {  private primaryInteractor;  private hitTestSession;  private transform: Transform;  @input  targetObject: SceneObject;  @input  filterEnabled: boolean;  onAwake() {    // create new hit session    this.hitTestSession = this.createHitTestSession(this.filterEnabled);    if (!this.sceneObject) {      print('Please set Target Object input');      return;    }    this.transform = this.targetObject.getTransform();    // disable target object when surface is not detected    this.targetObject.enabled = false;    // create update event    this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));  }  createHitTestSession(filterEnabled) {    // create hit test session with options    var options = HitTestSessionOptions.create();    options.filter = filterEnabled;    var session = WorldQueryModule.createHitTestSessionWithOptions(options);    return session;  }  onHitTestResult(results) {    if (results === null) {      this.targetObject.enabled = false;    } else {      this.targetObject.enabled = true;      // get hit information      const hitPosition = results.position;      const hitNormal = results.normal;      //identifying the direction the object should look at based on the normal of the hit location.      var lookDirection;      if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        lookDirection = vec3.forward();      } else {        lookDirection = hitNormal.cross(vec3.up());      }      const toRotation = quat.lookAt(lookDirection, hitNormal);      //set position and rotation      this.targetObject.getTransform().setWorldPosition(hitPosition);      this.targetObject.getTransform().setWorldRotation(toRotation);      if (        this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&        this.primaryInteractor.currentTrigger === InteractorTriggerType.None      ) {        // Called when a trigger ends        // Copy the plane/axis object        this.sceneObject.copyWholeHierarchy(this.targetObject);      }    }  }  onUpdate() {    this.primaryInteractor =      SIK.InteractionManager.getTargetingInteractors().shift();    if (      this.primaryInteractor &&      this.primaryInteractor.isActive() &&      this.primaryInteractor.isTargeting()    ) {      const rayStartOffset = new vec3(        this.primaryInteractor.startPoint.x,        this.primaryInteractor.startPoint.y,        this.primaryInteractor.startPoint.z + 30      );      const rayStart = rayStartOffset;      const rayEnd = this.primaryInteractor.endPoint;      this.hitTestSession.hitTest(        rayStart,        rayEnd,        this.onHitTestResult.bind(this)      );    } else {      this.targetObject.enabled = false;    }  }} // import required modules // import required modules  const WorldQueryModule = require('LensStudio:WorldQueryModule');  const  WorldQueryModule  =   require ( 'LensStudio:WorldQueryModule' ) ;  const SIK = require('SpectaclesInteractionKit/SIK').SIK;  const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const InteractorTriggerType =  const  InteractorTriggerType  =    require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorTriggerType;    require ( 'SpectaclesInteractionKit/Core/Interactor/Interactor' ) . InteractorTriggerType ;  const InteractorInputType =  const  InteractorInputType  =    require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;    require ( 'SpectaclesInteractionKit/Core/Interactor/Interactor' ) . InteractorInputType ;  const EPSILON = 0.01;  const   EPSILON   =   0.01 ;    @component  @ component  export class NewScript extends BaseScriptComponent {  export   class   NewScript   extends   BaseScriptComponent   {    private primaryInteractor;    private  primaryInteractor ;    private hitTestSession;    private  hitTestSession ;    private transform: Transform;    private  transform :  Transform ;      @input    @ input    targetObject: SceneObject;   targetObject :  SceneObject ;      @input    @ input    filterEnabled: boolean;   filterEnabled :   boolean ;      onAwake() {    onAwake ( )   {      // create new hit session      // create new hit session      this.hitTestSession = this.createHitTestSession(this.filterEnabled);      this . hitTestSession  =   this . createHitTestSession ( this . filterEnabled ) ;      if (!this.sceneObject) {      if   ( ! this . sceneObject )   {        print('Please set Target Object input');        print ( 'Please set Target Object input' ) ;        return;        return ;      }      }      this.transform = this.targetObject.getTransform();      this . transform  =   this . targetObject . getTransform ( ) ;      // disable target object when surface is not detected      // disable target object when surface is not detected      this.targetObject.enabled = false;      this . targetObject . enabled  =   false ;      // create update event      // create update event      this.createEvent('UpdateEvent').bind(this.onUpdate.bind(this));      this . createEvent ( 'UpdateEvent' ) . bind ( this . onUpdate . bind ( this ) ) ;    }    }      createHitTestSession(filterEnabled) {    createHitTestSession ( filterEnabled )   {      // create hit test session with options      // create hit test session with options      var options = HitTestSessionOptions.create();      var  options  =  HitTestSessionOptions . create ( ) ;      options.filter = filterEnabled;     options . filter  =  filterEnabled ;        var session = WorldQueryModule.createHitTestSessionWithOptions(options);      var  session  =  WorldQueryModule . createHitTestSessionWithOptions ( options ) ;      return session;      return  session ;    }    }      onHitTestResult(results) {    onHitTestResult ( results )   {      if (results === null) {      if   ( results  ===   null )   {        this.targetObject.enabled = false;        this . targetObject . enabled  =   false ;      } else {      }   else   {        this.targetObject.enabled = true;        this . targetObject . enabled  =   true ;        // get hit information        // get hit information        const hitPosition = results.position;        const  hitPosition  =  results . position ;        const hitNormal = results.normal;        const  hitNormal  =  results . normal ;          //identifying the direction the object should look at based on the normal of the hit location.        //identifying the direction the object should look at based on the normal of the hit location.          var lookDirection;        var  lookDirection ;        if (1 - Math.abs(hitNormal.normalize().dot(vec3.up())) < EPSILON) {        if   ( 1   -  Math . abs ( hitNormal . normalize ( ) . dot ( vec3 . up ( ) ) )   <   EPSILON )   {          lookDirection = vec3.forward();         lookDirection  =  vec3 . forward ( ) ;        } else {        }   else   {          lookDirection = hitNormal.cross(vec3.up());         lookDirection  =  hitNormal . cross ( vec3 . up ( ) ) ;        }        }          const toRotation = quat.lookAt(lookDirection, hitNormal);        const  toRotation  =  quat . lookAt ( lookDirection ,  hitNormal ) ;        //set position and rotation        //set position and rotation        this.targetObject.getTransform().setWorldPosition(hitPosition);        this . targetObject . getTransform ( ) . setWorldPosition ( hitPosition ) ;        this.targetObject.getTransform().setWorldRotation(toRotation);        this . targetObject . getTransform ( ) . setWorldRotation ( toRotation ) ;          if (        if   (          this.primaryInteractor.previousTrigger !== InteractorTriggerType.None &&          this . primaryInteractor . previousTrigger  !==  InteractorTriggerType . None  &&          this.primaryInteractor.currentTrigger === InteractorTriggerType.None          this . primaryInteractor . currentTrigger  ===  InteractorTriggerType . None       ) {        )   {          // Called when a trigger ends          // Called when a trigger ends          // Copy the plane/axis object          // Copy the plane/axis object          this.sceneObject.copyWholeHierarchy(this.targetObject);          this . sceneObject . copyWholeHierarchy ( this . targetObject ) ;        }        }      }      }    }    }      onUpdate() {    onUpdate ( )   {      this.primaryInteractor =      this . primaryInteractor  =        SIK.InteractionManager.getTargetingInteractors().shift();        SIK . InteractionManager . getTargetingInteractors ( ) . shift ( ) ;        if (      if   (        this.primaryInteractor &&        this . primaryInteractor  &&        this.primaryInteractor.isActive() &&        this . primaryInteractor . isActive ( )   &&        this.primaryInteractor.isTargeting()        this . primaryInteractor . isTargeting ( )      ) {      )   {        const rayStartOffset = new vec3(        const  rayStartOffset  =   new   vec3 (          this.primaryInteractor.startPoint.x,          this . primaryInteractor . startPoint . x ,          this.primaryInteractor.startPoint.y,          this . primaryInteractor . startPoint . y ,          this.primaryInteractor.startPoint.z + 30          this . primaryInteractor . startPoint . z  +   30        );        ) ;        const rayStart = rayStartOffset;        const  rayStart  =  rayStartOffset ;        const rayEnd = this.primaryInteractor.endPoint;        const  rayEnd  =   this . primaryInteractor . endPoint ;          this.hitTestSession.hitTest(        this . hitTestSession . hitTest (          rayStart,         rayStart ,          rayEnd,         rayEnd ,          this.onHitTestResult.bind(this)          this . onHitTestResult . bind ( this )        );        ) ;      } else {      }   else   {        this.targetObject.enabled = false;        this . targetObject . enabled  =   false ;      }      }    }    }  }  }   \nSave the script and add it to the scene.\n Save the script and add it to the scene. \nCreate a scene object to place on the surface and set it as a Target Object input of this script.\n Create a scene object to place on the surface and set it as a Target Object input of this script. Test this lens in Lens Studio preview (by moving the mouse and tapping) or send it to Connected Spectacles (point your hand to move and pinch to spawn an object). This example demonstrates the simplest method for spawning objects on a surface. Check out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library!   Check out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library! Check out the World Query Hit - Spawn On Surface and Surface Detection assets on Asset Library! Additional Resources\u200b Relevant APIs\u200b WorldQueryModule HitTestSession HitTestSessionOptions WorldQueryHitTestResult Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Web View Next Features OverviewExampleAdditional ResourcesRelevant APIs OverviewExampleAdditional ResourcesRelevant APIs OverviewExample Example Additional ResourcesRelevant APIs Relevant APIs AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/compatibility-list": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEventsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents Spectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResourcesOverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents Spectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResources Spectacles FeaturesFeaturesOn this pageCopy pageFeatures\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEventWas this page helpful?YesNoPreviousWorld Query ModuleNextResources  Spectacles Features Spectacles Features Features Features On this page Copy page  Copy page     page Features\nOverview\u200b\nSpectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details.\nMany features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.\nFeatures and resources\u200b\nHere is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates.\nFeatureResourcesAISample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchAsset Find the GPTs on the Asset LibraryAR TrackingSample Throw Lab includes SIK, Hand Tracking, PhysicsDocumentation Gesture ModuleDocumentation World Tracking PlanesDocumentation World Query ModuleDocumentation Object TrackingDocumentation Face TrackingDocumentation Point CloudAnimationAsset Find the LSTween Package on the Asset LibraryDocumentation AnimationAudioSample Voice Playback includes SIK, together with AudioDocumentation Audio In SpectaclesBitmoji AvatarDocumentation Bitmoji 2DDocumentation Bitmoji 3DDocumentation Bitmoji HeadCameraSample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and FetchSample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and FetchDocumentation Camera ModuleConnected LensesSample Air Hockey includes SIK, together with Connected Lenses, MultiplayerSample High Five includes SIK, together with Connected Lenses, MultiplayerSample Shared Sync Control includes SIK, together with Connected Lenses, MultiplayerSample Tic Tac Toe includes SIK, together with Connected Lenses, MultiplayerDocumentation Connected LensesGraphics, Material and ParticlesSample Material Library includes SIK, together with Post EffectsDocumentation MaterialsDocumentation Post EffectsDocumentation OccludersDocumentation ParticlesDocumentation Dynamic EnvironmentDocumentation Gaussian SplattingLocation ARSample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, PlacesDocumentation Location (GPS, Heading, Map Component, AR View)Documentation Places APIDocumentation Indoor Custom LocationsNetworkingSample Fetch includes SIK, together with Networking, FetchDocumentation FetchDocumentation Http RequestDocumentation Web SocketDocumentation Web ViewDocumentation Remote APIsPhriperhal ControlDocumentation Motion Controller ModulePersistent StorageDocumentation Single PlayerDocumentation MultiplayerPhysicsDocumentation ForceDocumentation RaycastDocumentation CollisionDocumentation World Query ModuleDocumentation ClothSIK (Spectacles Interaction Kit)Asset Find the Spectacles Interaction Kit on the Asset LibraryDocumentation SIKLogLevelConfigurationKeep in mind that this debugger does not give access to device internal logs, but allows you to visualize your statement in the Lens Studio editor when trying Lenses on deviceSnap MLAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Snap ML OverviewSpatial AnchorsSample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial AnchorsAsset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.)Documentation Spatial AnchorsSpatial ImageAsset Find the Spatial Image Gallery example on the Asset LibraryDocumentation Spatial ImageVoice MLDocumentation Speech RecognitionDocumentation Text To SpeechDocumentation Text To Speech 2D AnimatedDocumentation Audio ClassificationDocumentation Sentiment Analyzer\nUnsupported APIs\u200b\nSee below for a list of APIs that are not supported on Spectacles.\nAssets\u200b\nDialogModuleLocalizationsAsset\nComponents\u200b\nBlurNoiseEstimationDepthSetterEyeColorVisualFaceInsetVisualFaceMaskVisualFaceStretchVisualHintsComponentInteractionComponentManipulateComponentTrackedPointComponent\nEvents\u200b\nBrowsLoweredEventBrowsRaisedEventBrowsReturnedToNormalEventCameraBackEventCameraFrontEventKissFinishedEventKissStartedEventManipulateEndEventManipulateStartEventMouthClosedEventMouthOpenedEventSmileFinishedEventSmileStartedEventSnapImageCaptureEventSnapRecordStartEventSnapRecordStopEventTapEventTouchEndEventTouchMoveEventTouchStartEvent Features Overview\u200b Spectacles uses the same Lens Studio API set as Snapchat and Camera Kit. However, some APIs work exclusively on Spectacles and are marked as \"wearable only.\" Additionally, certain existing APIs do not function on the Spectacles (2024) device; refer to the list below for details. Many features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources.   Many features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources. Many features are still being ported and tested from the Snapchat API to Spectacles. You can try using these features since the Lens Studio workflow is similar. Features that are consolidated are usually accompanied by spectacles samples or asset. If a feature isn't listed or supported, you can test it and provide feedback to our team . Visit our Samples Repository for development resources. Features and resources\u200b Here is a detailed list of AR-specific and anticipated features, along with their current level of support on Spectacles and related resources. We are dedicated to continually improving the AR experience on Spectacles, so please stay tuned for future updates. Sample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and Fetch Sample SIK Text to Speech Speech To Text Vision Camera Fetch Asset Find the GPTs on the Asset Library Asset Sample Throw Lab includes SIK, Hand Tracking, Physics Sample SIK Hand Tracking Physics Documentation Gesture Module Documentation Documentation World Tracking Planes Documentation Documentation World Query Module Documentation Documentation Object Tracking Documentation Documentation Face Tracking Documentation Documentation Point Cloud Documentation Asset Find the LSTween Package on the Asset Library Asset Documentation Animation Documentation Sample Voice Playback includes SIK, together with Audio Sample SIK Audio Documentation Audio In Spectacles Documentation Documentation Bitmoji 2D Documentation Documentation Bitmoji 3D Documentation Documentation Bitmoji Head Documentation Sample AI Assistant includes SIK, together with Text to Speech, Speech To Text, Vision, together with Camera and Fetch Sample SIK Text to Speech Speech To Text Vision Camera Fetch Sample Crop includes SIK, together with Text to Speech, Speech To Text, Web View, Vision, together with Camera and Fetch Sample SIK Text to Speech Speech To Text Web View Vision Camera Fetch Documentation Camera Module Documentation Sample Air Hockey includes SIK, together with Connected Lenses, Multiplayer Sample SIK Connected Lenses Multiplayer Sample High Five includes SIK, together with Connected Lenses, Multiplayer Sample SIK Connected Lenses Multiplayer Sample Shared Sync Control includes SIK, together with Connected Lenses, Multiplayer Sample SIK Connected Lenses Multiplayer Sample Tic Tac Toe includes SIK, together with Connected Lenses, Multiplayer Sample SIK Connected Lenses Multiplayer Documentation Connected Lenses Documentation Sample Material Library includes SIK, together with Post Effects Sample SIK Post Effects Documentation Materials Documentation Documentation Post Effects Documentation Documentation Occluders Documentation Documentation Particles Documentation Documentation Dynamic Environment Documentation Documentation Gaussian Splatting Documentation Sample Outdoor Navigation includes SIK, together with Map Component, Outdoor Navigation, Places Sample SIK Map Component Outdoor Navigation Places Documentation Location (GPS, Heading, Map Component, AR View) Documentation Documentation Places API Documentation Documentation Indoor Custom Locations Documentation Sample Fetch includes SIK, together with Networking, Fetch Sample SIK Networking Fetch Documentation Fetch Documentation Documentation Http Request Documentation Documentation Web Socket Documentation Documentation Web View Documentation Documentation Remote APIs Documentation Documentation Motion Controller Module Documentation Documentation Single Player Documentation Documentation Multiplayer Documentation Documentation Force Documentation Documentation Raycast Documentation Documentation Collision Documentation Documentation World Query Module Documentation Documentation Cloth Documentation Asset Find the Spectacles Interaction Kit on the Asset Library Asset Documentation SIKLogLevelConfiguration Documentation Asset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.) Asset Documentation Snap ML Overview Documentation Sample Spatial Persistance includes SIK, together with Networking, Persistent StorageSpatial Anchors Sample SIK Networking Persistent Storage Spatial Anchors Asset Find the Snap ML modules on the Asset Library (Face Tracking, Object Tracking, Image Tracking etc.) Asset Documentation Spatial Anchors Documentation Asset Find the Spatial Image Gallery example on the Asset Library Asset Documentation Spatial Image Documentation Documentation Speech Recognition Documentation Documentation Text To Speech Documentation Documentation Text To Speech 2D Animated Documentation Documentation Audio Classification Documentation Documentation Sentiment Analyzer Documentation Unsupported APIs\u200b See below for a list of APIs that are not supported on Spectacles. Assets\u200b Components\u200b Events\u200b Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous World Query Module Next Resources OverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents OverviewFeatures and resourcesUnsupported APIsAssetsComponentsEvents OverviewFeatures and resources Features and resources Unsupported APIsAssetsComponentsEvents Assets Components Events AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/resources": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset LibraryAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset Library Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset Library Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset Library Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset Library Spectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverviewSamplesAsset Library Spectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverview Spectacles FeaturesResourcesOn this pageCopy pageResources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints.Was this page helpful?YesNoPreviousFeaturesNextOverview  Spectacles Features Spectacles Features Resources Resources On this page Copy page  Copy page     page Resources\nWhether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository.\nSamples\u200b\nBelow is a brief description of each sample and the features it showcases.\nIf you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.\nAI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192\nAsset Library\u200b\nIn addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio.\nSpectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints. Resources Whether you are an experienced developer or just starting out, we have the resources you need to create amazing experiences on Spectacles. Here you will find a list of resources to help you get started. We prepared a number of Samples that aim to illustrate the capabilities of Spectacles and provide a starting point for your own projects. Build your first lens or explore the samples on our GitHub Repository. Samples\u200b Below is a brief description of each sample and the features it showcases. If you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository.   If you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository. If you wish to use a feature that is not listed below or want to contribute to our sample repository, leave a feedback to our team or check out our Samples Repository. AI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192 AI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192 AI AssistantAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchCreate an AI Assistant that understands and responds to voice commands.View sample project \u2192 AI Assistant AICameraNetworkingLLMSpeech To TextText to SpeechVisionFetch AI Camera Networking LLM Speech To Text Text to Speech Vision Fetch Create an AI Assistant that understands and responds to voice commands. Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192 Air HockeyNetworkingConnected LensesMultiplayerMultiplayer air hockey game using connected lenses.View sample project \u2192 Air Hockey NetworkingConnected LensesMultiplayer Networking Connected Lenses Multiplayer Multiplayer air hockey game using connected lenses. CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192 CropAICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb ViewCrop images using hand gestures.View sample project \u2192 Crop AICameraNetworkingLLMSpeech To TextText to SpeechVisionFetchWeb View AI Camera Networking LLM Speech To Text Text to Speech Vision Fetch Web View Crop images using hand gestures. FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192 FetchNetworkingFetchWeb ViewFetch an API endpoint and display results.View sample project \u2192 Fetch NetworkingFetchWeb View Networking Fetch Web View Fetch an API endpoint and display results. High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192 High FiveNetworkingConnected LensesMultiplayerConnected lens displays a message when two users high-five.View sample project \u2192 High Five NetworkingConnected LensesMultiplayer Networking Connected Lenses Multiplayer Connected lens displays a message when two users high-five. Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192 Material LibraryGraphics, Material and ParticlesPost EffectsA collection of 3D materials and shaders for Spectacles.View sample project \u2192 Material Library Graphics, Material and ParticlesPost Effects Graphics, Material and Particles Post Effects A collection of 3D materials and shaders for Spectacles. Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192 Outdoor NavigationLocation AROutdoor NavigationMap ComponentPlacesUse GPS and Maps API to create navigation experiences.View sample project \u2192 Outdoor Navigation Location AROutdoor NavigationMap ComponentPlaces Location AR Outdoor Navigation Map Component Places Use GPS and Maps API to create navigation experiences. Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192 Shared Sync ControlsNetworkingConnected LensesMultiplayerSync UI controls between multiple connected users.View sample project \u2192 Shared Sync Controls NetworkingConnected LensesMultiplayer Networking Connected Lenses Multiplayer Sync UI controls between multiple connected users. Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192 Spatial PersistenceSpatial AnchorsSave objects in space across multiple sessions.View sample project \u2192 Spatial Persistence Spatial Anchors Spatial Anchors Save objects in space across multiple sessions. Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192 Throw LabPhysicsHand TrackingInteract with a dartboard using throwing gestures.View sample project \u2192 Throw Lab PhysicsHand Tracking Physics Hand Tracking Interact with a dartboard using throwing gestures. Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192 Tic Tac ToeNetworkingConnected LensesMultiplayerMultiplayer Tic Tac Toe game using connected lenses.View sample project \u2192 Tic Tac Toe NetworkingConnected LensesMultiplayer Networking Connected Lenses Multiplayer Multiplayer Tic Tac Toe game using connected lenses. Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192 Voice PlaybackAudioText to SpeechSpeech To TextRecord and play back messages using voice commands.View sample project \u2192 Voice Playback AudioText to SpeechSpeech To Text Audio Text to Speech Speech To Text Record and play back messages using voice commands. Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192 Spatial Image GallerySpatial ImageUpload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\".View sample project \u2192 Spatial Image Gallery Spatial Image Spatial Image Upload and visualize images in 3D space. Find it on Lens Studio home page in \"Sample projects\". Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192 Custom LocationsLocation ARMap real life areas and create AR experiences around those locations.View sample project \u2192 Custom Locations Location AR Location AR Map real life areas and create AR experiences around those locations. Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192 Path PioneerGraphics, Material and ParticlesGraphics, Material and ParticlesSIKPath creation and path walking experience.View sample project \u2192 Path Pioneer Graphics, Material and ParticlesGraphics, Material and ParticlesSIK Graphics, Material and Particles Graphics, Material and Particles SIK Path creation and path walking experience. Asset Library\u200b In addition to the samples above, we also provide an Asset Library that contains a collection of 3D models, materials, and textures that you can use in your projects. You can find the Asset Library in Lens Studio. Spectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses.Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses.World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces.Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens.Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh.PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes.RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles.Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces.LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio.Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API.Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints. Spectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses. Spectacles Interaction KitConnected LensesSIKSnap Inc.A package that provide a rich input interaction set up in lenses. Spectacles Interaction Kit Connected LensesSIK Connected Lenses SIK A package that provide a rich input interaction set up in lenses. Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses. Spectacles Sync KitConnected LensesSync KitSnap Inc.A package that enables real-time syncing between connected lenses. Spectacles Sync Kit Connected LensesSync Kit Connected Lenses Sync Kit A package that enables real-time syncing between connected lenses. World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces. World Query Hit - Spawn On SurfaceAR TrackingWorld QuerySnap Inc.Performs hit tests for real-world surfaces. World Query Hit - Spawn On Surface AR TrackingWorld Query AR Tracking World Query Performs hit tests for real-world surfaces. Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens. Web ViewNetworkingWeb ViewSnap Inc.Allows hosting web content inside a Lens. Web View NetworkingWeb View Networking Web View Allows hosting web content inside a Lens. Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh. Spatial ImageSpatial ImageSnap Inc.Transforms a 2D image into a spatialized 3D mesh. Spatial Image Spatial Image Spatial Image Transforms a 2D image into a spatialized 3D mesh. PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes. PointerPhysicsRaycastBennyPCreates directional indicators in AR scenes. Pointer PhysicsRaycast Physics Raycast Creates directional indicators in AR scenes. RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles. RaycasterPhysicsRaycastBennyPDemonstrates raycasting capabilities in Spectacles. Raycaster PhysicsRaycast Physics Raycast Demonstrates raycasting capabilities in Spectacles. Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces. Surface DetectionAR TrackingWorld QuerySnap Inc.Detects and recognizes real-world surfaces. Surface Detection AR TrackingWorld Query AR Tracking World Query Detects and recognizes real-world surfaces. LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio. LSTweenAnimationSnap Inc.Helps developers with animation handling in Lens Studio. LSTween Animation Animation Helps developers with animation handling in Lens Studio. Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API. Motion ControllerPeripheral ControlMotion ControllerSnap Inc.Helps developers handling the controller API. Motion Controller Peripheral ControlMotion Controller Peripheral Control Motion Controller Helps developers handling the controller API. Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints. Spectacles 3D Hand HintsGraphics, Material and ParticlesSnap Inc.Suite of animated Hand Gestures Hints. Spectacles 3D Hand Hints Graphics, Material and Particles Graphics, Material and Particles Suite of animated Hand Gestures Hints. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Features Next Overview SamplesAsset Library SamplesAsset Library Samples Asset Library AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/connected-lenses/overview": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitectureAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected Lenses Connected Lenses [Beta] Overview Using Connected Lenses Building Connected Lenses Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture Spectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected LensesAbout Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture Spectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected Lenses Spectacles FeaturesConnected Lenses [Beta]OverviewOn this pageCopy pageOverview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit.Was this page helpful?YesNoPreviousResourcesNextUsing Connected Lenses  Spectacles Features Spectacles Features Connected Lenses [Beta] Connected Lenses [Beta] Overview Overview On this page Copy page  Copy page     page Overview\nConnected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time.\nAbout Connected Lenses on Spectacles [Beta]\u200b\nSpectacles-to-Spectacles\u200b\nConnected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported.\nColocated\u200b\nConnected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map.\nSynchronous\u200b\nConnected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time.\nConnected Lenses System\u200b\nArchitecture\u200b\nDuring a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.\n\nSnapOS\u200b\nSnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses.\nLens Studio\u200b\nLens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit. Overview Connected Lenses on Spectacles are shared experiences that let multiple people see and interact with the same AR content at the same time. About Connected Lenses on Spectacles [Beta]\u200b Connected Lenses support Spectacles-to-Spectacles sessions. All users must join a Connected Lens from Spectacles. Joining the same Connected Lens from both Spectacles and mobile devices is not currently supported. Connected Lenses on Spectacles are colocated, meaning users are in the same physical space. The first user to join the Connected Lens is guided to map their surroundings, and subsequent users relocalize against the shared map. Connected Lenses on Spectacles are synchronous, meaning users experience the Lens at the same time. Lens data is updated across the network in real-time. Connected Lenses System\u200b Architecture\u200b During a Connected Lens session, multiple Spectacles devices connect to the same realtime data stores in the Connected Lenses backend. To synchronize Lens content, the backend stores data that is accessible to all devices and relays messages among devices.  SnapOS provides multiple ways for Spectacles users to join Connected Lenses from Lens Explorer. For more information, see Using Connected Lenses. Lens Studio offers tools to aid in the development and testing of Connected Lenses. For more information, see Building Connected Lenses. Spectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information, see Spectacles Sync Kit. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Resources Next Using Connected Lenses About Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture About Connected Lenses on Spectacles [Beta]Connected Lenses SystemArchitecture About Connected Lenses on Spectacles [Beta] Connected Lenses System Architecture AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/connected-lenses/using-connected-lenses": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate SpaceAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected Lenses Connected Lenses [Beta] Overview Using Connected Lenses Building Connected Lenses Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Spectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected LensesCreating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Spectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected Lenses Spectacles FeaturesConnected Lenses [Beta]Using Connected LensesOn this pageCopy pageUsing Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\nWas this page helpful?YesNoPreviousOverviewNextBuilding Connected Lenses  Spectacles Features Spectacles Features Connected Lenses [Beta] Connected Lenses [Beta] Using Connected Lenses Using Connected Lenses On this page Copy page  Copy page     page Using Connected Lenses\nSnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer.\nCreating a New Session\u200b\nConnected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps:\n\nSelect the Play Together category in Lens Explorer.\nLaunch your chosen Lens.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect Start New to create a new session.\n\n\nJoining an Existing Session\u200b\nThere are two ways to join an active Connected Lens session from Lens Explorer:\n\nPlay Together category\nActive Nearby category\n\nPlay Together\u200b\n\nSelect the Play Together category in Lens Explorer.\nLaunch the same Lens as the person you want to join.\nIn the Lens, pinch the Multiplayer button to open the system dialog for selecting a session.\nSelect the session that has the name(s) and bitmoji(s) of those you would like to join.\n\n\nActive Nearby\u200b\n\nSelect the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you.\nSelect the tile for a Connected Lens session to join that session directly.\n\n\nEstablishing a Shared Coordinate Space\u200b\nConnected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map.\nFollow these tips to ensure successful mapping and relocalization:\n\nLook around at physical objects and patterns\nAvoid looking at plain, solid-colored walls and surfaces\nMove at a slow and steady pace, including side-to-side movement\nUse adequate lighting to make features of your space detectable\n\nOnce you map or relocalize successfully, your Connected Lenses session setup is complete!\n Using Connected Lenses SnapOS provides multiple entry points for Connected Lenses on Spectacles. A Connected Lens session can be created or joined from Lens Explorer. Creating a New Session\u200b Connected Lenses are available under the Play Together section of Lens Explorer. To start a Connected Lens session, follow these steps: Select the Play Together category in Lens Explorer. Launch your chosen Lens. In the Lens, pinch the Multiplayer button to open the system dialog for selecting a session. Select Start New to create a new session.  Joining an Existing Session\u200b There are two ways to join an active Connected Lens session from Lens Explorer: Play Together category Active Nearby category Select the Play Together category in Lens Explorer. Launch the same Lens as the person you want to join. In the Lens, pinch the Multiplayer button to open the system dialog for selecting a session. Select the session that has the name(s) and bitmoji(s) of those you would like to join.  Select the Active Nearby category in Lens Explorer. Lens Explorer will show tiles of Connected Lenses sessions that are active around you. Select the tile for a Connected Lens session to join that session directly.  Establishing a Shared Coordinate Space\u200b Connected Lenses on Spectacles are colocated, meaning users share a coordinate space. To establish a shared coordinate space, the first user to join a session is prompted to map their surroundings. Subsequent users are guided to look around to relocalize against the shared map. Follow these tips to ensure successful mapping and relocalization: Look around at physical objects and patterns Avoid looking at plain, solid-colored walls and surfaces Move at a slow and steady pace, including side-to-side movement Use adequate lighting to make features of your space detectable Once you map or relocalize successfully, your Connected Lenses session setup is complete!  Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Overview Next Building Connected Lenses Creating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Creating a New SessionJoining an Existing SessionEstablishing a Shared Coordinate Space Creating a New Session Joining an Existing Session Establishing a Shared Coordinate Space AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/connected-lenses/building-connected-lenses": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictionsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected LensesAudio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta]OverviewUsing Connected LensesBuilding Connected Lenses Connected Lenses [Beta] Overview Using Connected Lenses Building Connected Lenses Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Spectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in SpectaclesSpectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Spectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in Spectacles Spectacles FeaturesConnected Lenses [Beta]Building Connected LensesOn this pageCopy pageBuilding Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions.Was this page helpful?YesNoPreviousUsing Connected LensesNextAudio in Spectacles  Spectacles Features Spectacles Features Connected Lenses [Beta] Connected Lenses [Beta] Building Connected Lenses Building Connected Lenses On this page Copy page  Copy page     page Building Connected Lenses\nConnected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device.\nSpectacles Sync Kit\u200b\nSpectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.\n\nTypeScript Status panel\u200b\nThe Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.\n\nProject Settings\u200b\nIn Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.\n\nConnected Lens Module\u200b\nThe Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.\n\nMultiple Previews\u200b\nConnected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.\n\nEach Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.\n\nTo exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.\n\nConnected Lenses Monitor [Beta]\u200b\nThe Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.\n\nTesting on Spectacles\u200b\nTo test your Connected Lens on Spectacles, follow these steps:\n\nFor multiple pairs of Spectacles, pair each device to a separate Snapchat account.\nPair each Snapchat account to Lens Studio.\nIn Lens Studio, select Send to All Paired Spectacles.\n\nKnown Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n\nRestrictions\u200b\nWhen building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions. Building Connected Lenses Connected Lenses for Spectacles are built in Lens Studio. Lens studio offers a number of tools for building and testing Connected Lenses in the editor and on device. Spectacles Sync Kit\u200b Spectacles Sync Kit is a Lens Studio package that provides the core logic, APIs, and components for building Connected Lenses on Spectacles. For more information about how to set up a project with Spectacles Sync Kit, see Spectacles Sync Kit > Getting Started.  TypeScript Status panel\u200b The Spectacles Sync Kit package is written in TypeScript, which needs to be compiled before you can preview your Lens. Add the TypeScript Status panel to your Lens Studio layout via Windows > Utilities > TypeScript Status panel to confirm compilation.  Project Settings\u200b In Project Settings > Platform Settings, select Spectacles. This configures the appropriate permissions for Connected Lenses on Spectacles.  Connected Lens Module\u200b The Spectacles Sync Kit package includes a Connected Lens Module that connects your Lens to the Connected Lenses backend. The Inspector panel for the Connected Lenses Module includes a Session ID field. The Session ID is is a unique string that identifies the session. Press the Randomize Session ID button to create a new session or to reset the session for all Preview panels.  Multiple Previews\u200b Connected Lenses can be tested in Lens Studio using multiple Preview panels. When multiple Preview panels are open, each Preview simulates a different user in a Connected Lenses session. To add an additional Preview panel to your Lens Studio layout, select Windows > General > Preview. Configure new Preview panels for Spectacles.  Each Preview panel shows the Session ID that it is connected to. This can be used to verify that Previews are joining the same session.  To exit the session from a Preview panel, press the Refresh button at the top right of the panel. This will reset the Preview to the Start Menu.  Connected Lenses Monitor [Beta]\u200b The Connected Lenses Monitor [Beta] is a tool that provides a real-time view of the session, including messages and data stores. It can be added from Windows > Utilities > Connected Lenses Monitor. For more information on how to use the Connected Lenses Monitor, see How to Use Connected Lenses Monitor.  Testing on Spectacles\u200b To test your Connected Lens on Spectacles, follow these steps: For multiple pairs of Spectacles, pair each device to a separate Snapchat account. Pair each Snapchat account to Lens Studio. In Lens Studio, select Send to All Paired Spectacles. Known Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n   Known Issues and Limitations\nOnly one pair of Spectacles can be paired to a given Snapchat account at a time.\nConnecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses.\nIf you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio.\nJoining the same session from Lens Studio and Spectacles is not currently supported.\n Known Issues and Limitations Only one pair of Spectacles can be paired to a given Snapchat account at a time. Connecting Spectacles to Lens Studio directly using the Connect Spectacles button is not currently supported for testing Connected Lenses. If you send a Connected Lens to Spectacles again while the Lens is still open on Spectacles, the Lens will not launch properly. As a workaround, close the Lens on Spectacles before sending it again from Lens Studio. Joining the same session from Lens Studio and Spectacles is not currently supported. Restrictions\u200b When building and using Connected Lenses, some APIs are restricted for privacy. For more information, see Connected Lenses > Restrictions. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Using Connected Lenses Next Audio in Spectacles Spectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Spectacles Sync KitTypeScript Status panelProject SettingsConnected Lens ModuleMultiple PreviewsConnected Lenses Monitor [Beta]Testing on SpectaclesRestrictions Spectacles Sync Kit TypeScript Status panel Project Settings Connected Lens Module Multiple Previews Connected Lenses Monitor [Beta] Testing on Spectacles Restrictions AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/about-spectacles-features/audio": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice ProfileAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Spectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting StartedMix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Spectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting Started Spectacles FeaturesAudio in SpectaclesOn this pageCopy pageAudio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.Was this page helpful?YesNoPreviousBuilding Connected LensesNextGetting Started  Spectacles Features Spectacles Features Audio in Spectacles Audio in Spectacles On this page Copy page  Copy page     page Audio in Spectacles\nWhen developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features.\nMix To Snap\u200b\nThe Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored.\nPlayback Modes\u200b\nThe Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power.\nLow Power\u200b\nReduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }}\nLow Latency\u200b\nMinimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback.\n@componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }}\nAudio Input Profiles\u200b\nSpectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied.\nAudio Input Profiles Include:\n\n\nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n\n\nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n\n\nAnalysis Profile\u200b\nApplied when Microphone Audio is used in your Lens project.\nWhen the Analysis Profile is active the following are used:\n\nEcho Cancellation\n\nVoice Profile\u200b\nApplied when VoiceMLModule is used in your Lens project.\nWhen the Voice Profile is active, the following are used:\n\nBystander Speech Rejection\nEcho Cancellation\n\nIf both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile. Audio in Spectacles When developing experiences for Spectacles, you must consider additional aspects of the Audio Component beyond the standard platform features. Mix To Snap\u200b The Mix to Snap feature, which allows recording sound directly into Snap, is automatically applied to all Audio Components by default in Spectacles. The flag for this in the Audio Component will be ignored. Playback Modes\u200b The Playback Mode property of the AudioComponent can only be set through a script. Spectacles default all Playback Modes to Low Power. Low Power\u200b Reduces power usage for the Spectacles device. but introduces latency in audio playback. Suitable for ambient sounds or background music where slight delays are acceptable. @componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }} @componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowPower;  }} @component @ component  export class NewScript extends BaseScriptComponent {  export   class   NewScript   extends   BaseScriptComponent   {    @input    @ input    audio: AudioComponent;   audio :  AudioComponent ;    onAwake() {    onAwake ( )   {      this.audio.playbackMode = Audio.PlaybackMode.LowPower;      this . audio . playbackMode  =  Audio . PlaybackMode . LowPower ;    }    }  }  }   Low Latency\u200b Minimizes audio playback latency but increases power usage for the Spectacles device. Recommended for audio requiring immediate auditory reaction, such as button press feedback. @componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }} @componentexport class NewScript extends BaseScriptComponent {  @input  audio: AudioComponent;  onAwake() {    this.audio.playbackMode = Audio.PlaybackMode.LowLatency;  }} @component @ component  export class NewScript extends BaseScriptComponent {  export   class   NewScript   extends   BaseScriptComponent   {    @input    @ input    audio: AudioComponent;   audio :  AudioComponent ;    onAwake() {    onAwake ( )   {      this.audio.playbackMode = Audio.PlaybackMode.LowLatency;      this . audio . playbackMode  =  Audio . PlaybackMode . LowLatency ;    }    }  }  }   Audio Input Profiles\u200b Spectacles automatically add audio profiles based on the features applied to your Lens. By default, no Audio Input Profiles are applied. Audio Input Profiles Include: \nBystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking.\n Bystander Speech Rejection: Ignores speech not coming from the Spectacles wearer, preventing unintended speech transcription from nearby conversations when the wearer is not talking. \nEcho Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone.\n Echo Cancellation: Eliminates the feedback loop from sounds generated by the speakers that would otherwise be picked up by the microphone. Analysis Profile\u200b Applied when Microphone Audio is used in your Lens project. When the Analysis Profile is active the following are used: Echo Cancellation Voice Profile\u200b Applied when VoiceMLModule is used in your Lens project. When the Voice Profile is active, the following are used: Bystander Speech Rejection Echo Cancellation If both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile.   If both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile. If both Microphone Audio and VoiceMLModule are used simultaneously, the Voice Profile will take precedence over the Analysis Profile. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Building Connected Lenses Next Getting Started Mix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Mix To SnapPlayback ModesLow PowerLow LatencyAudio Input ProfilesAnalysis ProfileVoice Profile Mix To Snap Playback ModesLow PowerLow Latency Low Power Low Latency Audio Input ProfilesAnalysis ProfileVoice Profile Analysis Profile Voice Profile AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/get-started": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a ProjectAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started Features Features Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Spectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverviewStarting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Spectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverview Spectacles FrameworksSpectacles Interaction KitGetting StartedOn this pageCopy pageGetting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.Was this page helpful?YesNoPreviousAudio in SpectaclesNextOverview  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Getting Started Getting Started On this page Copy page  Copy page     page Getting Started\n\nSpectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences.\nDownload the latest compatible version of Lens Studio 5.\nFor the most recent updates, please refer to the release notes.\nStarting a New Spectacles Project\u200b\nIn the Lens Studio home, select \u201cSpectacles\u201d under Start New Project.\n\nYour new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development.\nImporting Spectacles Interaction Kit into a new Lens Studio Project\u200b\nTo configure a new project from scratch using the Spectacles Interaction Kit, follow these steps:\n\n\nCreate a new project in Lens Studio.\n\n\nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n\n\nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n\n\nSet Platform Settings to Lens Is Made For > Spectacles.\n\n\n\nIn the Preview panel, set Device Type Override to Spectacles.\n\n\n\nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n\n\nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n\n\nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n\n\nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n\n\nSave your project, and you\u2019re all finished!\n\n\nUpgrading Spectacles Interaction Kit within a Project\u200b\nOpen your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it.\n\nTo manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library.\nNow your project is updated!\nCompilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits. Getting Started Spectacles Interaction Kit (SIK) is a collection of code components, modules, and assets for Lens Studio. SIK simplifies development for the Spectacles platform by providing common building blocks that facilitate the creation of interactive augmented reality experiences. Download the latest compatible version of Lens Studio 5. For the most recent updates, please refer to the release notes. Starting a New Spectacles Project\u200b In the Lens Studio home, select \u201cSpectacles\u201d under Start New Project. Your new project will already be preconfigured with Spectacles Interaction Kit and other essential configurations for Spectacles development. Importing Spectacles Interaction Kit into a new Lens Studio Project\u200b To configure a new project from scratch using the Spectacles Interaction Kit, follow these steps: \nCreate a new project in Lens Studio.\n Create a new project in Lens Studio. \nAdd the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status.\n\n Add the TypeScript Status panel to the Lens Studio UI by navigating to Window > TypeScript Status. \nAdd a Device Tracking component to your Camera and set tracking mode to World.\n\n\n Add a Device Tracking component to your Camera and set tracking mode to World. \nSet Platform Settings to Lens Is Made For > Spectacles.\n\n Set Platform Settings to Lens Is Made For > Spectacles. \nIn the Preview panel, set Device Type Override to Spectacles.\n\n In the Preview panel, set Device Type Override to Spectacles. \nInstall the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit.\n\n Install the latest Spectacles Interaction Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Interaction Kit. \nImport Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit.\n\n Import Spectacles Interaction Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesInteractionKit. \nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\nYou can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.\n After the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel. You can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path.   You can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path. You can also unpack the Spectacles Interaction Kit package by right-clicking\nand selecting Unpack. Unpacking enables direct editing of SIK but may complicate\nfuture upgrades to newer versions. Additionally, when unpacked, import components\nand modules in your TypeScript files using SpectaclesInteractionKit rather\nthan SpectaclesInteractionKit.lspkg in your import path. \nFrom the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.\n From the Asset Browser, drag Assets/SpectaclesInteractionKit/Prefabs/SpectaclesInteractionKit.prefab into the scene hierarchy. Instantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy.   Instantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy. Instantiating the prefab before TypeScript compilation is complete\nmay result in a corrupt prefab with missing links to TypeScript script\ncomponents. Ensure that TypeScript compilation is fully complete before\nadding a prefab instance to the scene hierarchy. \nIf the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy.\n\n If the Examples are no longer needed, disable or delete the [EXAMPLES] scene objects from the scene hierarchy. \nSave your project, and you\u2019re all finished!\n Save your project, and you\u2019re all finished! Upgrading Spectacles Interaction Kit within a Project\u200b Open your project. After TypeScript compilation is complete, right-click the SpectaclesInteractionKit package in the Asset Browser. If Pull Update from Library is available, select it. To manually check for updates, navigate to Asset Library > Spectacles > SpectaclesInteractionKit and verify if a new version is available for installation. You can also select a specific version to install. After updating, right-click the SpectaclesInteractionKit package in the Asset Browser and select Pull Update from Library. Now your project is updated! Compilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits.   Compilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits. Compilation errors may result after updates from changes in the API. Review the release notes for any updates on new APIs and make the necessary edits. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Audio in Spectacles Next Overview Starting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Starting a New Spectacles ProjectImporting Spectacles Interaction Kit into a new Lens Studio ProjectUpgrading Spectacles Interaction Kit within a Project Starting a New Spectacles Project Importing Spectacles Interaction Kit into a new Lens Studio Project Upgrading Spectacles Interaction Kit within a Project AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/interactionsystem": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand TrackingRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand Tracking Spectacles FrameworksSpectacles Interaction KitFeaturesInteraction SystemOn this pageCopy pageInteraction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake();Was this page helpful?YesNoPreviousOverviewNextHand Tracking  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Interaction System Interaction System On this page Copy page  Copy page     page Interaction System\nSpectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor.\nInteractors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs.\nSIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments.\n\nRelevant Components\u200b\nInteractor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene.\nInteractable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered.\nInteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event.\nInteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus.\nCode Example\u200b\nTypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); Interaction System Spectacles Interaction Kit features a multi-modal interaction system for creating interactive and dynamic AR experiences. This system consists of three core components: Interactors, Interactables, and the Interaction Manager. Interactors represent various input methods, such as hands or mobile devices. Each input type is associated with its own Interactor: for example, a user\u2019s hand is managed by a HandInteractor, while a mobile controller is managed by a MobileInteractor. Interactors manage interactions with Interactables, which are AR objects in the scene that users can interact with. Each Interactable has states that reflect its interaction status, such as hover, trigger started, or trigger ended. The Interaction Manager controls the lifecycle of interactions, ensuring smooth transitions and responses to user inputs. SIK supports two main input modalities: hand tracking and mobile device input. Hand tracking enables natural and intuitive interaction with AR content through gestures like direct-pinch and direct-poke. Mobile device input allows users to calibrate their Spectacles with the mobile app, using smartphones as controllers. This multi-modal approach enhances the user experience, offering versatile and contextually appropriate interactions in AR environments. Relevant Components\u200b Interactor reads input from either hands (using HandInteractor) or a phone (using MobileInteractor) to target, trigger, and drag Interactables in the scene. Interactable enables a SceneObject with a Collider component to be interacted with through an Interactor. Developers can attach callback logic to Interactable events, such as onTriggerEnd, to execute lens-specific actions after a button is triggered. InteractionManager oversees all interactions, providing a comprehensive event cascading system. It utilizes a bubble-up / trickle-down approach, where ancestor Interactables of a targeted Interactable also receive the event. InteractableManipulation allows Interactables to be translated, scaled, and rotated using an Interactor, facilitating the manipulation of movable elements like menus. Code Example\u200b TypeScriptJavaScriptimport { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); TypeScript JavaScript import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }} import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }} import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleInteractionScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let interactionManager = SIK.InteractionManager;    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    let interactable = this.sceneObject.getComponent(      Interactable.getTypeName()    );    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(      this.sceneObject    );    // Define the desired callback logic for the relevant Interactable event.    let onTriggerStartCallback = (event: InteractorEvent) => {      print(        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`      );    };    interactable.onInteractorTriggerStart(onTriggerStartCallback);  }} import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable'; import   {  Interactable  }   from   'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable' ;  import { InteractorEvent } from 'SpectaclesInteractionKit/Core/Interactor/InteractorEvent';  import   {  InteractorEvent  }   from   'SpectaclesInteractionKit/Core/Interactor/InteractorEvent' ;  import { SIK } from 'SpectaclesInteractionKit/SIK';  import   {   SIK   }   from   'SpectaclesInteractionKit/SIK' ;    @component  @ component  export class ExampleInteractionScript extends BaseScriptComponent {  export   class   ExampleInteractionScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      onStart() {    onStart ( )   {      let interactionManager = SIK.InteractionManager;      let  interactionManager  =   SIK . InteractionManager ;        // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.      // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.      let interactable = this.sceneObject.getComponent(      let  interactable  =   this . sceneObject . getComponent (        Interactable.getTypeName()       Interactable . getTypeName ( )      );      ) ;        // You could also retrieve the Interactable component like this:      // You could also retrieve the Interactable component like this:      interactable = interactionManager.getInteractableBySceneObject(     interactable  =  interactionManager . getInteractableBySceneObject (        this.sceneObject        this . sceneObject     );      ) ;        // Define the desired callback logic for the relevant Interactable event.      // Define the desired callback logic for the relevant Interactable event.      let onTriggerStartCallback = (event: InteractorEvent) => {      let   onTriggerStartCallback   =   ( event :  InteractorEvent )   =>   {        print(        print (          `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`          ` The Interactable has been triggered by an Interactor with input type:  ${ event . interactor . inputType }  at position:  ${ event . interactor . targetHitInfo . hit . position } `        );        ) ;      };      } ;        interactable.onInteractorTriggerStart(onTriggerStartCallback);     interactable . onInteractorTriggerStart ( onTriggerStartCallback ) ;    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.  var interactableTypename =    interactionConfiguration.requireType('Interactable');  var interactable = script.sceneObject.getComponent(interactableTypename);  // You could also retrieve the Interactable component like this:  interactable = interactionManager.getInteractableBySceneObject(    script.sceneObject  );  // Define the desired callback logic for the relevant Interactable event.  var onTriggerStartCallback = (event) => {    print(      `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`    );  };  interactable.onInteractorTriggerStart(onTriggerStartCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const interactionManager = SIK.InteractionManager;  const  interactionManager  =   SIK . InteractionManager ;  const interactionConfiguration = SIK.InteractionConfiguration;  const  interactionConfiguration  =   SIK . InteractionConfiguration ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    // This script assumes that an Interactable (and Collider) component have already been instantiated on the SceneObject.    var interactableTypename =    var  interactableTypename  =      interactionConfiguration.requireType('Interactable');     interactionConfiguration . requireType ( 'Interactable' ) ;    var interactable = script.sceneObject.getComponent(interactableTypename);    var  interactable  =  script . sceneObject . getComponent ( interactableTypename ) ;      // You could also retrieve the Interactable component like this:    // You could also retrieve the Interactable component like this:    interactable = interactionManager.getInteractableBySceneObject(   interactable  =  interactionManager . getInteractableBySceneObject (      script.sceneObject     script . sceneObject    );    ) ;      // Define the desired callback logic for the relevant Interactable event.    // Define the desired callback logic for the relevant Interactable event.    var onTriggerStartCallback = (event) => {    var   onTriggerStartCallback   =   ( event )   =>   {      print(      print (        `The Interactable has been triggered by an Interactor with input type: ${event.interactor.inputType} at position: ${event.interactor.targetHitInfo.hit.position}`        ` The Interactable has been triggered by an Interactor with input type:  ${ event . interactor . inputType }  at position:  ${ event . interactor . targetHitInfo . hit . position } `      );      ) ;    };    } ;      interactable.onInteractorTriggerStart(onTriggerStartCallback);   interactable . onInteractorTriggerStart ( onTriggerStartCallback ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Overview Next Hand Tracking Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/handtracking": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractorAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Spectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile ControllerRelevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Spectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile Controller Spectacles FrameworksSpectacles Interaction KitFeaturesHand TrackingOn this pageCopy pageHand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake();Was this page helpful?YesNoPreviousInteraction SystemNextMobile Controller  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Hand Tracking Hand Tracking On this page Copy page  Copy page     page Hand Tracking\nThe Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions.\n\nDevelopers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses.\nRelevant Components\u200b\nCode Example - Hand Input Data\u200b\nHandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake();\nCode Example - HandInteractor\u200b\nHandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System.\nTypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); Hand Tracking The Spectacles Interaction Kit enables developers to create immersive augmented reality experiences using hand tracking as an input method. This technology allows users to interact with their environment in a natural way, enhancing the sense of presence and social interaction. By leveraging hand tracking, developers can create more dynamic and realistic AR interactions. Developers can utilize several hand interaction modes within the Spectacles Interaction Kit, including indirect (ray-based) interactions, direct-pinch, and direct-poke, to enhance their lenses. These interaction modalities offer flexibility in user engagement with AR content, allowing for contextually appropriate experiences. SIK\u2019s hand tracking input also works in conjunction with other input methods, such as mobile controller input, providing a multi-faceted approach to user interaction with lenses. Relevant Components\u200b Code Example - Hand Input Data\u200b HandInputData interfaces with Lens Studio\u2019s ObjectTracking3D and GestureModule to provide hand tracking data to the rest of SIK, and in particular to the HandInteractor and HandVisual components. Please note that HandInputData provides hand tracking data only.\nTrackedHand represents and manages a tracked hand. It can be accessed using HandInputData\u2019s getHand API. TrackedHand provides useful properties, including all keypoints of the hand, among others. TypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake(); TypeScript JavaScript import { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake(); import { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }} import { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }} import { SIK } from './SpectaclesInteractionKit/SIK';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // Retrieve HandInputData from SIK's definitions.    let handInputData = SIK.HandInputData;    // Fetch the TrackedHand for left and right hands.    let leftHand = handInputData.getHand('left');    let rightHand = handInputData.getHand('right');    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {      print(        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`      );    });    rightHand.onPinchDown.add(() => {      print(        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`      );    });  }} import { SIK } from './SpectaclesInteractionKit/SIK'; import   {   SIK   }   from   './SpectaclesInteractionKit/SIK' ;    @component  @ component  export class ExampleHandScript extends BaseScriptComponent {  export   class   ExampleHandScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      onStart() {    onStart ( )   {      // Retrieve HandInputData from SIK's definitions.      // Retrieve HandInputData from SIK's definitions.      let handInputData = SIK.HandInputData;      let  handInputData  =   SIK . HandInputData ;        // Fetch the TrackedHand for left and right hands.      // Fetch the TrackedHand for left and right hands.      let leftHand = handInputData.getHand('left');      let  leftHand  =  handInputData . getHand ( 'left' ) ;      let rightHand = handInputData.getHand('right');      let  rightHand  =  handInputData . getHand ( 'right' ) ;        // Add print callbacks for whenever these hands pinch.      // Add print callbacks for whenever these hands pinch.      leftHand.onPinchDown.add(() => {     leftHand . onPinchDown . add ( ( )   =>   {        print(        print (          `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`          ` The left hand has pinched. The tip of the left index finger is:  ${ leftHand . indexTip . position } . `        );        ) ;      });      } ) ;      rightHand.onPinchDown.add(() => {     rightHand . onPinchDown . add ( ( )   =>   {        print(        print (          `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`          ` The right hand has pinched. The tip of the right index finger is:  ${ rightHand . indexTip . position } . `        );        ) ;      });      } ) ;    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const handInputData = SIK.HandInputData;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Fetch the TrackedHand for left and right hands.  var leftHand = handInputData.getHand('left');  var rightHand = handInputData.getHand('right');  // Add print callbacks for whenever these hands pinch.  leftHand.onPinchDown.add(() => {    print(      `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`    );  });  rightHand.onPinchDown.add(() => {    print(      `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`    );  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const handInputData = SIK.HandInputData;  const  handInputData  =   SIK . HandInputData ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    // Fetch the TrackedHand for left and right hands.    // Fetch the TrackedHand for left and right hands.    var leftHand = handInputData.getHand('left');    var  leftHand  =  handInputData . getHand ( 'left' ) ;    var rightHand = handInputData.getHand('right');    var  rightHand  =  handInputData . getHand ( 'right' ) ;      // Add print callbacks for whenever these hands pinch.    // Add print callbacks for whenever these hands pinch.    leftHand.onPinchDown.add(() => {   leftHand . onPinchDown . add ( ( )   =>   {      print(      print (        `The left hand has pinched. The tip of the left index finger is: ${leftHand.indexTip.position}.`        ` The left hand has pinched. The tip of the left index finger is:  ${ leftHand . indexTip . position } . `      );      ) ;    });    } ) ;    rightHand.onPinchDown.add(() => {   rightHand . onPinchDown . add ( ( )   =>   {      print(      print (        `The right hand has pinched. The tip of the right index finger is: ${rightHand.indexTip.position}.`        ` The right hand has pinched. The tip of the right index finger is:  ${ rightHand . indexTip . position } . `      );      ) ;    });    } ) ;  }  }    onAwake();  onAwake ( ) ;   Code Example - HandInteractor\u200b HandInteractor processes information from HandInputData to interact with scene objects in the lens that have an Interactable component. For more information about the interaction system, refer to Interaction System. TypeScriptJavaScriptimport { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); TypeScript JavaScript import { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); import { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }} import { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }} import { SIK } from './SpectaclesInteractionKit/SIK';import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';@componentexport class ExampleHandScript extends BaseScriptComponent {  onAwake() {    this.createEvent('UpdateEvent').bind(() => {      this.onUpdate();    });  }  onUpdate() {    // Retrieve InteractionManager from SIK's definitions.    let interactionManager = SIK.InteractionManager;    // Fetch the HandInteractor for left and right hands.    let leftHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.LeftHand    )[0];    let rightHandInteractor = interactionManager.getInteractorsByType(      InteractorInputType.RightHand    )[0];    // Print the position and direction of the HandInteractors each frame.    print(      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`    );    print(      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`    );  }} import { SIK } from './SpectaclesInteractionKit/SIK'; import   {   SIK   }   from   './SpectaclesInteractionKit/SIK' ;  import { InteractorInputType } from './SpectaclesInteractionKit/Core/Interactor/Interactor';  import   {  InteractorInputType  }   from   './SpectaclesInteractionKit/Core/Interactor/Interactor' ;  @component  @ component  export class ExampleHandScript extends BaseScriptComponent {  export   class   ExampleHandScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('UpdateEvent').bind(() => {      this . createEvent ( 'UpdateEvent' ) . bind ( ( )   =>   {        this.onUpdate();        this . onUpdate ( ) ;      });      } ) ;    }    }      onUpdate() {    onUpdate ( )   {      // Retrieve InteractionManager from SIK's definitions.      // Retrieve InteractionManager from SIK's definitions.      let interactionManager = SIK.InteractionManager;      let  interactionManager  =   SIK . InteractionManager ;        // Fetch the HandInteractor for left and right hands.      // Fetch the HandInteractor for left and right hands.      let leftHandInteractor = interactionManager.getInteractorsByType(      let  leftHandInteractor  =  interactionManager . getInteractorsByType (        InteractorInputType.LeftHand       InteractorInputType . LeftHand     )[0];      ) [ 0 ] ;      let rightHandInteractor = interactionManager.getInteractorsByType(      let  rightHandInteractor  =  interactionManager . getInteractorsByType (        InteractorInputType.RightHand       InteractorInputType . RightHand     )[0];      ) [ 0 ] ;        // Print the position and direction of the HandInteractors each frame.      // Print the position and direction of the HandInteractors each frame.      print(      print (        `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`        ` The left hand interactor is at position:  ${ leftHandInteractor . startPoint }  and is pointing in direction:  ${ leftHandInteractor . direction } . `      );      ) ;      print(      print (        `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`        ` The right hand interactor is at position:  ${ rightHandInteractor . startPoint }  and is pointing in direction:  ${ rightHandInteractor . direction } . `      );      ) ;    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionManager = SIK.InteractionManager;const interactorInputType =  require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('UpdateEvent').bind(() => {    onUpdate();  });}function onUpdate() {  // Fetch the HandInteractor for left and right hands.  var leftHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.LeftHand  )[0];  var rightHandInteractor = interactionManager.getInteractorsByType(    interactorInputType.RightHand  )[0];  // Print the position and direction of the HandInteractors each frame.  print(    `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`  );  print(    `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`  );}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const interactionManager = SIK.InteractionManager;  const  interactionManager  =   SIK . InteractionManager ;  const interactorInputType =  const  interactorInputType  =    require('SpectaclesInteractionKit/Core/Interactor/Interactor').InteractorInputType;    require ( 'SpectaclesInteractionKit/Core/Interactor/Interactor' ) . InteractorInputType ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('UpdateEvent').bind(() => {   script . createEvent ( 'UpdateEvent' ) . bind ( ( )   =>   {      onUpdate();      onUpdate ( ) ;    });    } ) ;  }  }    function onUpdate() {  function   onUpdate ( )   {    // Fetch the HandInteractor for left and right hands.    // Fetch the HandInteractor for left and right hands.    var leftHandInteractor = interactionManager.getInteractorsByType(    var  leftHandInteractor  =  interactionManager . getInteractorsByType (      interactorInputType.LeftHand     interactorInputType . LeftHand    )[0];    ) [ 0 ] ;    var rightHandInteractor = interactionManager.getInteractorsByType(    var  rightHandInteractor  =  interactionManager . getInteractorsByType (      interactorInputType.RightHand     interactorInputType . RightHand    )[0];    ) [ 0 ] ;      // Print the position and direction of the HandInteractors each frame.    // Print the position and direction of the HandInteractors each frame.    print(    print (      `The left hand interactor is at position: ${leftHandInteractor.startPoint} and is pointing in direction: ${leftHandInteractor.direction}.`      ` The left hand interactor is at position:  ${ leftHandInteractor . startPoint }  and is pointing in direction:  ${ leftHandInteractor . direction } . `    );    ) ;    print(    print (      `The right hand interactor is at position: ${rightHandInteractor.startPoint} and is pointing in direction: ${rightHandInteractor.direction}.`      ` The right hand interactor is at position:  ${ rightHandInteractor . startPoint }  and is pointing in direction:  ${ rightHandInteractor . direction } . `    );    ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Interaction System Next Mobile Controller Relevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Relevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Relevant ComponentsCode Example - Hand Input DataCode Example - HandInteractor Code Example - Hand Input Data Code Example - HandInteractor AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/cursor": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand VisualizationRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand Visualization Spectacles FrameworksSpectacles Interaction KitFeaturesCursorOn this pageCopy pageCursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake();Was this page helpful?YesNoPreviousMobile ControllerNextHand Visualization  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Cursor Cursor On this page Copy page  Copy page     page Cursor\nSpectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects.\n\nTo enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default.\n\nRelevant Components\u200b\nInteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state.\nCursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration.\nCode Example\u200b\nTypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); Cursor Spectacles Interaction Kit provides control and targeting feedback through the cursor components. To support scenarios where users interact from a distance greater than arm\u2019s reach (the \u201cfarfield\u201d use case), the InteractorCursor projects along the direction of the user's Interactor, helping users understand how to target more distant objects. The InteractorCursor offers automatic feedback when targeting objects. To enable each Interactor to benefit from cursor feedback, Spectacles Interaction Kit provides the CursorController component. This component automatically instantiates and manages an InteractorCursor for each Interactor in the scene. The CursorController is included in the starter project by default. Relevant Components\u200b InteractorCursor reads the state of an Interactor (such as position, direction, and target) to reposition the cursor and adjust visual feedback to reflect current state. CursorController automatically instantiates an InteractorCursor for each Interactor within the scene, streamlining integration. Code Example\u200b TypeScriptJavaScriptimport { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); TypeScript JavaScript import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }} import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }} import { SIK } from 'SpectaclesInteractionKit/SIK';@componentexport class ExampleCursorScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    let cursorController = SIK.CursorController;    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    for (const cursor of cursors) {      cursor.enabled = false;    }  }} import { SIK } from 'SpectaclesInteractionKit/SIK'; import   {   SIK   }   from   'SpectaclesInteractionKit/SIK' ;    @component  @ component  export class ExampleCursorScript extends BaseScriptComponent {  export   class   ExampleCursorScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      onStart() {    onStart ( )   {      let cursorController = SIK.CursorController;      let  cursorController  =   SIK . CursorController ;        // Disable the visual of all cursors in the scene.      // Disable the visual of all cursors in the scene.      let cursors = cursorController.getAllCursors();      let  cursors  =  cursorController . getAllCursors ( ) ;      for (const cursor of cursors) {      for   ( const  cursor  of  cursors )   {        cursor.enabled = false;       cursor . enabled  =   false ;      }      }    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const cursorController = SIK.CursorController;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // Disable the visual of all cursors in the scene.  let cursors = cursorController.getAllCursors();  for (const cursor of cursors) {    cursor.enabled = false;  }}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const cursorController = SIK.CursorController;  const  cursorController  =   SIK . CursorController ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    // Disable the visual of all cursors in the scene.    // Disable the visual of all cursors in the scene.    let cursors = cursorController.getAllCursors();    let  cursors  =  cursorController . getAllCursors ( ) ;    for (const cursor of cursors) {    for   ( const  cursor  of  cursors )   {      cursor.enabled = false;     cursor . enabled   =   false ;    }    }  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Mobile Controller Next Hand Visualization Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/hand-visualization": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor DynamicsRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor Dynamics Spectacles FrameworksSpectacles Interaction KitFeaturesHand VisualizationOn this pageCopy pageHand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake();Was this page helpful?YesNoPreviousCursorNextAnchor Dynamics  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Hand Visualization Hand Visualization On this page Copy page  Copy page     page Hand Visualization\nSpectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations.\n\nYou can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time.\nRelevant Components\u200b\nHandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism.\nLens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame.\nCode Example\u200b\nTypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); Hand Visualization Spectacles Interaction Kit modifies the user\u2019s hand visualization with effects to enhance AR interactions. For example, the Pinch Glow effect brightens when the user pinches, and the Hand Occluder hides AR objects behind the center of the hand\u2019s palm. Use the HandVisual component to toggle active visualizations. You can also create custom effects using the ObjectTracking3D API. By setting attachment points on specific hand keypoints, custom objects and effects can be configured to follow these keypoints in real-time. Relevant Components\u200b HandVisual processes information from HandInputData to overlay visuals on the hand. Its main functions are to provide a glow on the fingertips when pinching and to occlude SceneObjects behind the hand, adding a sense of realism. Lens Studio\u2019s ObjectTracking3D allows you to set attachment points onto a hand\u2019s keypoint. This is useful for creating custom visual effects that follow targeted hand keypoints on each frame. Code Example\u200b TypeScriptJavaScript@componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); TypeScript JavaScript @componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }}// @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); @componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }} @componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }} @componentexport class ExampleHandVisualizationScript extends BaseScriptComponent {  @input()  visualSceneObject: SceneObject;  @input()  rightHand: HandTracking3DAsset;  private objectTracking3DComponent: ObjectTracking3D;  onAwake() {    this.objectTracking3DComponent = this.sceneObject.createComponent(      'Component.ObjectTracking3D'    );    this.objectTracking3DComponent.trackingAsset = this.rightHand;    this.objectTracking3DComponent.addAttachmentPoint(      'index-3',      this.visualSceneObject    );  }} @component @ component  export class ExampleHandVisualizationScript extends BaseScriptComponent {  export   class   ExampleHandVisualizationScript   extends   BaseScriptComponent   {    @input()    @ input ( )    visualSceneObject: SceneObject;   visualSceneObject :  SceneObject ;      @input()    @ input ( )    rightHand: HandTracking3DAsset;   rightHand :  HandTracking3DAsset ;      private objectTracking3DComponent: ObjectTracking3D;    private  objectTracking3DComponent :  ObjectTracking3D ;      onAwake() {    onAwake ( )   {      this.objectTracking3DComponent = this.sceneObject.createComponent(      this . objectTracking3DComponent  =   this . sceneObject . createComponent (        'Component.ObjectTracking3D'        'Component.ObjectTracking3D'      );      ) ;      this.objectTracking3DComponent.trackingAsset = this.rightHand;      this . objectTracking3DComponent . trackingAsset  =   this . rightHand ;      this.objectTracking3DComponent.addAttachmentPoint(      this . objectTracking3DComponent . addAttachmentPoint (        'index-3',        'index-3' ,        this.visualSceneObject        this . visualSceneObject     );      ) ;    }    }  }  }   // @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); // @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); // @input SceneObject visualSceneObject// @input Component.HandTracking3DAsset rightHandconst SIK = require('SpectaclesInteractionKit/SIK').SIK;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const objectTracking3DComponent = script.sceneObject.createComponent(    'Component.ObjectTracking3D'  );  objectTracking3DComponent.trackingAsset = script.rightHand;  objectTracking3DComponent.addAttachmentPoint(    'index-3',    script.visualSceneObject  );}onAwake(); // @input SceneObject visualSceneObject // @input SceneObject visualSceneObject  // @input Component.HandTracking3DAsset rightHand  // @input Component.HandTracking3DAsset rightHand    const SIK = require('SpectaclesInteractionKit/SIK').SIK;  const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    const objectTracking3DComponent = script.sceneObject.createComponent(    const  objectTracking3DComponent  =  script . sceneObject . createComponent (      'Component.ObjectTracking3D'      'Component.ObjectTracking3D'    );    ) ;    objectTracking3DComponent.trackingAsset = script.rightHand;   objectTracking3DComponent . trackingAsset   =  script . rightHand ;    objectTracking3DComponent.addAttachmentPoint(   objectTracking3DComponent . addAttachmentPoint (      'index-3',      'index-3' ,      script.visualSceneObject     script . visualSceneObject    );    ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Cursor Next Anchor Dynamics Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/anchor-dynamics": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI ElementsRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI Elements Spectacles FrameworksSpectacles Interaction KitFeaturesAnchor DynamicsOn this pageCopy pageAnchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake();Was this page helpful?YesNoPreviousHand VisualizationNextUI Elements  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Anchor Dynamics Anchor Dynamics On this page Copy page  Copy page     page Anchor Dynamics\nSpectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction.\nRelevant Components\u200b\nBillboard rotates a SceneObject to always face the user\u2019s position.\nHeadlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze.\nCode Example\u200b\nTypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); Anchor Dynamics Spectacles Interaction Kit provides components that dynamically anchor objects relative to the user. The Billboard component ensures that a SceneObject always faces the user by updating its rotation based on the user\u2019s position every frame. The Headlock component keeps a SceneObject within the user's field of view by adjusting its position according to the user\u2019s gaze direction. Relevant Components\u200b Billboard rotates a SceneObject to always face the user\u2019s position. Headlock positions a SceneObject to maintain the same position relative to the user\u2019s gaze. Code Example\u200b TypeScriptJavaScriptimport { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); TypeScript JavaScript import { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); import { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }} import { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }} import { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard';import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleAnchorDynamicsScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    let billboard = this.sceneObject.getComponent(Billboard.getTypeName());    // Disable billboarding initially.    billboard.xAxisEnabled = false;    billboard.yAxisEnabled = false;    billboard.zAxisEnabled = false;    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {      billboard.yAxisEnabled = state;    });  }} import { Billboard } from 'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard'; import   {  Billboard  }   from   'SpectaclesInteractionKit/Components/Interaction/Billboard/Billboard' ;  import { Interactable } from 'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable';  import   {  Interactable  }   from   'SpectaclesInteractionKit/Components/Interaction/Interactable/Interactable' ;  import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';  import   {  ToggleButton  }   from   'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton' ;    @component  @ component  export class ExampleAnchorDynamicsScript extends BaseScriptComponent {  export   class   ExampleAnchorDynamicsScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      onStart() {    onStart ( )   {      // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.      // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.      let billboard = this.sceneObject.getComponent(Billboard.getTypeName());      let  billboard  =   this . sceneObject . getComponent ( Billboard . getTypeName ( ) ) ;        // Disable billboarding initially.      // Disable billboarding initially.      billboard.xAxisEnabled = false;     billboard . xAxisEnabled  =   false ;      billboard.yAxisEnabled = false;     billboard . yAxisEnabled  =   false ;      billboard.zAxisEnabled = false;     billboard . zAxisEnabled  =   false ;        let toggleButton = this.sceneObject.getComponent(      let  toggleButton  =   this . sceneObject . getComponent (        ToggleButton.getTypeName()       ToggleButton . getTypeName ( )      );      ) ;        // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.      // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.      toggleButton.onStateChanged.add((state) => {     toggleButton . onStateChanged . add ( ( state )   =>   {        billboard.yAxisEnabled = state;       billboard . yAxisEnabled  =  state ;      });      } ) ;    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.  var billboard = script.sceneObject.getComponent(    interactionConfiguration.requireType('Billboard')  );  // Disable billboarding initially.  billboard.xAxisEnabled = false;  billboard.yAxisEnabled = false;  billboard.zAxisEnabled = false;  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.  toggleButton.onStateChanged.add((state) => {    billboard.yAxisEnabled = state;  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const interactionConfiguration = SIK.InteractionConfiguration;  const  interactionConfiguration  =   SIK . InteractionConfiguration ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    // This script assumes that a Billboard (and ToggleButton + Interactable + Collider) component have already been instantiated on the SceneObject.    var billboard = script.sceneObject.getComponent(    var  billboard  =  script . sceneObject . getComponent (      interactionConfiguration.requireType('Billboard')     interactionConfiguration . requireType ( 'Billboard' )    );    ) ;      // Disable billboarding initially.    // Disable billboarding initially.    billboard.xAxisEnabled = false;   billboard . xAxisEnabled   =   false ;    billboard.yAxisEnabled = false;   billboard . yAxisEnabled   =   false ;    billboard.zAxisEnabled = false;   billboard . zAxisEnabled   =   false ;      var toggleButton = script.sceneObject.getComponent(    var  toggleButton  =  script . sceneObject . getComponent (      interactionConfiguration.requireType('ToggleButton')     interactionConfiguration . requireType ( 'ToggleButton' )    );    ) ;      // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    // Whenever toggling the state of the button, enable billboarding along the y-axis to reflect the state.    toggleButton.onStateChanged.add((state) => {   toggleButton . onStateChanged . add ( ( state )   =>   {      billboard.yAxisEnabled = state;     billboard . yAxisEnabled   =  state ;    });    } ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Hand Visualization Next UI Elements Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/ui-elements": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI CompositesRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI Composites Spectacles FrameworksSpectacles Interaction KitFeaturesUI ElementsOn this pageCopy pageUI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake();Was this page helpful?YesNoPreviousAnchor DynamicsNextUI Composites  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features UI Elements UI Elements On this page Copy page  Copy page     page UI Elements\nSpectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK.\nBy using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details.\nRelevant Components\u200b\nPinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic.\nToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered.\nSlider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume.\nCode Example\u200b\nTypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); UI Elements Spectacles Interaction Kit provides various UI elements you can add to your AR scenes to create interactive experiences. These elements include PinchButton, ToggleButton, Slider, and ScrollBar. Each element is designed to be intuitive and responsive. You can customize them with helper components to add audio and visual feedback, making their states clear to users. Use event callbacks to connect user interactions with specific behaviors and outcomes. These UI elements extend the Interactable component, ensuring compatibility with any Interactor and interaction modality supported by SIK. By using the pre-built UI elements in SIK, you can focus on designing unique user experiences rather than managing technical implementation details. Relevant Components\u200b PinchButton allows an Interactable to be triggered, and it broadcasts the onButtonPinched event for lens-specific callback logic. ToggleButton allows an Interactable to be triggered, and it broadcasts the onStateChanged event. Unlike PinchButton, this component maintains a stored state that toggles and is sent via the event payload when triggered. Slider enables an Interactable to be moved along a track and broadcasts the onValueUpdate event whenever it is adjusted. This is useful for setting values such as volume. Code Example\u200b TypeScriptJavaScriptimport { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); TypeScript JavaScript import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }} import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }} import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton';@componentexport class ExampleUIScript extends BaseScriptComponent {  onAwake() {    this.createEvent('OnStartEvent').bind(() => {      this.onStart();    });  }  onStart() {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    let toggleButton = this.sceneObject.getComponent(      ToggleButton.getTypeName()    );    let onStateChangedCallback = (state: boolean) => {      print(`The toggle button has been triggered, setting to state: ${state}`);    };    toggleButton.onStateChanged.add(onStateChangedCallback);  }} import { ToggleButton } from 'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton'; import   {  ToggleButton  }   from   'SpectaclesInteractionKit/Components/UI/ToggleButton/ToggleButton' ;    @component  @ component  export class ExampleUIScript extends BaseScriptComponent {  export   class   ExampleUIScript   extends   BaseScriptComponent   {    onAwake() {    onAwake ( )   {      this.createEvent('OnStartEvent').bind(() => {      this . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {        this.onStart();        this . onStart ( ) ;      });      } ) ;    }    }      onStart() {    onStart ( )   {      // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.      // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.      let toggleButton = this.sceneObject.getComponent(      let  toggleButton  =   this . sceneObject . getComponent (        ToggleButton.getTypeName()       ToggleButton . getTypeName ( )      );      ) ;        let onStateChangedCallback = (state: boolean) => {      let   onStateChangedCallback   =   ( state :   boolean )   =>   {        print(`The toggle button has been triggered, setting to state: ${state}`);        print ( ` The toggle button has been triggered, setting to state:  ${ state } ` ) ;      };      } ;        toggleButton.onStateChanged.add(onStateChangedCallback);     toggleButton . onStateChanged . add ( onStateChangedCallback ) ;    }    }  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.  var toggleButton = script.sceneObject.getComponent(    interactionConfiguration.requireType('ToggleButton')  );  var onStateChangedCallback = (state) => {    print(`The toggle button has been triggered, setting to state: ${state}`);  };  toggleButton.onStateChanged.add(onStateChangedCallback);}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const interactionConfiguration = SIK.InteractionConfiguration;  const  interactionConfiguration  =   SIK . InteractionConfiguration ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    // This script assumes that a ToggleButton (and Interactable + Collider) component have already been instantiated on the SceneObject.    var toggleButton = script.sceneObject.getComponent(    var  toggleButton  =  script . sceneObject . getComponent (      interactionConfiguration.requireType('ToggleButton')     interactionConfiguration . requireType ( 'ToggleButton' )    );    ) ;      var onStateChangedCallback = (state) => {    var   onStateChangedCallback   =   ( state )   =>   {      print(`The toggle button has been triggered, setting to state: ${state}`);      print ( ` The toggle button has been triggered, setting to state:  ${ state } ` ) ;    };    } ;      toggleButton.onStateChanged.add(onStateChangedCallback);   toggleButton . onStateChanged . add ( onStateChangedCallback ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Anchor Dynamics Next UI Composites Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/ui-composites": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode ExampleAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode Example Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpersRelevant ComponentsCode Example Spectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpers Spectacles FrameworksSpectacles Interaction KitFeaturesUI CompositesOn this pageCopy pageUI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake();Was this page helpful?YesNoPreviousUI ElementsNextHelpers  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features UI Composites UI Composites On this page Copy page  Copy page     page UI Composites\nSpectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space.\nThe ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container.\nBy using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components.\nRelevant Components\u200b\nScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor.\nScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView.\nContainer acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences.\n\nTo use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy:\n\nThe component as it appears in Lens Studio, with settings configured for the above scene.\nAuto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container\n\n\nBillboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n\nSnapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n\nFollow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n\nClose Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n\n\nCode Example\u200b\nTypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); UI Composites Spectacles Interaction Kit provides UI composites for creating sophisticated AR interfaces. For example, the Container composite component serves as a window for AR content (with various customization options), that allows users to view and manipulate assets in their physical space. The ScrollView is a UI composite component that allows scroll interactions within a defined visual boundary using an Interactor. This boundary hides UI elements outside of it, offering a familiar 2D scrolling experience similar to interacting with a panel or screen. ScrollView is ideal for creating vertically or horizontally scrollable lists and menus, enhancing navigation in your AR application, and can also be used within a Container. By using these ready-made components, you can focus on crafting unique user experiences instead of developing your own UI compositing components. Relevant Components\u200b ScrollView is a component for displaying scrollable content, such as lists of friends. It must be placed under a Canvas component in the hierarchy. All children of the ScrollView can be scrolled by dragging with an Interactor. ScrollBar serves as a visual representation of how scrolled the content of a ScrollView is. The ScrollBar can also be dragged to move the ScrollView. Container acts as a window for your content on Spectacles \u2013 an interactive UX solution for AR experiences. To use the Container in Lens Studio as a component, add the ContainerFrame component to an empty scene object. Then, add your content as a child to the container object as in the following hierarchy: The component as it appears in Lens Studio, with settings configured for the above scene. Auto Show HideShow and Hide Container on hoverInner Size: The size of the content areaBorder: The thickness of the borderConstant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example.Allow Scaling: Allows scaling of the Container from the cornerAuto Scale Content: Let the Container handle the rescaling of your contentIs Content Interactable: Disables Container interactions within the\ncontent area.Allow Translation: Allows translation manipulation of the ContainerMin/Max Size: Minimum and Maximum size for scaling the container Auto Show Hide Show and Hide Container on hover Inner Size: The size of the content area Border: The thickness of the border Constant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example. Constant padding: An option to add a constant (un-scaled) padding on the x\nor the y. Useful for toolbars for example. Allow Scaling: Allows scaling of the Container from the corner Auto Scale Content: Let the Container handle the rescaling of your content Auto Scale Content: Let the Container handle the rescaling of your content Is Content Interactable: Disables Container interactions within the\ncontent area. Is Content Interactable: Disables Container interactions within the\ncontent area. Allow Translation: Allows translation manipulation of the Container Min/Max Size: Minimum and Maximum size for scaling the container Billboarding\n\nA behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n\n\n A behavior for the Container to face the camera\n\nCan turn on during translation or always on either the x or the y\n\nOn translation or always\n\n\n\n Can turn on during translation or always on either the x or the y\n\nOn translation or always\n\n On translation or always Snapping\n\nItem to Item snapping\n\nContainers can snap to each other for positioning\n\n\nWorld Snapping\n\nContainers can also snap to the real world\n\n\n\n Item to Item snapping\n\nContainers can snap to each other for positioning\n\n Containers can snap to each other for positioning World Snapping\n\nContainers can also snap to the real world\n\n Containers can also snap to the real world Follow Behavior\n\nShow follow button\nFront Follow Behavior attaches the default follow behavior to the button\nIs Following turns on the following behavior on creation\n\n Show follow button Front Follow Behavior attaches the default follow behavior to the button Is Following turns on the following behavior on creation Close Button\n\nShow Close Button\nNote: The close button has no attached default function. See example of attaching your close logic below:\n\n Show Close Button Note: The close button has no attached default function. See example of attaching your close logic below: Code Example\u200b TypeScriptJavaScriptimport { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); TypeScript JavaScript import { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })}const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); import { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })} import { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })} import { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"@componentexport class ExampleContainerScript extends BaseScriptComponent {  container: ContainerFrame  onAwake() {    this.container = this.sceneObject.createComponent(    ContainerFrame.getTypeName()  )  this.container.enableCloseButton(true)  this.container.enableFollowButton(true)  this.container.autoShowHide = false  this.container.showVisual()  this.container.closeButton.onTrigger.add(() => {    // close function logic here  })} import { ContainerFrame } from \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\" import   {  ContainerFrame  }   from   \"./SpectaclesInteractionKit/Components/UI/ContainerFrame/ContainerFrame\"    @component  @ component  export class ExampleContainerScript extends BaseScriptComponent {  export   class   ExampleContainerScript   extends   BaseScriptComponent   {    container: ContainerFrame   container :  ContainerFrame     onAwake() {    onAwake ( )   {      this.container = this.sceneObject.createComponent(      this . container  =   this . sceneObject . createComponent (      ContainerFrame.getTypeName()     ContainerFrame . getTypeName ( )    )    )      this.container.enableCloseButton(true)    this . container . enableCloseButton ( true )    this.container.enableFollowButton(true)    this . container . enableFollowButton ( true )    this.container.autoShowHide = false    this . container . autoShowHide  =   false    this.container.showVisual()    this . container . showVisual ( )      this.container.closeButton.onTrigger.add(() => {    this . container . closeButton . onTrigger . add ( ( )   =>   {      // close function logic here      // close function logic here    })    } )  }  }   const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK;const interactionConfiguration = SIK.InteractionConfiguration;function onAwake() {  // Wait for other components to initialize by deferring to OnStartEvent.  script.createEvent('OnStartEvent').bind(() => {    onStart();  });}function onStart() {  const container = this.sceneObject.createComponent(    interactionConfiguration.requireType('ContainerFrame')  );  container.enableCloseButton(true);  container.enableFollowButton(true);  container.autoShowHide = false;  container.showVisual();  this.container.closeButton.onTrigger.add(() => {    // close function logic here  });}onAwake(); const SIK = require('SpectaclesInteractionKit/SIK').SIK; const   SIK   =   require ( 'SpectaclesInteractionKit/SIK' ) . SIK ;  const interactionConfiguration = SIK.InteractionConfiguration;  const  interactionConfiguration  =   SIK . InteractionConfiguration ;    function onAwake() {  function   onAwake ( )   {    // Wait for other components to initialize by deferring to OnStartEvent.    // Wait for other components to initialize by deferring to OnStartEvent.    script.createEvent('OnStartEvent').bind(() => {   script . createEvent ( 'OnStartEvent' ) . bind ( ( )   =>   {      onStart();      onStart ( ) ;    });    } ) ;  }  }    function onStart() {  function   onStart ( )   {    const container = this.sceneObject.createComponent(    const  container  =   this . sceneObject . createComponent (      interactionConfiguration.requireType('ContainerFrame')     interactionConfiguration . requireType ( 'ContainerFrame' )    );    ) ;      container.enableCloseButton(true);   container . enableCloseButton ( true ) ;    container.enableFollowButton(true);   container . enableFollowButton ( true ) ;    container.autoShowHide = false;   container . autoShowHide   =   false ;    container.showVisual();   container . showVisual ( ) ;      this.container.closeButton.onTrigger.add(() => {    this . container . closeButton . onTrigger . add ( ( )   =>   {      // close function logic here      // close function logic here    });    } ) ;  }  }    onAwake();  onAwake ( ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous UI Elements Next Helpers Relevant ComponentsCode Example Relevant ComponentsCode Example Relevant ComponentsCode Example Code Example AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/helper": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant ComponentsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant Components Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant Components Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant Components Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease NotesSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction KitGetting StartedFeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpersArchitectureComponents ListRelease Notes Spectacles Interaction Kit Getting Started FeaturesOverviewInteraction SystemHand TrackingMobile ControllerCursorHand VisualizationAnchor DynamicsUI ElementsUI CompositesHelpers Features Overview Interaction System Hand Tracking Mobile Controller Cursor Hand Visualization Anchor Dynamics UI Elements UI Composites Helpers Architecture Components List Release Notes Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant Components Spectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitectureRelevant Components Spectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitecture Spectacles FrameworksSpectacles Interaction KitFeaturesHelpersOn this pageCopy pageHelpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions.Was this page helpful?YesNoPreviousUI CompositesNextArchitecture  Spectacles Frameworks Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Features Features Helpers Helpers On this page Copy page  Copy page     page Helpers\nSpectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component.\nThese helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles.\nRelevant Components\u200b\nCapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons.\nInteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable.\nInteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable.\nInteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable.\nInteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable.\nScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView).\nSliderFeedback customizes the visual feedback for a Slider component\u2019s interactions.\nToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions. Helpers Spectacles Interaction Kit includes helper components that provide common callback logic, such as audio feedback when triggering an Interactable or visual feedback when hovering over an Interactable. These helper scripts generally require the SceneObject to have an Interactable component. These helper components also serve as a reference for developing your own components that integrate with SIK components and modules. Creating custom components allows for greater creativity and the development of unique experiences for Spectacles. Relevant Components\u200b CapsuleMeshCustomizer creates a custom capsule mesh for some of SIK\u2019s lengthier capsule buttons. InteractableAudioFeedback attaches an audio feedback component when hovering / triggering an Interactable. InteractableColorFeedback attaches a color feedback component when hovering / triggering an Interactable. InteractableOutlineFeedback attaches an outline feedback component when hovering / triggering an Interactable. InteractableSquishFeedback attaches a squish feedback component when hovering / triggering an Interactable. ScreenTransformAdapter forces a SceneObject to use ScreenTransform to comply with certain UI components (ScrollView). SliderFeedback customizes the visual feedback for a Slider component\u2019s interactions. ToggleFeedback customizes the visual feedback for a ToggleButton component\u2019s interactions. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous UI Composites Next Architecture Relevant Components Relevant Components Relevant Components AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/getting-started": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee AlsoAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeatures Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeatures Spectacles Sync Kit Getting Started Lifecycle Features Features Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Spectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecyclePrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Spectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecycle Spectacles FrameworksSpectacles Sync KitGetting StartedOn this pageCopy pageGetting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components.Was this page helpful?YesNoPreviousRelease NotesNextLifecycle  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Getting Started Getting Started On this page Copy page  Copy page     page Getting Started\nPrerequisites\u200b\n\nLens Studio v.5.4 or later\nSpectacles SnapOS v.5.59 or later\n\nAbout Spectacles Sync Kit [Beta]\u200b\nSpectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences.\nTo start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio.\nKey Features\u200b\nThis list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit.\n\nSessionController: Access information about the current session using SessionController APIs.\nSyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class.\nStorage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes.\nNetworked Events: Add networked events to SyncEntities to send one-time messages across the network.\nInstantiator: Instantiate prefabs across the network.\nLifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use.\n\nPackage Resources\u200b\nThe Spectacles Sync Kit package includes the following resources when imported into your Asset Browser:\n\nSpectacles Interaction Kit: Included in the package, does not need to be installed separately.\nConnected Lens Module: Required to connect to the Connected Lenses backend.\nLocation Cloud Storage Module: Required for colocated mapping.\nCore: TypeScript files required for Connected Lenses to function.\nComponents: TypeScript components and helpers to aid Lens development.\nMapping: Resources that handle joining and colocated mapping flow logic and UI.\nExamples: Resources that demonstrate features and components, including TypeScript and JavaScript files.\n\nStarting a New project\u200b\nOption 1: Starting a New Connected Lenses Project\u200b\n\nIn the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.\n\n\n\n\nCheck that you are logged in to Lens Studio. If not, log in.\n\n\nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n\n\n\nOption 2: Importing Spectacles Sync Kit into a New Lens Studio Project\u200b\nTo start building a Connected Lenses project for Spectacles from scratch, follow these steps:\n\nFollow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles.\n\nDo not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.\n\nInstall the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.\n\n\n\nImport Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.\n\n\n\nAfter the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.\n\n\nYou can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.\n\nFrom the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy.\n\nInstantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.\n\n\nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n\n\nThe import is complete. Save your project!\n\n\nUpdating an Existing Project\u200b\nTo import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps:\nImporting Spectacles Sync Kit into an Existing Lens Studio Project that Contains SIK\u200b\nTo import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps:\n\nSave and close your project.\nWith your project closed, navigate to your project files in your file system.\nIn the Project > Assets folder, delete the SpectaclesInteractionKit package folder.\nOpen your project in Lens Studio.\nDelete the SpectaclesInteractionKit scene object from the scene hierarchy.\nFollow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project.\nThe Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package.\n\nImporting Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.\nUpdating Spectacles Sync Kit Within a Project\u200b\nTo upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps:\n\n\nOpen your project in Lens Studio.\n\n\nLog in to Lens Studio.\n\n\nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n\n\nIn the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.\nUpgrading from Sync Framework\u200b\nPrevious versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps:\n\n\nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n\n\nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n\n\nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n\n\nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n\n\nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n\n\nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n\n\nSee Also\u200b\nBuilding Connected Lenses\u200b\nMore information about building and testing Connected Lenses in Lens Studio and on Spectacles.\nSample Projects\u200b\nExamples demonstrating Spectacles Sync Kit features and components. Getting Started Prerequisites\u200b Lens Studio v.5.4 or later Spectacles SnapOS v.5.59 or later About Spectacles Sync Kit [Beta]\u200b Spectacles Sync Kit is a Lens Studio package that enables the development of Connected Lenses for Spectacles. The package includes scripts and components that can be used to sync data among Spectacles users in shared experiences. To start building Connected Lenses using Spectacles Sync Kit, follow the instructions below to start a new project or update an existing project in Lens Studio. This list outlined key features to be familiar with when building Connected Lenses with Spectacles Sync Kit. SessionController: Access information about the current session using SessionController APIs. SyncEntity: Turn any script into an entity that can synchronize data using the SyncEntity class. Storage Properties: Add storage properties to SyncEntities to store data and notify the network of state changes. Networked Events: Add networked events to SyncEntities to send one-time messages across the network. Instantiator: Instantiate prefabs across the network. Lifecycle: Understand the setup flow of a Connected Lens session and when features are ready to use. The Spectacles Sync Kit package includes the following resources when imported into your Asset Browser: Spectacles Interaction Kit: Included in the package, does not need to be installed separately. Connected Lens Module: Required to connect to the Connected Lenses backend. Location Cloud Storage Module: Required for colocated mapping. Core: TypeScript files required for Connected Lenses to function. Components: TypeScript components and helpers to aid Lens development. Mapping: Resources that handle joining and colocated mapping flow logic and UI. Examples: Resources that demonstrate features and components, including TypeScript and JavaScript files. Starting a New project\u200b In the Lens Studio home under Sample Projects, open the Connected Spectacles [BETA] project. This project already includes the Spectacles Sync Kit package and is configured for Spectacles development.  \nCheck that you are logged in to Lens Studio. If not, log in.\n Check that you are logged in to Lens Studio. If not, log in. \nIn the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.\n In the Asset Browser, right click on the Spectacles Sync Kit package and select Register to Library. Registering the package to your library ensures that future updates can be detected. If you already have the package registered to your library, this option will not be available.  To start building a Connected Lenses project for Spectacles from scratch, follow these steps: Follow steps 1 to 5 of Importing Spectacles Interaction Kit into a new Lens Studio Project to configure your project for Spectacles. Do not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package.   Do not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package. Do not import Spectacles Interaction Kit (SIK) into your project. SIK is already included in the Spectacles Sync Kit package. Install the latest Spectacles Sync Kit version from the Asset Library via Window > Asset Library > Spectacles > Spectacles Sync Kit.  Import Spectacles Sync Kit by clicking the + symbol on the Asset Browser. Under the Installed Packages tab, select SpectaclesSyncKit.  After the TypeScript Status panel confirms that compilation is complete, click the Refresh button on the Preview panel.  You can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path.   You can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path. You can also unpack the Spectacles Sync Kit package by right-clicking and selecting Unpack. Unpacking enables direct editing of Spectacles Sync Kit but may complicate future upgrades to newer versions. Additionally, when unpacked, import components and modules in your TypeScript files using SpectaclesSyncKit rather than SpectaclesSyncKit.lspkg in your import path. From the Asset Browser, drag Assets/SpectaclesSyncKit/SpectaclesSyncKit.prefab into the Scene Hierarchy. Instantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy.   Instantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy. Instantiating the prefab before TypeScript compilation is complete may result in a corrupt prefab with missing links to TypeScript script components. Ensure that TypeScript compilation is fully complete before adding a prefab instance to the scene hierarchy. \nIf the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy.\n If the Examples included in the SpectaclesSyncKit prefab are no longer needed, disable or delete them from the Scene Hierarchy. \nThe import is complete. Save your project!\n The import is complete. Save your project! Updating an Existing Project\u200b To import Spectacles Sync Kit into an existing project that already contains Spectacles Interaction Kit, follow these steps: To import Spectacles Sync Kit to an existing project that already contains Spectacles Interaction Kit (SIK), follow these steps: Save and close your project. With your project closed, navigate to your project files in your file system. In the Project > Assets folder, delete the SpectaclesInteractionKit package folder. Open your project in Lens Studio. Delete the SpectaclesInteractionKit scene object from the scene hierarchy. Follow the steps above under Starting a New Project > Option 2: Importing Spectacles into a New Lens Studio Project. The Spectacles Sync Kit references in your project (e.g., in the scene hierarchy and prefabs) should automatically relink to the SpectaclesInteractionKit included in the imported SpectaclesSyncKit package. Importing Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved.   Importing Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved. Importing Spectacles Sync Kit before deleting Spectacles Interaction Kit may cause Lens Studio 5.4 to freeze. This issue will be resolved. To upgrade Spectacles Sync Kit in a project that already includes the package, follow these steps: \nOpen your project in Lens Studio.\n Open your project in Lens Studio. \nLog in to Lens Studio.\n Log in to Lens Studio. \nIn the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio.\n In the Asset Browser panel, right click on the SpectaclesSyncKit package folder. Select Pull Update from Library. If this option is not available in the context menu, check that you are logged in to Lens Studio. In the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail.   In the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail. In the Asset Library, the Spectacles Sync Kit package thumbnail will show if an update is available. The method outlined above is the recommended way to update your package, rather than selecting Update on the Asset Library thumbnail. Previous versions of Sync Framework for Spectacles and for mobile are incompatible with Spectacles Sync Kit. To upgrade a project from Sync Framework to Spectacles Sync Kit, follow these steps: \nIn the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy.\n In the Scene Hierarchy, pull out any colocated Lens content that you may have within the Sync Framework hierarchy. \nDelete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene.\n Delete the Sync Framework root scene object from the Scene Hierarchy. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit from your scene. \nDelete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser.\n Delete the Sync Framework package or resources from the Asset Browser. Note: If you are using Sync Framework for Spectacles, which includes Spectacles Interaction Kit, this will also delete Spectacles Interaction Kit resources from your Asset Browser. \nImport Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene.\n Import Spectacles Sync Kit into your project from the Asset Library and add the Spectacles Sync Kit prefab to your scene. \nUpdate Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc.\n Update Spectacles Interaction Kit and Spectacles Sync Kit components throughout your project, e.g., components on scene objects, references in your scene, prefabs, etc. \nMove any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy.\n Move any colocated Lens content from Step 1 back into the Spectacles Sync Kit hierarchy. See Also\u200b More information about building and testing Connected Lenses in Lens Studio and on Spectacles. Examples demonstrating Spectacles Sync Kit features and components. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Release Notes Next Lifecycle PrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also PrerequisitesAbout Spectacles Sync Kit [Beta]Starting a New projectUpdating an Existing ProjectSee Also Prerequisites About Spectacles Sync Kit [Beta] Starting a New project Updating an Existing Project See Also AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/lifecycle": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey TakeawaysAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey Takeaways Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey Takeaways Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey Takeaways Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeatures Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeatures Spectacles Sync Kit Getting Started Lifecycle Features Features Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey Takeaways Spectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent PlacementSession Setup StepsSession Setup FlowchartKey Takeaways Spectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent Placement Spectacles FrameworksSpectacles Sync KitLifecycleOn this pageCopy pageLifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\nWas this page helpful?YesNoPreviousGetting StartedNextContent Placement  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Lifecycle Lifecycle On this page Copy page  Copy page     page Lifecycle\nThe following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do.\nSession Setup Steps\u200b\n\nSessionController is initialized.\nSyncEntities in the scene subscribe to events on SessionController.\n\n\u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.\n\nInstantiators in the scene are initialized and subscribe to events on SessionController.\nOn mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed.\n\n\u2705 SessionController is now completely ready to use and SessionController APIs can be called.\n\nAll SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks.\n\n\u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.\n\nWhen each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks.\n\n\u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.\nSession Setup Flowchart\u200b\n\nKey Takeaways\u200b\nThese are the most important points to remember as a developer:\nSessionController\u200b\n\nIf you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true.\n\nSyncEntity\u200b\n\nIf you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point.\nIf you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true.\n\nInstantiator\u200b\n\nIf you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true.\n Lifecycle The following steps and flowchart describe how Spectacles Sync Kit sets up a Connected Lenses session and when certain actions are safe to do. Session Setup Steps\u200b SessionController is initialized. SyncEntities in the scene subscribe to events on SessionController. \u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities.   \u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities. \u2705 It is okay to add storage properties, subscribe to events, and request ownership of SyncEntities. Instantiators in the scene are initialized and subscribe to events on SessionController. On mapping or relocalization success, SessionController begins to set up the session. When the session setup is finished, SessionController.getIsReady() becomes true and all SessionController.notifyOnReady() callbacks are executed. \u2705 SessionController is now completely ready to use and SessionController APIs can be called.   \u2705 SessionController is now completely ready to use and SessionController APIs can be called. \u2705 SessionController is now completely ready to use and SessionController APIs can be called. All SyncEntities begin to initialize themselves. Depending on the situation, they may initialize immediately, or may take up to a second to initialize. When each SyncEntity is ready and fully initialized, they execute their SyncEntity.notifyOnReady() callbacks. \u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received.   \u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received. \u2705 SyncEntity is now completely ready to use. Storage properties can be set and retrieved, and networked events can be sent and received. When each Instantiator is ready and fully initialized, they execute their Instantiator.notifyOnReady() callbacks. \u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network.   \u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network. \u2705 Instantiator is now completely ready to use. Prefabs can be instantiated across the network. Session Setup Flowchart\u200b  Key Takeaways\u200b These are the most important points to remember as a developer: If you need to use SessionController APIs, wait for the SessionController.notifyOnReady() callback to run, or for SessionController.getIsReady() to return true. If you need to set up a SyncEntity by adding storage properties or callbacks, it is fine to do so at any point. If you need to make changes to a SyncEntity (e.g., get / set a storage property) or start its behavior, wait for the SyncEntity.notifyOnReady() callback, or for SyncEntity.isSetupFinished to return true. If you need to instantiate a prefab using the Instantiator, wait for the Instantiator.notifyOnReady() callback, or for Instantiator.isReady() to return true. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Getting Started Next Content Placement Session Setup StepsSession Setup FlowchartKey Takeaways Session Setup StepsSession Setup FlowchartKey Takeaways Session Setup Steps Session Setup Flowchart Key Takeaways AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/content-placement": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to UsersAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to Users Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to Users Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to Users Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to Users Spectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebuggingPlacing Colocated ContentPlacing Content Relative to Users Spectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebugging Spectacles FrameworksSpectacles Sync KitFeaturesContent PlacementOn this pageCopy pageContent Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information.Was this page helpful?YesNoPreviousLifecycleNextDebugging  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Content Placement Content Placement On this page Copy page  Copy page     page Content Placement\nConnected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space.\nAs a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users.\nPlacing Colocated Content\u200b\nIn the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.\n\nPlacing Content Relative to Users\u200b\nConsider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user.\nOne way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users.\nAdd the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.\n\nConfigure the PositionInitializer inputs in the Inspector:\n\nPosition In Front of Camera: Set the x, y, and z offsets relative to the first user.\nShould Face Camera: If enabled, content will initially rotate about its y-axis to face the user.\nTrigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended).\n\nConfigure the SyncTransform. Refer to Sync Transform > Setup for more information. Content Placement Connected Lenses on Spectacles are colocated, meaning they are experienced by multiple users in the same physical space at the same time. Spectacles Sync Kit provides a joining flow for guiding users to either map their space or relocalize against a shared coordinate space. As a developer, it is important to understand how to place content relative to the shared coordinate space and relative to Spectacles users. Placing Colocated Content\u200b In the Scene Hierarchy, the Spectacles Sync Kit package includes a scene object called ColocatedWorld [CONFIGURE_ME]. This object is the root of the shared, colocated coordinate space. Children of the ColocatedWorld scene object are positioned, rotated, and scaled relative to the shared coordinate space.  Placing Content Relative to Users\u200b Consider how users will find colocated content upon joining the session. When a user moves around to map their space or relocalize, they will likely move away from their own world origin and the origin of the shared coordinate space. This means that if content is placed relative to the world origin or ColocatedWorld scene object, it may not be noticeable to users right away. For example, content may appear behind a user. One way to handle content placement is to position shared content relative to the first user to join the session. Spectacles Sync Framework includes a script called PositionInitializer, which can be combined with SyncTransform, to serve this purpose. The PositionInitializer will position content relative to the first user, and SyncTransform will synchronize that placement for subsequent users. Add the PositionInitializer and SyncTransform to the colocated scene object. This could be the root scene object for all shared content, or an individual child.  Configure the PositionInitializer inputs in the Inspector: Position In Front of Camera: Set the x, y, and z offsets relative to the first user. Should Face Camera: If enabled, content will initially rotate about its y-axis to face the user. Trigger Only For Mapper: If enabled, the scene object transform is set only for the first user to join the session. (This is recommended). Configure the SyncTransform. Refer to Sync Transform > Setup for more information. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Lifecycle Next Debugging Placing Colocated ContentPlacing Content Relative to Users Placing Colocated ContentPlacing Content Relative to Users Placing Colocated Content Placing Content Relative to Users AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/debugging": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebugAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebug Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebug Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebug Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebug Spectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper ScriptsSyncEntityDebug Spectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper Scripts Spectacles FrameworksSpectacles Sync KitFeaturesDebuggingOn this pageCopy pageDebugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\nWas this page helpful?YesNoPreviousContent PlacementNextHelper Scripts  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Debugging Debugging On this page Copy page  Copy page     page Debugging\nSpectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow.\nSyncEntityDebug\u200b\nSyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes:\n\nNetwork ID\nOwner Display Name\nOwner ID\nStorage Properties\n\nSetup Steps\u200b\nTo use the SyncEntityDebug in your scene, follow these steps:\n\nAdd text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.\n\n\n\nAdd the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.\n\n\n\n\nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n\n\nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n\n\nOnce the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.\n Debugging Spectacles Sync Kit and Lens Studio provide a number of tools to help debug Connected Lenses on device and in the editor. For more information on Lens Studio Connected Lenses features, including the Connected Lenses Monitor and Multiple Previews, see Connected Lenses > Development Workflow. SyncEntityDebug\u200b SyncEntityDebug is a helper script that can show SyncEntity information using text components. It is especially useful for debugging SyncEntities on device. SyncEntity information that can be shown by the SyncEntityDebug script includes: Network ID Owner Display Name Owner ID Storage Properties To use the SyncEntityDebug in your scene, follow these steps: Add text components in the Scene Hierarchy for each piece of SyncEntity information you want to debug.  Add the SyncEntityDebug script to a scene object in the Scene Hierarchy. Populate the relevant text component inputs in the SyncEntityDebug script Inspector panel. You do not have to populate all inputs.  \nFor Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with.\n For Target Type, use Network Root if you are using SyncEntityDebug script within a prefab. Otherwise, set Target Type to Sync Entity, and populate the Sync Entity Script input with the script that the SyncEntity is associated with. \nThe Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched.\n The Ownership Button can be populated with an SIK Pinch Button, which will clear ownership of the SyncEntity when pinched. Once the SyncEntityDebug is set up, the text components will update to reflect SyncEntity information in the Preview panel and on Spectacles when pushed to device.  Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Content Placement Next Helper Scripts SyncEntityDebug SyncEntityDebug SyncEntityDebug AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/helper-scripts": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwnerAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwner Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwner Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwner Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwner Spectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked EventsDisplayStoragePropertySetEnabledIfOwner Spectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked Events Spectacles FrameworksSpectacles Sync KitFeaturesHelper ScriptsOn this pageCopy pageHelper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled.Was this page helpful?YesNoPreviousDebuggingNextNetworked Events  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Helper Scripts Helper Scripts On this page Copy page  Copy page     page Helper Scripts\nHelper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs.\nDisplayStorageProperty\u200b\nThe DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.\n\n\nThe Property Key should match the one being used by the storage property.\nText is the Text component that will display the value text.\nIf Use Format is enabled, a formatting string can be specified instead of displaying the value directly.\nAlt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet.\n\nSetEnabledIfOwner\u200b\nThe SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.\n\nEntity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation.\nWhenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled.\nWhenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled. Helper Scripts Helper scripts are not SyncEntities, meaning they do not sync data but run locally like any normal script. They are designed to work with SyncEntities and help with common synchronization needs. DisplayStorageProperty\u200b The DisplayStorageProperty helper displays a storage property value found on the specified SyncEntity. This can be useful either as a UI helper or a debugging tool.  The Property Key should match the one being used by the storage property. Text is the Text component that will display the value text. If Use Format is enabled, a formatting string can be specified instead of displaying the value directly. Alt Text will be displayed if the value is undefined, such as before the session has connected or the storage property has not been defined yet. SetEnabledIfOwner\u200b The SetEnabledIfOwner helper enables and disables scene objects based on the target SyncEntity or instantiated object's ownership state.  Entity Target can either be a Network Root, for use inside of prefabs link, or a SyncEntity. If Network Root is selected, this script will automatically find its network root upon instantiation. Whenever the Entity Target becomes owned by the current user (or starts that way), each object in Owner Objects will be enabled, and each object in Non Owner Objects will be disabled. Whenever the Entity Target becomes not owned by the current user (or starts that way), each object in Owner Objects will be disabled, and each object in Non Owner Objects will be enabled. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Debugging Next Networked Events DisplayStoragePropertySetEnabledIfOwner DisplayStoragePropertySetEnabledIfOwner DisplayStorageProperty SetEnabledIfOwner AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/networked-events": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate LimitsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Spectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate LimitsSimple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Spectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate Limits Spectacles FrameworksSpectacles Sync KitFeaturesNetworked EventsOn this pageCopy pageNetworked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousHelper ScriptsNextPayload and Rate Limits  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Networked Events Networked Events On this page Copy page  Copy page     page Networked Events\nNetworked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users.\nNetworked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network.\nSimple Sending and Receiving\u200b\nBefore a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished.\nTypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi);\nTo send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi');\nMessageInfo\u200b\nIf you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message.\n\nMessageInfo.senderUserId: The user ID of the sender.\nMessageInfo.senderConnectionId: The connection ID of the sender.\nMessageInfo.message: The name of the event.\nMessageInfo.data: The data sent with the event.\n\nTypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});\nIncluding Event data\u200b\nIt is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB.\nTypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);});\nEvent data supports the following data types:\n\nstring\nvec2\nvec3\nvec4\nquat\nobject, as long as it is JSON serializable (example below)\n\nTypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});\nEvent Recipients\u200b\nNetworked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent().\nTypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true);\nPayload and Rate Limits\u200b\nNetworked events count against payload and message rate limits. Each networked event counts as one message.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect. Networked Events Networked events can be added to SyncEntities for the purpose of sending one-off, real-time events or messages that do not change the state of the SyncEntity. For example, a networked event can be sent to play a sound effect or trigger a particle effect for all users. Networked events are useful for triggering behaviors that take effect immediately and do not involve data that needs to be referenced again like a storage property. In simple terms, you can think of networked events as calling a function on all instances of a SyncEntity across the network. Simple Sending and Receiving\u200b Before a SyncEntity sends an event, tell the SyncEntity to listen for the event using SyncEntity.onEventReceived. In the add method, pass the name of the event (e.g., \u201csayHi\u201d) and the callback function that should run when the event is received. This can be done before the SyncEntity setup is finished. TypeScriptJavaScriptsayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi); TypeScript JavaScript sayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi);function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi); sayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi); sayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi); sayHi() {    print(\"Hi!\");}this.syncEntity.onEventReceived.add('sayHi', sayHi); sayHi() { sayHi ( )   {      print(\"Hi!\");      print ( \"Hi!\" ) ;  }  }    this.syncEntity.onEventReceived.add('sayHi', sayHi);  this . syncEntity . onEventReceived . add ( 'sayHi' ,  sayHi ) ;   function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi); function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi); function sayHi() {  print('Hi!');}syncEntity.onEventReceived.add('sayHi', sayHi); function sayHi() { function   sayHi ( )   {    print('Hi!');    print ( 'Hi!' ) ;  }  }    syncEntity.onEventReceived.add('sayHi', sayHi); syncEntity . onEventReceived . add ( 'sayHi' ,  sayHi ) ;   To send an event, use SyncEntity.sendEvent(), passing the name of the event as a parameter. TypeScriptJavaScriptthis.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi'); TypeScript JavaScript this.syncEntity.sendEvent('sayHi');syncEntity.sendEvent('sayHi'); this.syncEntity.sendEvent('sayHi'); this.syncEntity.sendEvent('sayHi'); this.syncEntity.sendEvent('sayHi'); this.syncEntity.sendEvent('sayHi'); this . syncEntity . sendEvent ( 'sayHi' ) ;   syncEntity.sendEvent('sayHi'); syncEntity.sendEvent('sayHi'); syncEntity.sendEvent('sayHi'); syncEntity.sendEvent('sayHi'); syncEntity . sendEvent ( 'sayHi' ) ;   MessageInfo\u200b If you need more information about the event, like who it was sent by, the SyncEntity.onEventReceived callback supplies a MessageInfo object that provides info about the event message. MessageInfo.senderUserId: The user ID of the sender. MessageInfo.senderConnectionId: The connection ID of the sender. MessageInfo.message: The name of the event. MessageInfo.data: The data sent with the event. TypeScriptJavaScriptthis.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); TypeScript JavaScript this.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);});syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); this.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); this.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); this.syncEntity.onEventReceived.add('myEventName', (messageInfo) => {  print('event sender: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); this.syncEntity.onEventReceived.add('myEventName', (messageInfo) => { this . syncEntity . onEventReceived . add ( 'myEventName' ,   ( messageInfo )   =>   {    print('event sender: ' + messageInfo.senderUserId);    print ( 'event sender: '   +  messageInfo . senderUserId ) ;    print('event sender connectionId: ' + messageInfo.senderConnectionId);    print ( 'event sender connectionId: '   +  messageInfo . senderConnectionId ) ;    print('event name: ' + messageInfo.message);    print ( 'event name: '   +  messageInfo . message ) ;    print('event data: ' + messageInfo.data);    print ( 'event data: '   +  messageInfo . data ) ;  });  } ) ;   syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); syncEntity.onEventReceived.add('myEventName', function (messageInfo) {  print('event sender userId: ' + messageInfo.senderUserId);  print('event sender connectionId: ' + messageInfo.senderConnectionId);  print('event name: ' + messageInfo.message);  print('event data: ' + messageInfo.data);}); syncEntity.onEventReceived.add('myEventName', function (messageInfo) { syncEntity . onEventReceived . add ( 'myEventName' ,   function   ( messageInfo )   {    print('event sender userId: ' + messageInfo.senderUserId);    print ( 'event sender userId: '   +  messageInfo . senderUserId ) ;    print('event sender connectionId: ' + messageInfo.senderConnectionId);    print ( 'event sender connectionId: '   +  messageInfo . senderConnectionId ) ;    print('event name: ' + messageInfo.message);    print ( 'event name: '   +  messageInfo . message ) ;    print('event data: ' + messageInfo.data);    print ( 'event data: '   +  messageInfo . data ) ;  });  } ) ;   Including Event data\u200b It is often useful to include data along with an event, just like passing information into a function using parameters.\nSyncEntity.sendEvent() has an optional second parameter for event data that gets included in the MessageInfo object as MessageInfo.data.\nNote: Networked events have a payload size limit of 100 KB. TypeScriptJavaScriptthis.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);}); TypeScript JavaScript this.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);});syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);}); this.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);}); this.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);}); this.syncEntity.sendEvent('printMessage', 'this is my event data!');this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  print(messageInfo.data);}); this.syncEntity.sendEvent('printMessage', 'this is my event data!'); this . syncEntity . sendEvent ( 'printMessage' ,   'this is my event data!' ) ;    this.syncEntity.onEventReceived.add('printMessage', (messageInfo) => {  this . syncEntity . onEventReceived . add ( 'printMessage' ,   ( messageInfo )   =>   {    print(messageInfo.data);    print ( messageInfo . data ) ;  });  } ) ;   syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);}); syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);}); syncEntity.sendEvent('printMessage', 'this is my event data!');syncEntity.onEventReceived.add('printMessage', function (messageInfo) {  print(messageInfo.data);}); syncEntity.sendEvent('printMessage', 'this is my event data!'); syncEntity . sendEvent ( 'printMessage' ,   'this is my event data!' ) ;    syncEntity.onEventReceived.add('printMessage', function (messageInfo) { syncEntity . onEventReceived . add ( 'printMessage' ,   function   ( messageInfo )   {    print(messageInfo.data);    print ( messageInfo . data ) ;  });  } ) ;   Event data supports the following data types: string vec2 vec3 vec4 quat object, as long as it is JSON serializable (example below) TypeScriptJavaScriptconst soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); TypeScript JavaScript const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);});const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};this.syncEntity.sendEvent('playSound', soundData);this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = { const  soundData  =   {    clipName: 'bounce',   clipName :   'bounce' ,    volume: 0.5,   volume :   0.5 ,    loops: 1,   loops :   1 ,    position: new vec3(1, 2, 3),   position :   new   vec3 ( 1 ,   2 ,   3 ) ,  };  } ;    this.syncEntity.sendEvent('playSound', soundData);  this . syncEntity . sendEvent ( 'playSound' ,  soundData ) ;    this.syncEntity.onEventReceived.add('playSound', (messageInfo) => {  this . syncEntity . onEventReceived . add ( 'playSound' ,   ( messageInfo )   =>   {    let soundData = messageInfo.data;    let  soundData  =  messageInfo . data ;    print('clipName: ' + soundData.clipName);    print ( 'clipName: '   +  soundData . clipName ) ;    print('volume: ' + soundData.volume);    print ( 'volume: '   +  soundData . volume ) ;    print('loops: ' + soundData.loops);    print ( 'loops: '   +  soundData . loops ) ;    print('position: ' + soundData.position);    print ( 'position: '   +  soundData . position ) ;  });  } ) ;   const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = {  clipName: 'bounce',  volume: 0.5,  loops: 1,  position: new vec3(1, 2, 3),};syncEntity.sendEvent('playSound', soundData);syncEntity.onEventReceived.add('playSound', function (messageInfo) {  let soundData = messageInfo.data;  print('clipName: ' + soundData.clipName);  print('volume: ' + soundData.volume);  print('loops: ' + soundData.loops);  print('position: ' + soundData.position);}); const soundData = { const  soundData  =   {    clipName: 'bounce',    clipName :   'bounce' ,    volume: 0.5,    volume :   0.5 ,    loops: 1,    loops :   1 ,    position: new vec3(1, 2, 3),    position :   new   vec3 ( 1 ,   2 ,   3 ) ,  };  } ;    syncEntity.sendEvent('playSound', soundData); syncEntity . sendEvent ( 'playSound' ,  soundData ) ;    syncEntity.onEventReceived.add('playSound', function (messageInfo) { syncEntity . onEventReceived . add ( 'playSound' ,   function   ( messageInfo )   {    let soundData = messageInfo.data;    let  soundData  =  messageInfo . data ;    print('clipName: ' + soundData.clipName);    print ( 'clipName: '   +  soundData . clipName ) ;    print('volume: ' + soundData.volume);    print ( 'volume: '   +  soundData . volume ) ;    print('loops: ' + soundData.loops);    print ( 'loops: '   +  soundData . loops ) ;    print('position: ' + soundData.position);    print ( 'position: '   +  soundData . position ) ;  });  } ) ;   Event Recipients\u200b Networked events are sent to all users in the session by default. To send a networked event to remote users only, pass true as a third parameter to SyncEntity.sendEvent(). TypeScriptJavaScriptthis.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true); TypeScript JavaScript this.syncEntity.sendEvent('remoteMessage', {}, true);syncEntity.sendEvent('remoteMessage', {}, true); this.syncEntity.sendEvent('remoteMessage', {}, true); this.syncEntity.sendEvent('remoteMessage', {}, true); this.syncEntity.sendEvent('remoteMessage', {}, true); this.syncEntity.sendEvent('remoteMessage', {}, true); this . syncEntity . sendEvent ( 'remoteMessage' ,   { } ,   true ) ;   syncEntity.sendEvent('remoteMessage', {}, true); syncEntity.sendEvent('remoteMessage', {}, true); syncEntity.sendEvent('remoteMessage', {}, true); syncEntity.sendEvent('remoteMessage', {}, true); syncEntity . sendEvent ( 'remoteMessage' ,   { } ,   true ) ;   Payload and Rate Limits\u200b Networked events count against payload and message rate limits. Each networked event counts as one message. If limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Helper Scripts Next Payload and Rate Limits Simple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Simple Sending and ReceivingMessageInfoIncluding Event dataEvent RecipientsPayload and Rate Limits Simple Sending and Receiving MessageInfo Including Event data Event Recipients Payload and Rate Limits AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/prefab-instantiation": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation FlowAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Spectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession ControllerSetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Spectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession Controller Spectacles FrameworksSpectacles Sync KitFeaturesPrefab InstantiationOn this pageCopy pagePrefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\nWas this page helpful?YesNoPreviousPayload and Rate LimitsNextSession Controller  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Prefab Instantiation Prefab Instantiation On this page Copy page  Copy page     page Prefab Instantiation\nTo create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network.\nSetup\u200b\nAdd the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects.\nIn the Inspector panel, configure the following inputs:\n\nPrefabs: Add any prefabs to be instantiated by this Intantiator.\nSpawner Owns Object: If enabled, the local user will claim ownership of the instantiated object.\nSpawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object.\nAuto Instantiate: See more information below.\n\n\nOnce the Instantiator is set up in the Scene Hierarchy, it can be called on from another script.\nFirst, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.\n\nAdd an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list.\nTypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;\nSimple Instantiation\u200b\nBefore instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter.\nTypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);});\nAdvanced Instantiation Options\u200b\nUse the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter.\nBelow is the full list of options with examples:\n\nInstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean).\nInstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object.\nInstantiationOptions.overrideNetworkId: Sets a custom network ID (string).\nInstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0).\nInstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity().\nInstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1).\nInstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated.\nInstantiationOptions.onError: Function called when an error occurs during instantiation.\n\nTypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options);\nYou can also pass a simple JS object instead of the InstantionOptions class.\nTypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),});\nAuto Instantiation\u200b\nAuto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear:\n\nPrefabs: Click + Add Value to add the prefabs to be auto instantiated.\nPersistence: See SyncEntity > Persistence for options.\nAuto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned.\n\nReferencing the New Object\u200b\nThe newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity:\n\nNetworkRootInfo.instantiatedObject: The instantiated scene object.\nNetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user.\nNetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation.\nNetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone.\nNetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user.\nNetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user.\nNetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null.\nNetworkRootInfo.getOwnerId(): UserInfo of the owner, or null.\nNetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId.\nNetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo.\nNetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store.\nNetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store.\n\nOnSuccess Callback\u200b\nAn onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object.\nHere is an example of how to reference the instantiated object in an onSuccess callback:\nTypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);});\nSyncEntity.networkRoot\u200b\nAny SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo.\nTypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }}\nHierarchy of Instantiated Objects\u200b\nWhen a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.\n\nIt is important to keep the following points in mind regarding transformations:\n\nThe position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object.\nAfter instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally.\n\nInstantiation Flow\u200b\nHere is a recap of how instantiation works using the Instantiator component:\n\nOnce the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true.\nOnce the Instantiator is ready, Instantiator.instantiate() can be called.\nAn empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object.\nThe prefab is instantiated as a child of the NetworkRootInfo scene object.\nImmediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab.\nImmediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed.\n Prefab Instantiation To create new objects at run-time, use the Instantiator script. Any objects instantiated through this script will automatically be instantiated across the network. Setup\u200b Add the Instantiator script to a scene object in the Scene Hierarchy. The scene object should be located lower in the Scene Hierarchy than the SessionController, and above the script that will reference it to instantiate objects. In the Inspector panel, configure the following inputs: Prefabs: Add any prefabs to be instantiated by this Intantiator. Spawner Owns Object: If enabled, the local user will claim ownership of the instantiated object. Spawn as Children: If enabled, a Spawn Under Parent input will be visible. Populate this input with the scene object that will be the parent of instantiated objects. If left blank, objects will be spawned under the Instantiator scene object. Auto Instantiate: See more information below.  Once the Instantiator is set up in the Scene Hierarchy, it can be called on from another script. First, reference the Instantiator from the other script. In the Inspector for the script component, verify that the instantiator field is populated with the Instantiator script.  Add an ObjectPrefab input and populate the Inspector with the prefab to be instantiated. The prefab should already be added to the Instantiator's Prefabs list. TypeScriptJavaScript @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab; TypeScript JavaScript  @input() instantiator: Instantiator @input() prefab: ObjectPrefab// @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab;  @input() instantiator: Instantiator @input() prefab: ObjectPrefab  @input() instantiator: Instantiator @input() prefab: ObjectPrefab  @input() instantiator: Instantiator @input() prefab: ObjectPrefab  @input()   @ input ( )   instantiator: Instantiator  instantiator :  Instantiator    @input()   @ input ( )   prefab: ObjectPrefab  prefab :  ObjectPrefab   // @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab; // @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab; // @input Component.ScriptComponent instantiator// @input Asset.ObjectPrefab prefabconst instantiator = script.instantiator;const prefab = script.prefab; // @input Component.ScriptComponent instantiator // @input Component.ScriptComponent instantiator  // @input Asset.ObjectPrefab prefab  // @input Asset.ObjectPrefab prefab    const instantiator = script.instantiator;  const  instantiator  =  script . instantiator ;  const prefab = script.prefab;  const  prefab  =  script . prefab ;   Simple Instantiation\u200b Before instantiating an object, check that the Instantiator has finished setting up and is ready to use. This can be done either by checking Instantiator.isReady(), which returns a boolean, or Instantiator.notifyOnReady() similar to SyncEntity and SessionController. Once ready, the Instantiator can be used to instantiate the prefab by calling Instantiator.instantiate(), which takes the prefab as a requried parameter. TypeScriptJavaScriptonReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);}); TypeScript JavaScript onReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);}); onReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })} onReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })} onReady() {    this.instantiator.instantiate(this.prefab)}onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })} onReady() { onReady ( )   {      this.instantiator.instantiate(this.prefab)      this . instantiator . instantiate ( this . prefab )  }  }    onAwake() {  onAwake ( )   {      this.instantiator.notifyOnReady(() => { this.onReady() })      this . instantiator . notifyOnReady ( ( )   =>   {   this . onReady ( )   } )  }  }   instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);}); instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);}); instantiator.notifyOnReady(function () {  instantiator.instantiate(prefab);}); instantiator.notifyOnReady(function () { instantiator . notifyOnReady ( function   ( )   {    instantiator.instantiate(prefab);   instantiator . instantiate ( prefab ) ;  });  } ) ;   Advanced Instantiation Options\u200b Use the InstantiationOptions class to define options for instantiation. Each of the options on this class is optional, meaning you only need to define the ones you want to use. The options object can be passed into the Instantiator.instantiate() method as an optional second parameter. Below is the full list of options with examples: InstantiationOptions.claimOwnership: If true, the local user who instantiates the prefab will own its Realtime Store (boolean). InstantiationOptions.persistence: RealtimeStore.Persistence of the instantiated object. InstantiationOptions.overrideNetworkId: Sets a custom network ID (string). InstantiationOptions.localPosition: The initial local position of the prefab\u2019s root (vec3), defaults to (0,0,0). InstantiationOptions.localRotation: The initial local rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity(). InstantiationOptions.localScale: The initial local scale of the prefab\u2019s root (vec3), defaults to (1,1,1). InstantiationOptions.worldPosition: The initial world position of the prefab\u2019s root (vec3), defaults to (0,0,0). InstantiationOptions.worldRotation: The initial world rotation of the prefab\u2019s root (quat), defaults to quat.quatIdentity(). InstantiationOptions.worldScale: The initial world scale of the prefab\u2019s root (vec3), defaults to (1,1,1). InstantiationOptions.onSuccess: Function called when the prefab is successfully instantiated. InstantiationOptions.onError: Function called when an error occurs during instantiation. TypeScriptJavaScriptonAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options); TypeScript JavaScript onAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)}const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options); onAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)} onAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)} onAwake() {    this.sessionController: SessionController = SessionController.getInstance()    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    const customDataStore = GeneralDataStore.create()    customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())    const options = new InstantiationOptions()    options.claimOwnership = true    options.persistence = 'Session'    options.localPosition = new vec3(0, 0, 0)    options.localRotation = quat.quatIdentity()    options.localScale = new vec3(0, 0, 0)    options.onSuccess = (networkRootInfo) => {print(\"Success!\")}    options.onError = (error) => {print(\"Error!\")}    options.customDataStore = customDataStore    this.instantiator.instantiate(this.prefab, options)} onAwake() { onAwake ( )   {      this.sessionController: SessionController = SessionController.getInstance()      this . sessionController :  SessionController  =  SessionController . getInstance ( )      this.instantiator.notifyOnReady(() => { this.onReady() })      this . instantiator . notifyOnReady ( ( )   =>   {   this . onReady ( )   } )  }  }    onReady() {  onReady ( )   {      const customDataStore = GeneralDataStore.create()      const  customDataStore  =  GeneralDataStore . create ( )      customDataStore.putString(\"displayName\", this.sessionController.getLocalUserName())     customDataStore . putString ( \"displayName\" ,   this . sessionController . getLocalUserName ( ) )        const options = new InstantiationOptions()      const  options  =   new   InstantiationOptions ( )      options.claimOwnership = true     options . claimOwnership  =   true      options.persistence = 'Session'     options . persistence  =   'Session'      options.localPosition = new vec3(0, 0, 0)     options . localPosition  =   new   vec3 ( 0 ,   0 ,   0 )      options.localRotation = quat.quatIdentity()     options . localRotation  =  quat . quatIdentity ( )      options.localScale = new vec3(0, 0, 0)     options . localScale  =   new   vec3 ( 0 ,   0 ,   0 )      options.onSuccess = (networkRootInfo) => {print(\"Success!\")}     options . onSuccess   =   ( networkRootInfo )   =>   { print ( \"Success!\" ) }      options.onError = (error) => {print(\"Error!\")}     options . onError   =   ( error )   =>   { print ( \"Error!\" ) }      options.customDataStore = customDataStore     options . customDataStore  =  customDataStore       this.instantiator.instantiate(this.prefab, options)      this . instantiator . instantiate ( this . prefab ,  options )  }  }   const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options); const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options); const customDataStore = GeneralDataStore.create();customDataStore.putString(  'displayName',  global.sessionController.getLocalUserName());const options = new InstantiationOptions();options.claimOwnership = true;options.localPosition = new vec3(0, 0, 0);options.localRotation = quat.quatIdentity();options.localScale = new vec3(0, 0, 0);options.persistence = 'Session';options.onSuccess = function (networkRootInfo) {  print('Success!');};options.onError = function (error) {  print('Error!');};options.customDataStore = customDataStore;instantiator.instantiate(prefab, options); const customDataStore = GeneralDataStore.create(); const  customDataStore  =   GeneralDataStore . create ( ) ;  customDataStore.putString( customDataStore . putString (    'displayName',    'displayName' ,    global.sessionController.getLocalUserName()   global . sessionController . getLocalUserName ( )  );  ) ;    const options = new InstantiationOptions();  const  options  =   new   InstantiationOptions ( ) ;  options.claimOwnership = true; options . claimOwnership   =   true ;  options.localPosition = new vec3(0, 0, 0); options . localPosition   =   new   vec3 ( 0 ,   0 ,   0 ) ;  options.localRotation = quat.quatIdentity(); options . localRotation   =  quat . quatIdentity ( ) ;  options.localScale = new vec3(0, 0, 0); options . localScale   =   new   vec3 ( 0 ,   0 ,   0 ) ;  options.persistence = 'Session'; options . persistence   =   'Session' ;  options.onSuccess = function (networkRootInfo) { options . onSuccess   =   function   ( networkRootInfo )   {    print('Success!');    print ( 'Success!' ) ;  };  } ;  options.onError = function (error) { options . onError   =   function   ( error )   {    print('Error!');    print ( 'Error!' ) ;  };  } ;  options.customDataStore = customDataStore; options . customDataStore   =  customDataStore ;    instantiator.instantiate(prefab, options); instantiator . instantiate ( prefab ,  options ) ;   You can also pass a simple JS object instead of the InstantionOptions class. TypeScriptJavaScriptonAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),}); TypeScript JavaScript onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })}instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),}); onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })} onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })} onAwake() {    this.instantiator.notifyOnReady(() => { this.onReady() })}onReady() {    this.instantiator.instantiate(this.prefab, {        claimOwnership: true,        persistence: 'Session',        localPosition: new vec3(0, 0, -100),    })} onAwake() { onAwake ( )   {      this.instantiator.notifyOnReady(() => { this.onReady() })      this . instantiator . notifyOnReady ( ( )   =>   {   this . onReady ( )   } )  }  }    onReady() {  onReady ( )   {      this.instantiator.instantiate(this.prefab, {      this . instantiator . instantiate ( this . prefab ,   {          claimOwnership: true,         claimOwnership :   true ,          persistence: 'Session',         persistence :   'Session' ,          localPosition: new vec3(0, 0, -100),         localPosition :   new   vec3 ( 0 ,   0 ,   - 100 ) ,      })      } )  }  }   instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),}); instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),}); instantiator.instantiate(prefab, {  claimOwnership: true,  persistence: 'Session',  localPosition: new vec3(0, 0, -100),}); instantiator.instantiate(prefab, { instantiator . instantiate ( prefab ,   {    claimOwnership: true,    claimOwnership :   true ,    persistence: 'Session',    persistence :   'Session' ,    localPosition: new vec3(0, 0, -100),    localPosition :   new   vec3 ( 0 ,   0 ,   - 100 ) ,  });  } ) ;   Auto Instantiation\u200b Auto instantiation automatically instantiates prefabs when a user joins the session. To use auto instantiation, enable Auto Instantiate in the Instantiator Inspector panel. When enabled, additional input fields will appear: Prefabs: Click + Add Value to add the prefabs to be auto instantiated. Persistence: See SyncEntity > Persistence for options. Auto Instantiate Owner: If Owned is selected, the local user who spawns the prefab will claim ownership of the object. Otherwise, the instantiated object will be unowned. Referencing the New Object\u200b The newly created object can be referenced from its NetworkRootInfo. A NetworkRootInfo is created on the root scene object of every instantiated prefab and can be accessed either from the onSuccess callback, or from the object\u2019s SyncEntity, if it has one. A NetworkRootInfo provides the following information about the instantiated object, as well as methods similar to SyncEntity: NetworkRootInfo.instantiatedObject: The instantiated scene object. NetworkRootInfo.locallyCreated: Boolean indicating whether the prefab was instantiated by the local user. NetworkRootInfo.dataStore: The GeneralDataStore associated with the instantiated object, which stores information about the prefab\u2019s instantiation. Returns the InstantiationOptions.customDataStore, if set during instantiation. NetworkRootInfo.onDestroyed: Event that fires when the instantiated object is destroyed by anyone. NetworkRootInfo.onLocalDestroyed: Event that fires when the instantiated object is destroyed by the local user. NetworkRootInfo.onRemoteDestroyed: Event that fires when the instantiated object is destroyed by another user. NetworkRootInfo.getOwnerUserId(): UserInfo.userId of the owner (string), or null. NetworkRootInfo.getOwnerId(): UserInfo of the owner, or null. NetworkRootInfo.isOwnedBy(connectionId: string): Returns a boolean indicating whether the instantiated object is owned by a user with the given connectionId. NetworkRootInfo.isOwnedByUserInfo(user: ConnectedLensModule.UserInfo): Returns a boolean indicating whether the instantiated object is owned by the given UserInfo. NetworkRootInfo.canIModifyStore(): Returns a boolean indicating whether the local user can modify the instantiated object\u2019s Realtime Store. NetworkRootInfo.doIOwnStore(): Returns a boolean indicating whether the local user owns the instantiated object\u2019s Realtime Store. An onSuccess callback receives a NetworkRootInfo parameter when it is called. An onSuccess callback can be passed to Instantiator.instantiate() as part of the InstantiationOptions. It can also be passed as a third parameter, which overrides a callback set in the InstantationOptions object. Here is an example of how to reference the instantiated object in an onSuccess callback: TypeScriptJavaScriptthis.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);}); TypeScript JavaScript this.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  });instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);}); this.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  }); this.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  }); this.instantiator.instantiate(  this.prefab,  {},  (networkRootInfo: NetworkRootInfo) => {    const newObj = networkRootInfo.instantiatedObject;    print('instantiated new object: ' + newObj);  }); this.instantiator.instantiate( this . instantiator . instantiate (    this.prefab,    this . prefab ,    {},    { } ,    (networkRootInfo: NetworkRootInfo) => {    ( networkRootInfo :  NetworkRootInfo )   =>   {      const newObj = networkRootInfo.instantiatedObject;      const  newObj  =  networkRootInfo . instantiatedObject ;      print('instantiated new object: ' + newObj);      print ( 'instantiated new object: '   +  newObj ) ;    }    }  );  ) ;   instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);}); instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);}); instantiator.instantiate(prefab, {}, function (networkRootInfo) {  const newObj = networkRootInfo.instantiatedObject;  print('instantiated new object: ' + newObj);}); instantiator.instantiate(prefab, {}, function (networkRootInfo) { instantiator . instantiate ( prefab ,   { } ,   function   ( networkRootInfo )   {    const newObj = networkRootInfo.instantiatedObject;    const  newObj  =  networkRootInfo . instantiatedObject ;    print('instantiated new object: ' + newObj);    print ( 'instantiated new object: '   +  newObj ) ;  });  } ) ;   Any SyncEntity that has been instantiated will also have a reference to its NetworkRootInfo. TypeScriptJavaScriptif (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }} TypeScript JavaScript if (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }}if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }} if (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }} if (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }} if (this.syncEntity.networkRoot) {    print('Looks like I've been instantiated')    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.')    } else {        print('I was created by another user.')    }} if (this.syncEntity.networkRoot) { if   ( this . syncEntity . networkRoot )   {      print('Looks like I've been instantiated')      print ( 'Looks like I' ve been instantiated' )        if (this.syncEntity.networkRoot.locallyCreated) {      if   ( this . syncEntity . networkRoot . locallyCreated )   {          print('I was created by the local user.')          print ( 'I was created by the local user.' )      } else {      }   else   {          print('I was created by another user.')          print ( 'I was created by another user.' )      }      }  }  }   if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }} if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }} if (syncEntity.networkRoot) {    print('Looks like I've been instantiated');    if (this.syncEntity.networkRoot.locallyCreated) {        print('I was created by the local user.');    } else {        print('I was created by another user.');    }} if (syncEntity.networkRoot) { if   ( syncEntity . networkRoot )   {      print('Looks like I've been instantiated');      print ( 'Looks like I' ve been instantiated' ) ;        if (this.syncEntity.networkRoot.locallyCreated) {      if   ( this . syncEntity . networkRoot . locallyCreated )   {          print('I was created by the local user.');          print ( 'I was created by the local user.' ) ;      } else {      }   else   {          print('I was created by another user.');          print ( 'I was created by another user.' ) ;      }      }  }  }   Hierarchy of Instantiated Objects\u200b When a prefab is instantiated by the Instantiator, it is created as a child of a root scene object that holds the NetworkRootInfo. The NetworkRootInfo scene object is spawned as a child of the Spawn Under Parent set in the Instantiator Inspector inputs. This was done to solve a few problems, including enabling SyncEntities to find their NetworkRootInfo, and ensuring initial transforms are correct.  It is important to keep the following points in mind regarding transformations: The position, rotation, and scale options specific in InstantiationOptions are applied to the NetworkRootInfo scene object, not the instaniated object. After instantiation, any changes to the instantiated object\u2019s local position, rotation, and scale are made relative to the NetworkRootInfo scene object. This is especially relevant if you include a SyncTransform on a prefab that is configured to sync locally. Instantiation Flow\u200b Here is a recap of how instantiation works using the Instantiator component: Once the Instantiator setup is finished, Instantiator.notifyOnReady() will execute its callbacks and Instantiator.isReady() will return true. Once the Instantiator is ready, Instantiator.instantiate() can be called. An empty scene object is created using the world or local transform data, as specified in optional InstantiationOptions. NetworkRootInfo is attached to the empty scene object. The prefab is instantiated as a child of the NetworkRootInfo scene object. Immediately OnAwake, all SyncEntities look upward in their hierarchy to search for a NetworkRootInfo and determine if they are under a prefab. If a NetworkRootInfo is found, they generate their network ID in a special way in order to be tied to the prefab. Immediately after instantiating the object and after SyncEntities have found their NetworkRootInfos, the onSuccess callback is executed. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Payload and Rate Limits Next Session Controller SetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow SetupSimple InstantiationAdvanced Instantiation OptionsAuto InstantiationReferencing the New ObjectHierarchy of Instantiated ObjectsInstantiation Flow Setup Simple Instantiation Advanced Instantiation Options Auto Instantiation Referencing the New Object Hierarchy of Instantiated Objects Instantiation Flow AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/session-controller": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Spectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single PlayerSkip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Spectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single Player Spectacles FrameworksSpectacles Sync KitFeaturesSession ControllerOn this pageCopy pageSession Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately.Was this page helpful?YesNoPreviousPrefab InstantiationNextStart Menu and Single Player  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Session Controller Session Controller On this page Copy page  Copy page     page Session Controller\nThe SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.\n\n\nSkip Joining Flow UI in Lens Studio\u200b\nThe Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.\n\nReferencing SessionController\u200b\nThe SessionController script can be referenced from other scripts as follows:\nTypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController;\nSession Setup\u200b\nThe SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately.\nTypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use});\nYou can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean.\nTypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready}\nLogging\u200b\nThe SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.\n\nAPIs\u200b\nThe SessionController script offers the following APIs for accessing information about the current shared session.\nImportant: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.\nSessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized.\nSessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user.\nSessionController.getLocalUserId(): string\nReturns the user ID of the local user.\nSessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user.\nSessionController.getLocalUserName(): string\nReturns the display name of the local user.\nSessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change.\nSessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects.\nSessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists.\nSessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID.\nSessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available.\nSessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer.\nSessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately. Session Controller The SessionController is a script that manages the state of a shared session and provides APIs for accessing session information, such as users in the session and server time. It acts as a central interface that all Spectacles Sync Kit scripts can talk to and get the information they need. The SessionControllerComponent can be found on the SessionController [CONFIGURE_ME] scene object in the Scene Hierarchy.   Skip Joining Flow UI in Lens Studio\u200b The Spectacles Sync Kit package includes UI for guiding users to map or relocalize against the colocated coordinate space. The UI can be viewed in the Preview panel after selecting Multiplayer in the Start Menu. During Lens development, you can choose to skip this UI by enabling Skip UI in Lens Studio in the Inspector panel.  Referencing SessionController\u200b The SessionController script can be referenced from other scripts as follows: TypeScriptJavaScriptimport {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController; TypeScript JavaScript import {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()}const sessionController = global.sessionController; import {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()} import {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()} import {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differonAwake() {   const sessionController: SessionController = SessionController.getInstance()} import {SessionController} from \"SpectacleSyncKit/Core/SessionController\" // path may differ import   { SessionController }   from   \"SpectacleSyncKit/Core/SessionController\"   // path may differ    onAwake() {  onAwake ( )   {     const sessionController: SessionController = SessionController.getInstance()     const  sessionController :  SessionController  =  SessionController . getInstance ( )  }  }   const sessionController = global.sessionController; const sessionController = global.sessionController; const sessionController = global.sessionController; const sessionController = global.sessionController; const  sessionController  =  global . sessionController ;   Session Setup\u200b The SessionController handles a number of steps when a user starts or joins a shared session. It is important that the SessionController has completed this setup before other scripts attempt to use SessionController APIs. To check that the SessionController is ready to use, call SessionController.notifyOnReady(). The method accepts a callback function that executes once session setup is complete. If setup is already finished, the callback function runs immediately. TypeScriptJavaScriptconst sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use}); TypeScript JavaScript const sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use});global.sessionController.notifyOnReady(() => {  // SessionController is ready to use}); const sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use}); const sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use}); const sessionController: SessionController = SessionController.getInstance();sessionController.notifyOnReady(() => {  // SessionController is ready to use}); const sessionController: SessionController = SessionController.getInstance(); const  sessionController :  SessionController  =  SessionController . getInstance ( ) ;    sessionController.notifyOnReady(() => { sessionController . notifyOnReady ( ( )   =>   {    // SessionController is ready to use    // SessionController is ready to use  });  } ) ;   global.sessionController.notifyOnReady(() => {  // SessionController is ready to use}); global.sessionController.notifyOnReady(() => {  // SessionController is ready to use}); global.sessionController.notifyOnReady(() => {  // SessionController is ready to use}); global.sessionController.notifyOnReady(() => { global . sessionController . notifyOnReady ( ( )   =>   {    // SessionController is ready to use    // SessionController is ready to use  });  } ) ;   You can also check if setup has completed by checking SessionController.getIsReady(), which returns a boolean. TypeScriptJavaScriptif (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready} TypeScript JavaScript if (SessionController.getInstance().getIsReady()) {  // Session is ready}if (global.sessionController.getIsReady()) {  // Session is ready} if (SessionController.getInstance().getIsReady()) {  // Session is ready} if (SessionController.getInstance().getIsReady()) {  // Session is ready} if (SessionController.getInstance().getIsReady()) {  // Session is ready} if (SessionController.getInstance().getIsReady()) { if   ( SessionController . getInstance ( ) . getIsReady ( ) )   {    // Session is ready    // Session is ready  }  }   if (global.sessionController.getIsReady()) {  // Session is ready} if (global.sessionController.getIsReady()) {  // Session is ready} if (global.sessionController.getIsReady()) {  // Session is ready} if (global.sessionController.getIsReady()) { if   ( global . sessionController . getIsReady ( ) )   {    // Session is ready    // Session is ready  }  }   Logging\u200b The SessionController scene object has a script attached to it called SyncKitLogLevelConfiguration. From the Inspector panel, select the Log Level Filter to control what type of logs are printed from Spectacles Sync Kit in the Logger. The available options are: Error, Warning, Info, Debug, or Verbose.  APIs\u200b The SessionController script offers the following APIs for accessing information about the current shared session. Important: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs.   Important: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs. Important: Wait until SessionController.notifyOnReady() has been called or SessionController.getIsReady() returns true before using these APIs. SessionController.getIsReady(): boolean\nReturns true if the SessionController has finished joining the session, and the user has successfully mapped or relocalized. SessionController.getLocalConnectionId(): string\nReturns the connection ID of the local user. SessionController.getLocalUserId(): string\nReturns the user ID of the local user. SessionController.getLocalUserInfo(): ConnectedLensModule.UserInfo\nReturns the UserInfo object of the local user. SessionController.getLocalUserName(): string\nReturns the display name of the local user. SessionController.getSession(): MultiplayerSession\nReturns the current MultiplayerSession object. Always use this instead of storing a reference to the session, since it is possible for the session to change. SessionController.getUsers(): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session, represented as UserInfo objects. SessionController.getUserByConnectionId(connectionId: string): ConnectedLensModule.UserInfo\nReturns the UserInfo of a user currently connected to the session with the matching connection ID, or null if none exists. SessionController.getUsersByUserId(userId: string): ConnectedLensModule.UserInfo[]\nReturns a list of users currently connected to the session who have a matching user ID. SessionController.getServerTimeInSeconds(): number?\nReturns the current server timestamp in seconds, or null if not available. SessionController.isSingleplayer(): boolean\nReturns true if the current session is singleplayer, and false if it is multiplayer. SessionController.notifyOnReady(onReady)\nCalls the onReady callback as soon as the SessionController has finished all setup, and the user has successfully mapped or relocalized. If setup is already finished, the callback is called immediately. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Prefab Instantiation Next Start Menu and Single Player Skip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Skip Joining Flow UI in Lens StudioReferencing SessionControllerSession SetupLoggingAPIs Skip Joining Flow UI in Lens Studio Referencing SessionController Session Setup Logging APIs AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/start-menu-and-single-player": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player ConfigurationAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player Configuration Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player Configuration Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player Configuration Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player Configuration Spectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage PropertiesUI CustomizationSingle Player Configuration Spectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage Properties Spectacles FrameworksSpectacles Sync KitFeaturesStart Menu and Single PlayerOn this pageCopy pageStart Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object.Was this page helpful?YesNoPreviousSession ControllerNextStorage Properties  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Start Menu and Single Player Start Menu and Single Player On this page Copy page  Copy page     page Start Menu and Single Player\nWhile Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons.\nThe Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.\n\nUI Customization\u200b\nYou are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.\n\nUpdate the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons.\nIn the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.\n\nThe Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens.\nSingle Player Configuration\u200b\nThe Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes.\nSingle Player Type\u200b\nFrom the dropdown, choose how single player logic will be handled in your Lens. There are two options:\n\nMocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server.\nManual: When this option is selected, you will need to handle your own single player logic.\n\nEnable on Singleplayer Nodes\u200b\nThe scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object. Start Menu and Single Player While Connected Lenses are meant to be experienced together, users should also have the option to enjoy the Lens on their own. For that purpose, the Spectacles Sync Kit package includes an example Start Menu, which has Multiplayer and Singleplayer buttons. The Start Menu is shown to users when they enter a Connected Lens. If a user selects Multiplayer, they enter the co-located joining flow. If they choose Singleplayer, they skip the joining flow and are put directly into the Lens.  UI Customization\u200b You are highly encouraged to customize the Start Menu to match the design of your Lens. To change the Start Menu visuals, locate the StartMenu [CUSTOMIZE_ME] scene object in the Scene Hierarchy.  Update the Multiplayer buttons to suit your design. For example, update or replace the ButtonMesh and Text scene objects. You may also want to add a title splashscreen for your Lens. The example buttons include Spectacles Interaction Kit components to make them pinchable \u2013 keep these components or use them as a guide for how to set up your own buttons. In the Inspector panel for the StartMenu script, ensure the Single Player Button and Multi Player Button inputs are populated with the PinchButton components for each button.  The Start Menu Distance From User input sets how far in front of the user in centimeters the Start Menu appears when they join the Lens. Single Player Configuration\u200b The Inspector panel for the StartMenu script includes two inputs for configuring single player mode \u2013 Single Player Type and Enable On Singleplayer Nodes. From the dropdown, choose how single player logic will be handled in your Lens. There are two options: Mocked Online (Automatic): When this option is selected, your synchronization logic will automatically work for single player. The Lens will use a mock Connected Lens module to mimic communication with the Connected Lenses server. Manual: When this option is selected, you will need to handle your own single player logic. The scene objects added to this list are enabled after a user selects Singleplayer from the Start Menu. By default, this input is populated with the EnableOnReady scene object. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Session Controller Next Storage Properties UI CustomizationSingle Player Configuration UI CustomizationSingle Player Configuration UI Customization Single Player Configuration AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/storage-properties": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate LimitsAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Spectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync EntityAdding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Spectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync Entity Spectacles FrameworksSpectacles Sync KitFeaturesStorage PropertiesOn this pageCopy pageStorage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect.Was this page helpful?YesNoPreviousStart Menu and Single PlayerNextSync Entity  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Storage Properties Storage Properties On this page Copy page  Copy page     page Storage Properties\nStorage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object.\nAdding Storage Properties to a SyncEntity\u200b\nStorage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible.\nTypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp);\nStorage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true)));\nManual Storage Properties\u200b\nManual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable.\nManual Storage Types\u200b\nManual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions).\nSmoothing is not available for some StorageTypes, like string, boolean, and integer.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });\nCustom Manual Storage Property\u200b\nStorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());\nSetting Storage Property Values\u200b\nManual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it.\nStorage property updates are sent to the server in the LateUpdate event.\nTypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');\nIn some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore().\nTypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1);\nGetting Storage Property Values\u200b\nThere are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue.\nCurrentValue\u200b\nStorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend.\nPendingValue\u200b\nStorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate.\nStorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user).\nCurrentOrPendingValue\u200b\nStorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from.\nWhen accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend.\nReacting to Storage Property changes\u200b\nStorage properties have events that fire when their values change, depending on who made the change.\nOnAnyChange\u200b\nThe StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property.\nTypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);});\nOnLocalChange\u200b\nThe StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user.\nTypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});\nOnRemoteChange\u200b\nThe StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user.\nTypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nOnPendingValueChange\u200b\nUse the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network.\nTypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});\nAutomatic Storage Properties\u200b\nAutomatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about.\nFor example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized.\nYou can either use built-in automatic storage properties, or create your own.\nAutmomatic Transform, Position, Rotation, and scale\u200b\nFor transform-related automatic properties, you need to pass in the PropertyType:\n\nPropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space.\nPropertyType.Location: Synced relative to the co-located coordinate space.\nPropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation.\n\nHere are examples of how to set up automatic storage properties for transform data:\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);\nAutomatic Base Color\u200b\nStorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);\nAutomatic Material Property\u200b\nStorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType.\nTypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);\nAutomatic Material Mesh Visual\u200b\nStorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places.\nTypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);\nAutomatic Text\u200b\nStorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter.\nTypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent);\nAutomatic Storage Types\u200b\nAutomatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nCustom Automatic Storage Property\u200b\nStorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions).\nTypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });\nLimiting Send Rate\u200b\nSet StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network.\nWhen using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue.\nTypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10;\nSmoothing Property Values\u200b\nFor properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer.\nSnapshotBufferOptions\u200b\nSmoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional.\n\nSnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25)\nSnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used\nSnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType\nSnapshotBufferOptions.size: Max number of snapshots stored (default = 20)\n\nTypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25;\nCall StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing.\nTypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 });\nYou can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty.\nTypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,});\nSnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing.\nTypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});\nStorage Types\u200b\n\nStorageTypes.bool\nStorageTypes.float\nStorageTypes.double\nStorageTypes.int\nStorageTypes.string\nStorageTypes.vec2\nStorageTypes.vec3\nStorageTypes.vec4\nStorageTypes.quat\nStorageTypes.mat2\nStorageTypes.mat3\nStorageTypes.mat4\nStorageTypes.boolArray\nStorageTypes.floatArray\nStorageTypes.doubleArray\nStorageTypes.intArray\nStorageTypes.stringArray\nStorageTypes.vec2Array\nStorageTypes.vec3Array\nStorageTypes.vec4Array\nStorageTypes.quatArray\nStorageTypes.mat2Array\nStorageTypes.mat3Array\nStorageTypes.mat4Array\nStorageTypes.packedTransform (same as StorageTypes.vec4Array)\n\nPayload and Rate Limits\u200b\nStorage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits.\nIf limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect. Storage Properties Storage properties allow data to be easily synchronized on a SyncEntity and stored for subsequent use. For example, storage properties can be used to update a scoreboard or synchronize the position of an object. Adding Storage Properties to a SyncEntity\u200b Storage properties need to be added to a SyncEntity before they have any effect. You can add a storage property to a SyncEntity at any time, but it is recommended to do it as early as possible. TypeScriptJavaScript\u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp); TypeScript JavaScript \u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);}const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp); \u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);} \u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);} \u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0);onAwake() {    const syncEntity: SyncEntity = new SyncEntity(script);    syncEntity.addStorageProperty(exampleProp);} \u200b\u200bprivate exampleProp = StorageProperty.manualInt('exampleProp', 0); \u200b\u200b private  exampleProp  =  StorageProperty . manualInt ( 'exampleProp' ,   0 ) ;    onAwake() {  onAwake ( )   {      const syncEntity: SyncEntity = new SyncEntity(script);      const  syncEntity :  SyncEntity  =   new   SyncEntity ( script ) ;      syncEntity.addStorageProperty(exampleProp);     syncEntity . addStorageProperty ( exampleProp ) ;  }  }   const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp); const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp); const exampleProp = StorageProperty.manualInt('example', 0);const syncEntity = new SyncEntity(script);syncEntity.addStorageProperty(exampleProp); const exampleProp = StorageProperty.manualInt('example', 0); const  exampleProp  =   StorageProperty . manualInt ( 'example' ,   0 ) ;  const syncEntity = new SyncEntity(script);  const  syncEntity  =   new   SyncEntity ( script ) ;  syncEntity.addStorageProperty(exampleProp); syncEntity . addStorageProperty ( exampleProp ) ;   Storage properties can also be added to a SyncEntity during construction as a StoragePropertySet. A StoragePropertySet constructor takes an array of StorageProperties as an argument, letting you add them all at once instead of one at a time. TypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true))); TypeScript JavaScript const syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true);const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true))); const syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true); const syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true); const syncEntity: SyncEntity = new SyncEntity(  this,  new StoragePropertySet([    StorageProperty.manualString('myString', 'hello'),    StorageProperty.forPosition(this.getTransform(), true),  ]),  true); const syncEntity: SyncEntity = new SyncEntity( const  syncEntity :  SyncEntity  =   new   SyncEntity (    this,    this ,    new StoragePropertySet([    new   StoragePropertySet ( [      StorageProperty.manualString('myString', 'hello'),     StorageProperty . manualString ( 'myString' ,   'hello' ) ,      StorageProperty.forPosition(this.getTransform(), true),     StorageProperty . forPosition ( this . getTransform ( ) ,   true ) ,    ]),    ] ) ,    true    true  );  ) ;   const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true))); const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true))); const syncEntity = new SyncEntity(script, new StoragePropertSet([    StorageProperty.manualString(\"myString\", \"hello\"),    StorageProperty.forPosition(script.getTransform(), true))); const syncEntity = new SyncEntity(script, new StoragePropertSet([ const  syncEntity  =   new   SyncEntity ( script ,   new   StoragePropertSet ( [      StorageProperty.manualString(\"myString\", \"hello\"),      StorageProperty . manualString ( \"myString\" ,   \"hello\" ) ,      StorageProperty.forPosition(script.getTransform(), true)      StorageProperty . forPosition ( script . getTransform ( ) ,   true )  ));  ) ) ;   Manual Storage Properties\u200b Manual storage properties are handled by the developer and meant to be accessed through script. You will be responsible for changing the value and reacting to changes. Depending on the StorageType, smoothing may be configurable. Manual storage properties are also available for all regular StorageTypes. These storage properties follow the format StorageProperty.manual[StorageType](), and take the following parameters: key (string), initial value, and optional smoothing options (SnapshotBufferOptions). Smoothing is not available for some StorageTypes, like string, boolean, and integer. TypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); TypeScript JavaScript const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 });const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  this.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0); const  scoreProp  =  StorageProperty . manualInt ( 'score' ,   0 ) ;    const nameProp = StorageProperty.manualString('name', 'hello');  const  nameProp  =  StorageProperty . manualString ( 'name' ,   'hello' ) ;    const scaleProp = StorageProperty.manualVec3(  const  scaleProp  =  StorageProperty . manualVec3 (    'scale',    'scale' ,    this.getTransfrom().getLocalScale(),    this . getTransfrom ( ) . getLocalScale ( ) ,    { interpolationTarget: -0.25 }    {  interpolationTarget :   - 0.25   }  );  ) ;   const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0);const nameProp = StorageProperty.manualString('name', 'hello');const scaleProp = StorageProperty.manualVec3(  'scale',  script.getTransfrom().getLocalScale(),  { interpolationTarget: -0.25 }); const scoreProp = StorageProperty.manualInt('score', 0); const  scoreProp  =   StorageProperty . manualInt ( 'score' ,   0 ) ;    const nameProp = StorageProperty.manualString('name', 'hello');  const  nameProp  =   StorageProperty . manualString ( 'name' ,   'hello' ) ;    const scaleProp = StorageProperty.manualVec3(  const  scaleProp  =   StorageProperty . manualVec3 (    'scale',    'scale' ,    script.getTransfrom().getLocalScale(),   script . getTransfrom ( ) . getLocalScale ( ) ,    { interpolationTarget: -0.25 }    {   interpolationTarget :   - 0.25   }  );  ) ;   StorageProperty.manual() can be used to configure your own manual storage property, by passing in a property name (string), StorageType, initial value, and optional smoothing options (SnapshotBufferOptions). TypeScriptJavaScriptconst matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); TypeScript JavaScript const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity());const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual( const  matrixProp  =  StorageProperty . manual (    'matrix',    'matrix' ,    StorageTypes.mat4,   StorageTypes . mat4 ,    mat4.identity()   mat4 . identity ( )  );  ) ;   const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual(  'matrix',  StorageTypes.mat4,  mat4.identity()); const matrixProp = StorageProperty.manual( const  matrixProp  =   StorageProperty . manual (    'matrix',    'matrix' ,    StorageTypes.mat4,    StorageTypes . mat4 ,    mat4.identity()   mat4 . identity ( )  );  ) ;   Setting Storage Property Values\u200b Manual storage properties can be updated by calling StorageProperty.setPendingValue(). In order to have the value sent to the network, the storage property must be modifiable, meaning the SyncEntity\u2019s store is unowned or owned by the user who is setting it. Storage property updates are sent to the server in the LateUpdate event.   Storage property updates are sent to the server in the LateUpdate event. Storage property updates are sent to the server in the LateUpdate event. TypeScriptJavaScriptconst scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); TypeScript JavaScript const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!');const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0); const  scoreProp  =  StorageProperty . manualInt ( 'score' ,   0 ) ;    scoreProp.setPendingValue(3); scoreProp . setPendingValue ( 3 ) ;    const textProp = StorageProperty.manualString('myText', '');  const  textProp  =  StorageProperty . manualString ( 'myText' ,   '' ) ;    textProp.setPendingValue('new text!'); textProp . setPendingValue ( 'new text!' ) ;   const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0);scoreProp.setPendingValue(3);const textProp = StorageProperty.manualString('myText', '');textProp.setPendingValue('new text!'); const scoreProp = StorageProperty.manualInt('score', 0); const  scoreProp  =   StorageProperty . manualInt ( 'score' ,   0 ) ;    scoreProp.setPendingValue(3); scoreProp . setPendingValue ( 3 ) ;    const textProp = StorageProperty.manualString('myText', '');  const  textProp  =   StorageProperty . manualString ( 'myText' ,   '' ) ;    textProp.setPendingValue('new text!'); textProp . setPendingValue ( 'new text!' ) ;   In some rare situations, you might want to immediately set the current value of the property without waiting until end of frame, or going through the pending loop. You can do this by calling StorageProperty.setValueImmediate() and passing in its SyncEntity.currentStore and the new value. Only use this if you are confident that you have permission to change the store, for example after checking SyncEntity.canIModifyStore(). TypeScriptJavaScriptscoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1); TypeScript JavaScript scoreProp.setValueImmediate(this.syncEntity.currentStore, -1);scoreProp.setValueImmediate(syncEntity.currentStore, -1); scoreProp.setValueImmediate(this.syncEntity.currentStore, -1); scoreProp.setValueImmediate(this.syncEntity.currentStore, -1); scoreProp.setValueImmediate(this.syncEntity.currentStore, -1); scoreProp.setValueImmediate(this.syncEntity.currentStore, -1); scoreProp . setValueImmediate ( this . syncEntity . currentStore ,   - 1 ) ;   scoreProp.setValueImmediate(syncEntity.currentStore, -1); scoreProp.setValueImmediate(syncEntity.currentStore, -1); scoreProp.setValueImmediate(syncEntity.currentStore, -1); scoreProp.setValueImmediate(syncEntity.currentStore, -1); scoreProp . setValueImmediate ( syncEntity . currentStore ,   - 1 ) ;   Getting Storage Property Values\u200b There are three ways to access the value of a storage property: StorageProperty.currentValue, StorageProperty.pendingValue, and StorageProperty.currentOrPendingValue. StorageProperty.currentValue is the value currently stored on the server and expected to be synced across the network. StorageProperty.currentValue is updated only after the value has been updated on the backend. StorageProperty.pendingValue is the local value that will potentially be sent to the backend, but not necessarily. This can be useful if you want your own local value distinct from the network value. For example, you may want to simulate state on an object while you wait for an update from the owner. It is also useful if you want to access the locally changed value of a storage property in the time before it is sent to the server during LateUpdate. StorageProperty.pendingValue is only sent to the server if the SyncEntity can be modified (i.e., the SyncEntity is unowned or owned by the local user). StorageProperty.currentOrPendingValue can be either the currentValue or the pendingValue, depending on which was set most recently. This is useful if you just want to use whichever value was updated last and do not need to care about where it came from. When accessing a storage property value from a SyncEntity.notifyOnReady() callback, use StorageProperty.currentOrPendingValue. For the first user to join the session StorageProperty.currentValue may return null because the value has not yet been updated on the backend. Reacting to Storage Property changes\u200b Storage properties have events that fire when their values change, depending on who made the change. The StorageProperty.onAnyChange event occurs when any user has changed the StorageProperty.currentValue of the storage property. TypeScriptJavaScriptscoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); TypeScript JavaScript scoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);});scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add((newValue: number, oldValue: number) => {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add((newValue: number, oldValue: number) => { scoreProp . onAnyChange . add ( ( newValue :   number ,  oldValue :   number )   =>   {    print('Current value changed from ' + oldValue + ' to ' + newValue);    print ( 'Current value changed from '   +  oldValue  +   ' to '   +  newValue ) ;  });  } ) ;   scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add(function (newValue, oldValue) {  print('Current value changed from ' + oldValue + ' to ' + newValue);}); scoreProp.onAnyChange.add(function (newValue, oldValue) { scoreProp . onAnyChange . add ( function   ( newValue ,  oldValue )   {    print('Current value changed from ' + oldValue + ' to ' + newValue);    print ( 'Current value changed from '   +  oldValue  +   ' to '   +  newValue ) ;  });  } ) ;   The StorageProperty.onLocalChange event occurs when the StorageProperty.currentValue has been changed by the local user. TypeScriptJavaScriptscoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); TypeScript JavaScript scoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );});scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add((newValue: number, oldValue: number) => {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add((newValue: number, oldValue: number) => { scoreProp . onLocalChange . add ( ( newValue :   number ,  oldValue :   number )   =>   {    print(    print (      'I changed the current value changed from ' + oldValue + ' to ' + newValue      'I changed the current value changed from '   +  oldValue  +   ' to '   +  newValue   );    ) ;  });  } ) ;   scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add(function (newValue, oldValue) {  print(    'I changed the current value changed from ' + oldValue + ' to ' + newValue  );}); scoreProp.onLocalChange.add(function (newValue, oldValue) { scoreProp . onLocalChange . add ( function   ( newValue ,  oldValue )   {    print(    print (      'I changed the current value changed from ' + oldValue + ' to ' + newValue      'I changed the current value changed from '   +  oldValue  +   ' to '   +  newValue   );    ) ;  });  } ) ;   The StorageProperty.onRemoteChange event occurs when the StorageProperty.currentValue has been changed by another user. TypeScriptJavaScriptscoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); TypeScript JavaScript scoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add((newValue: number, oldValue: number) => { scoreProp . onRemoteChange . add ( ( newValue :   number ,  oldValue :   number )   =>   {    print(    print (      'Someone else changed the current value changed from ' +      'Someone else changed the current value changed from '   +        oldValue +       oldValue  +        ' to ' +        ' to '   +        newValue       newValue   );    ) ;  });  } ) ;   scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onRemoteChange.add(function (newValue, oldValue) { scoreProp . onRemoteChange . add ( function   ( newValue ,  oldValue )   {    print(    print (      'Someone else changed the current value changed from ' +      'Someone else changed the current value changed from '   +        oldValue +       oldValue  +        ' to ' +        ' to '   +        newValue       newValue   );    ) ;  });  } ) ;   Use the StorageProperty.onPendingValueChange event to be notified whenever the StorageProperty.pendingValue is changed. This is useful in cases where send rate limits are enabled on the property, and you would like to react to local changes before they are sent out to the network. TypeScriptJavaScriptscoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); TypeScript JavaScript scoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );});scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add((newValue: number, oldValue: number) => { scoreProp . onPendingValueChange . add ( ( newValue :   number ,  oldValue :   number )   =>   {    print(    print (      'Someone else changed the current value changed from ' +      'Someone else changed the current value changed from '   +        oldValue +       oldValue  +        ' to ' +        ' to '   +        newValue       newValue   );    ) ;  });  } ) ;   scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add(function (newValue, oldValue) {  print(    'Someone else changed the current value changed from ' +      oldValue +      ' to ' +      newValue  );}); scoreProp.onPendingValueChange.add(function (newValue, oldValue) { scoreProp . onPendingValueChange . add ( function   ( newValue ,  oldValue )   {    print(    print (      'Someone else changed the current value changed from ' +      'Someone else changed the current value changed from '   +        oldValue +       oldValue  +        ' to ' +        ' to '   +        newValue       newValue   );    ) ;  });  } ) ;   Automatic Storage Properties\u200b Automatic storage properties can be used to automatically update component and asset properties, such as position, rotation, scale, text, and base color. Automatic storage properties are given getter and setter functions that automatically read and write to an external target. They are meant to be added to a SyncEntity and forgotten about. For example, a storage property for localPosition automatically reads from Transform.getLocalPosition() to check the current local value, and writes to Transform.setLocalPosition() when receiving a network value. You can add the storage property to the object\u2019s SyncEntity, and the local position is automatically kept synchronized. You can either use built-in automatic storage properties, or create your own. For transform-related automatic properties, you need to pass in the PropertyType: PropertyType.Local: Synced relative to the scene object\u2019s parent\u2019s coordinate space. PropertyType.Location: Synced relative to the co-located coordinate space. PropertyType.World: Synced relative to Spectacles\u2019 world origin, which is not recommended for colocation. Here are examples of how to set up automatic storage properties for transform data: TypeScriptJavaScriptconst transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); TypeScript JavaScript const transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local);const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform: Transform = this.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform: Transform = this.getTransform(); const  transform :  Transform  =   this . getTransform ( ) ;  const positionType = PropertyType.Local;  const  positionType  =  PropertyType . Local ;  const rotationType = PropertyType.Local;  const  rotationType  =  PropertyType . Local ;  const scaleType = PropertyType.Local;  const  scaleType  =  PropertyType . Local ;    const transformProp = StorageProperty.forTransform(  const  transformProp  =  StorageProperty . forTransform (    transform,   transform ,    positionType,   positionType ,    rotationType,   rotationType ,    scaleType   scaleType );  ) ;  const positionProp = StorageProperty.forPosition(transform, StorageType.Local);  const  positionProp  =  StorageProperty . forPosition ( transform ,  StorageType . Local ) ;  const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);  const  rotationProp  =  StorageProperty . forRotation ( transform ,  StorageType . Local ) ;  const scaleProp = StorageProperty.forScale(transform, StorageType.Local);  const  scaleProp  =  StorageProperty . forScale ( transform ,  StorageType . Local ) ;   const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform = script.getTransform();const positionType = PropertyType.Local;const rotationType = PropertyType.Local;const scaleType = PropertyType.Local;const transformProp = StorageProperty.forTransform(  transform,  positionType,  rotationType,  scaleType);const positionProp = StorageProperty.forPosition(transform, StorageType.Local);const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);const scaleProp = StorageProperty.forScale(transform, StorageType.Local); const transform = script.getTransform(); const  transform  =  script . getTransform ( ) ;  const positionType = PropertyType.Local;  const  positionType  =   PropertyType . Local ;  const rotationType = PropertyType.Local;  const  rotationType  =   PropertyType . Local ;  const scaleType = PropertyType.Local;  const  scaleType  =   PropertyType . Local ;    const transformProp = StorageProperty.forTransform(  const  transformProp  =   StorageProperty . forTransform (    transform,   transform ,    positionType,   positionType ,    rotationType,   rotationType ,    scaleType   scaleType );  ) ;  const positionProp = StorageProperty.forPosition(transform, StorageType.Local);  const  positionProp  =   StorageProperty . forPosition ( transform ,   StorageType . Local ) ;  const rotationProp = StorageProperty.forRotation(transform, StorageType.Local);  const  rotationProp  =   StorageProperty . forRotation ( transform ,   StorageType . Local ) ;  const scaleProp = StorageProperty.forScale(transform, StorageType.Local);  const  scaleProp  =   StorageProperty . forScale ( transform ,   StorageType . Local ) ;   StorageProperty.forMeshVisualBaseColor() automatically syncs the base color of a MaterialMeshVisual. The first parameter is the MaterialMeshVisual, and the second is an optional clone boolean. If clone is true, the material will be cloned and applied back to the visual, which is useful if the material is used in multiple places. TypeScriptJavaScriptconst colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); TypeScript JavaScript const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false);const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const  colorProp  =  StorageProperty . forMeshVisualBaseColor ( visual ,   false ) ;   const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const colorProp = StorageProperty.forMeshVisualBaseColor(visual, false); const  colorProp  =   StorageProperty . forMeshVisualBaseColor ( visual ,   false ) ;   StorageProperty.forMaterialProperty() automatically syncs a Material property of your choice. It expects the following parameters: Material, key (string), and StorageType. TypeScriptJavaScriptconst materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); TypeScript JavaScript const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float);const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty( const  materialProp  =  StorageProperty . forMaterialProperty (    material,   material ,    'propName',    'propName' ,    StorageTypes.float   StorageTypes . float );  ) ;   const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty(  material,  'propName',  StorageTypes.float); const materialProp = StorageProperty.forMaterialProperty( const  materialProp  =   StorageProperty . forMaterialProperty (    material,   material ,    'propName',    'propName' ,    StorageTypes.float    StorageTypes . float  );  ) ;   StorageProperty.forMeshVisualProperty() automatically syncs a MeshVisual property of your choice. It expects the following parameters: MaterialMeshVisual, property name (string), StorageType, and an optional clone boolean. If clone is true, the MeshVisual\u2019s material will be cloned and applied back to the visual, which is useful if the material is used in multiple places. TypeScriptJavaScriptconst meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); TypeScript JavaScript const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true);const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty( const  meshProp  =  StorageProperty . forMeshVisualProperty (    visual,   visual ,    'propName',    'propName' ,    StorageTypes.float,   StorageTypes . float ,    true    true  );  ) ;   const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty(  visual,  'propName',  StorageTypes.float,  true); const meshProp = StorageProperty.forMeshVisualProperty( const  meshProp  =   StorageProperty . forMeshVisualProperty (    visual,   visual ,    'propName',    'propName' ,    StorageTypes.float,    StorageTypes . float ,    true    true  );  ) ;   StorageProperty.forTextText() automatically syncs the text on a TextComponent. It takes the TextComponent as a parameter. TypeScriptJavaScriptconst textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent); TypeScript JavaScript const textProp = StorageProperty.forTextText(textComponent);const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const  textProp  =  StorageProperty . forTextText ( textComponent ) ;   const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const textProp = StorageProperty.forTextText(textComponent); const  textProp  =   StorageProperty . forTextText ( textComponent ) ;   Automatic storage properties are available for all regular StorageTypes. These storage properties follow the format StorageProperty.auto[StorageType](), and take the following parameters: key (string), getter (function), setter (function), and optional smoothing options (SnapshotBufferOptions). TypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); TypeScript JavaScript const transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform(); const  transform :  Transform  =   this . getTransform ( ) ;    const scaleProp = StorageProperty.autoVec3(  const  scaleProp  =  StorageProperty . autoVec3 (    'localScale',    'localScale' ,    () => {    ( )   =>   {      // getter      // getter      return transform.getLocalScale();      return  transform . getLocalScale ( ) ;    },    } ,    (val) => {    ( val )   =>   {      // setter      // setter      transform.setLocalScale(val);     transform . setLocalScale ( val ) ;    },    } ,    { interpolationTarget: -0.25 }    {  interpolationTarget :   - 0.25   }  );  ) ;   const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform();const scaleProp = StorageProperty.autoVec3(  'localScale',  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform(); const  transform  =  script . getTransform ( ) ;    const scaleProp = StorageProperty.autoVec3(  const  scaleProp  =   StorageProperty . autoVec3 (    'localScale',    'localScale' ,    function () {    function   ( )   {      // getter      // getter      return transform.getLocalScale();      return  transform . getLocalScale ( ) ;    },    } ,    function (val) {    function   ( val )   {      // setter      // setter      transform.setLocalScale(val);     transform . setLocalScale ( val ) ;    },    } ,    { interpolationTarget: -0.25 }    {   interpolationTarget :   - 0.25   }  );  ) ;   StorageProperty.auto() can be used to configure your own automatic storage property, by passing in a property name (string), StorageType, and getter and setter functions, and optional smoothing options (SnapshotBufferOptions). TypeScriptJavaScriptconst transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); TypeScript JavaScript const transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 });const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  () => {    // getter    return transform.getLocalScale();  },  (val) => {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform: Transform = this.getTransform(); const  transform :  Transform  =   this . getTransform ( ) ;    const scaleProp = StorageProperty.auto(  const  scaleProp  =  StorageProperty . auto (    'localScale',    'localScale' ,    StorageTypes.vec3,   StorageTypes . vec3 ,    () => {    ( )   =>   {      // getter      // getter      return transform.getLocalScale();      return  transform . getLocalScale ( ) ;    },    } ,    (val) => {    ( val )   =>   {      // setter      // setter      transform.setLocalScale(val);     transform . setLocalScale ( val ) ;    },    } ,    { interpolationTarget: -0.25 }    {  interpolationTarget :   - 0.25   }  );  ) ;   const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform();const scaleProp = StorageProperty.auto(  'localScale',  StorageTypes.vec3,  function () {    // getter    return transform.getLocalScale();  },  function (val) {    // setter    transform.setLocalScale(val);  },  { interpolationTarget: -0.25 }); const transform = script.getTransform(); const  transform  =  script . getTransform ( ) ;    const scaleProp = StorageProperty.auto(  const  scaleProp  =   StorageProperty . auto (    'localScale',    'localScale' ,    StorageTypes.vec3,    StorageTypes . vec3 ,    function () {    function   ( )   {      // getter      // getter      return transform.getLocalScale();      return  transform . getLocalScale ( ) ;    },    } ,    function (val) {    function   ( val )   {      // setter      // setter      transform.setLocalScale(val);     transform . setLocalScale ( val ) ;    },    } ,    { interpolationTarget: -0.25 }    {   interpolationTarget :   - 0.25   }  );  ) ;   Limiting Send Rate\u200b Set StorageProperty.sendsPerSecondLimit to a value greater than zero to limit how many times per second the property will send updates to the network. When using this feature, StorageProperty.currentValue will only be updated when the value is actually sent to the network. To get the most recent local version of a value, check StorageProperty.currentOrPendingValue. TypeScriptJavaScriptscoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10; TypeScript JavaScript scoreProp.sendsPerSecondLimit = 10;scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp . sendsPerSecondLimit  =   10 ;   scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp.sendsPerSecondLimit = 10; scoreProp . sendsPerSecondLimit   =   10 ;   Smoothing Property Values\u200b For properties that change very frequently, especially when the send rate is limited, it can be useful to smooth out received values. Otherwise, the result will appear choppy since values are only updated as they are received. When smoothing is enabled, intermediate values are approximated by interpolating between updates. Smoothing is not available for some StorageTypes, like string, boolean, and integer. Smoothing is configurable using the SnapshotBufferOptions class, which includes the properties below. All properties are optional. SnapshotBufferOptions.interpolationTarget: Time delta in local seconds to target (default = -0.25) SnapshotBufferOptions.storageType: Override the StorageType, if blank the StorageProperty's StorageType will be used SnapshotBufferOptions.lerpFunc: Override the function used for interpolating values, if blank one will be chosen based on StorageType SnapshotBufferOptions.size: Max number of snapshots stored (default = 20) TypeScriptJavaScriptprivate options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25; TypeScript JavaScript private options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25;const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25; private options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25; private options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25; private options = new SnapshotBufferOptions();this.options.interpolationTarget = -0.25; private options = new SnapshotBufferOptions(); private  options  =   new   SnapshotBufferOptions ( ) ;  this.options.interpolationTarget = -0.25;  this . options . interpolationTarget  =   - 0.25 ;   const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25; const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25; const options = new SnapshotBufferOptions();options.interpolationTarget = -0.25; const options = new SnapshotBufferOptions(); const  options  =   new   SnapshotBufferOptions ( ) ;  options.interpolationTarget = -0.25; options . interpolationTarget   =   - 0.25 ;   Call StorageProperty.setSmoothing() to configure smoothing for the property. You can either pass in a SnapshotBufferOptions object, or a JS object with matching properties. Even passing in an empty object like {} is enough to enable smoothing. TypeScriptJavaScriptscoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 }); TypeScript JavaScript scoreProp.setSmoothing({ interpolationTarget: -0.25 });scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp . setSmoothing ( {  interpolationTarget :   - 0.25   } ) ;   scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp.setSmoothing({ interpolationTarget: -0.25 }); scoreProp . setSmoothing ( {   interpolationTarget :   - 0.25   } ) ;   You can pass in SnapshotBufferOptions as an optional third parameter when constructing a StorageProperty. TypeScriptJavaScriptprivate prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,}); TypeScript JavaScript private prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,});const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,}); private prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,}); private prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,}); private prop = StorageProperty('prop', StorageTypes.float, {    interpolationTarget: -0.25,}); private prop = StorageProperty('prop', StorageTypes.float, { private  prop  =   StorageProperty ( 'prop' ,  StorageTypes . float ,   {      interpolationTarget: -0.25,     interpolationTarget :   - 0.25 ,  });  } ) ;   const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,}); const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,}); const prop = new StorageProperty('prop', StorageTypes.float, {  interpolationTarget: -0.25,}); const prop = new StorageProperty('prop', StorageTypes.float, { const  prop  =   new   StorageProperty ( 'prop' ,   StorageTypes . float ,   {    interpolationTarget: -0.25,    interpolationTarget :   - 0.25 ,  });  } ) ;   SnapshotBufferOptions can also be passed into automatic and manual StorageProperty constructors that support smoothing. TypeScriptJavaScriptconst prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); TypeScript JavaScript const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,});const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, { const  prop  =  StorageProperty . forPosition ( transform ,   true ,   {    interpolationTarget: -0.25,   interpolationTarget :   - 0.25 ,  });  } ) ;    const otherProp = StorageProperty.manualFloat('myFloat', 0, {  const  otherProp  =  StorageProperty . manualFloat ( 'myFloat' ,   0 ,   {    interpolationTarget: -0.25,   interpolationTarget :   - 0.25 ,  });  } ) ;   const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, {  interpolationTarget: -0.25,});const otherProp = StorageProperty.manualFloat('myFloat', 0, {  interpolationTarget: -0.25,}); const prop = StorageProperty.forPosition(transform, true, { const  prop  =   StorageProperty . forPosition ( transform ,   true ,   {    interpolationTarget: -0.25,    interpolationTarget :   - 0.25 ,  });  } ) ;    const otherProp = StorageProperty.manualFloat('myFloat', 0, {  const  otherProp  =   StorageProperty . manualFloat ( 'myFloat' ,   0 ,   {    interpolationTarget: -0.25,    interpolationTarget :   - 0.25 ,  });  } ) ;   Storage Types\u200b StorageTypes.bool StorageTypes.float StorageTypes.double StorageTypes.int StorageTypes.string StorageTypes.vec2 StorageTypes.vec3 StorageTypes.vec4 StorageTypes.quat StorageTypes.mat2 StorageTypes.mat3 StorageTypes.mat4 StorageTypes.boolArray StorageTypes.floatArray StorageTypes.doubleArray StorageTypes.intArray StorageTypes.stringArray StorageTypes.vec2Array StorageTypes.vec3Array StorageTypes.vec4Array StorageTypes.quatArray StorageTypes.mat2Array StorageTypes.mat3Array StorageTypes.mat4Array StorageTypes.packedTransform (same as StorageTypes.vec4Array) Payload and Rate Limits\u200b Storage Property updates are batched each frame and sent as a single message, which counts against the payload and message rate limits. If limits are exceeded in Lens Studio, an error will print in the Logger and the session may disconnect. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Start Menu and Single Player Next Sync Entity Adding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Adding Storage Properties to a SyncEntityManual Storage PropertiesSetting Storage Property ValuesGetting Storage Property ValuesReacting to Storage Property changesAutomatic Storage PropertiesLimiting Send RateSmoothing Property ValuesStorage TypesPayload and Rate Limits Adding Storage Properties to a SyncEntity Manual Storage Properties Setting Storage Property Values Getting Storage Property Values Reacting to Storage Property changes Automatic Storage Properties Limiting Send Rate Smoothing Property Values Storage Types Payload and Rate Limits AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/sync-entity": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntityAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Spectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync MaterialsCreating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Spectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync Materials Spectacles FrameworksSpectacles Sync KitFeaturesSync EntityOn this pageCopy pageSync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');});Was this page helpful?YesNoPreviousStorage PropertiesNextSync Materials  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Sync Entity Sync Entity On this page Copy page  Copy page     page Sync Entity\nThe SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data.\nCreating a New SyncEntity\u200b\nCreating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter.\nTypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script);\nThe constructor also accepts the following optional parameters:\n\nstoragePropertySet: StoragePropertySet\nclaimOwnership: boolean, see Ownership\npersistence: RealtimeStoreCreateOptions.Persistence, see Persistence\nnetworkIdOptions: NetworkIdOptions\n\nTypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null);\nGetting an Existing SyncEntity\u200b\nThere may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one.\nA SyncEntity can be found on a scene object by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject());\nA SyncEntity can be found on a component by calling:\nTypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);\nIf no SyncEntity is found, null is returned.\nSyncEntity Setup\u200b\nBefore a SyncEntity is fully available for use, it needs to finish its setup process. This includes:\n\nWaiting for the multiplayer session to connect.\nConnecting to its existing Realtime Store, or creating a new one if none is found.\n\nIt is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately.\nTypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady);\nYou can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean.\nTypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished}\nMany actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties.\nOwnership\u200b\nEach SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership.\nRequest Ownership\u200b\nThere are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward.\nRequest Ownership on Creation\u200b\nOwnership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true.\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true);\nIf the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.\nAfter the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request.\nOne-Time Ownership Request\u200b\nTo do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError);\nQueued Ownership Request\u200b\nAlternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible.\nTypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError);\nRevoke Ownership\u200b\nOwnership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity.\nTypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');});\nCheck Ownership\u200b\nCheck if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned().\nTypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');}\nCheck if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore().\nTypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');}\nCheck if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore().\nTypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();}\nTo get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned.\nTypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);\nReact to Ownership changes\u200b\nUse the SyncEntity.onOwnerUpdated event to be notified when ownership changes.\nTypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);});\nPersistence\u200b\nEach SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\").\n\nSession: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user.\nPersist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles.\nOwner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed.\nEphemeral: This is not suggested for use with SyncEntity.\n\nTypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session');\nStorage Properties\u200b\nStorage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object.\nFor more information on how to create, modify, and respond to storage property changes, see Storage Properties.\nNetwork ID\u200b\nA Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped.\n\nNetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom\nNetworkID.customNetworkId: Custom string\nNetworkID.customPrefix: Custom string to prepend to the Network ID\n\nNetworked Events\u200b\nNetworked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication.\nFor more information on how to create, send, and respond to networked events, see Networked Events.\nDestroying a SyncEntity\u200b\nTo destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to.\nTypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy();\nDestroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed.\nTypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();}\nSyncEntities also have events that are triggered when destroyed.\n\nSyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user.\nSyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user.\nSyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user.\n\nA callback function can be added to these events.\nTypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); Sync Entity The SyncEntity class is the scripting foundation for syncing data in a Lens. A SyncEntity connects a script to a Realtime Store, letting the script manage the state of the entity, including synced data. Creating a New SyncEntity\u200b Creating a new SyncEntity is as simple as calling the SyncEntity constructor. The script component associated with the SyncEntity is passed as a required parameter. TypeScriptJavaScriptconst syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script); TypeScript JavaScript const syncEntity: SyncEntity = new SyncEntity(this);const syncEntity = new SyncEntity(script); const syncEntity: SyncEntity = new SyncEntity(this); const syncEntity: SyncEntity = new SyncEntity(this); const syncEntity: SyncEntity = new SyncEntity(this); const syncEntity: SyncEntity = new SyncEntity(this); const  syncEntity :  SyncEntity  =   new   SyncEntity ( this ) ;   const syncEntity = new SyncEntity(script); const syncEntity = new SyncEntity(script); const syncEntity = new SyncEntity(script); const syncEntity = new SyncEntity(script); const  syncEntity  =   new   SyncEntity ( script ) ;   The constructor also accepts the following optional parameters: storagePropertySet: StoragePropertySet claimOwnership: boolean, see Ownership persistence: RealtimeStoreCreateOptions.Persistence, see Persistence networkIdOptions: NetworkIdOptions TypeScriptJavaScriptthis.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null); TypeScript JavaScript this.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null)const syncEntity = new SyncEntity(script, null, true, 'Session', null); this.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null) this.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null) this.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null) this.syncEntity: SyncEntity = new SyncEntity(this, null, true, \"Session\", null) this . syncEntity :  SyncEntity  =   new   SyncEntity ( this ,   null ,   true ,   \"Session\" ,   null )   const syncEntity = new SyncEntity(script, null, true, 'Session', null); const syncEntity = new SyncEntity(script, null, true, 'Session', null); const syncEntity = new SyncEntity(script, null, true, 'Session', null); const syncEntity = new SyncEntity(script, null, true, 'Session', null); const  syncEntity  =   new   SyncEntity ( script ,   null ,   true ,   'Session' ,   null ) ;   Getting an Existing SyncEntity\u200b There may be cases where a scene object already has a script with a sync entity attached to it, such as a SyncTransform component. In this case, the existing SyncEntity can be accessed from another script, instead of creating a new one. A SyncEntity can be found on a scene object by calling: TypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject()); TypeScript JavaScript this.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject());const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject()); this.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject()); this.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject()); this.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject()); this.syncEntity = SyncEntity.getSyncEntityOnSceneObject(this.getSceneObject()); this . syncEntity  =  SyncEntity . getSyncEntityOnSceneObject ( this . getSceneObject ( ) ) ;   const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject()); const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject()); const syncEntity = SyncEntity.getSyncEntityOnSceneObject(  script.getSceneObject()); const syncEntity = SyncEntity.getSyncEntityOnSceneObject( const  syncEntity  =   SyncEntity . getSyncEntityOnSceneObject (    script.getSceneObject()   script . getSceneObject ( )  );  ) ;   A SyncEntity can be found on a component by calling: TypeScriptJavaScriptthis.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); TypeScript JavaScript this.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent);const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); this.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); this.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); this.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); this.syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); this . syncEntity  =  SyncEntity . getSyncEntityOnComponent ( scriptComponent ) ;   const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); const syncEntity = SyncEntity.getSyncEntityOnComponent(scriptComponent); const  syncEntity  =   SyncEntity . getSyncEntityOnComponent ( scriptComponent ) ;   If no SyncEntity is found, null is returned. SyncEntity Setup\u200b Before a SyncEntity is fully available for use, it needs to finish its setup process. This includes: Waiting for the multiplayer session to connect. Connecting to its existing Realtime Store, or creating a new one if none is found. It is important that setup is finished before data is accessed from the SyncEntity. To check that a SyncEntity is ready to use, call SyncEntity.notifyOnReady(). The method accepts a callback function that runs once the SyncEntity is ready. If setup is already finished, the callback function executes immediately. TypeScriptJavaScriptonReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady); TypeScript JavaScript onReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady())function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady); onReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady()) onReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady()) onReady() {    print('The session has started and this entity is ready!')    // Start your entity's behavior here!}this.syncEntity.notifyOnReady(() => this.onReady()) onReady() { onReady ( )   {      print('The session has started and this entity is ready!')      print ( 'The session has started and this entity is ready!' )      // Start your entity's behavior here!      // Start your entity's behavior here!  }  }    this.syncEntity.notifyOnReady(() => this.onReady())  this . syncEntity . notifyOnReady ( ( )   =>   this . onReady ( ) )   function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady); function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady); function onReady() {  print('The session has started and this entity is ready!');  // Start your entity's behavior here!}syncEntity.notifyOnReady(onReady); function onReady() { function   onReady ( )   {    print('The session has started and this entity is ready!');    print ( 'The session has started and this entity is ready!' ) ;    // Start your entity's behavior here!    // Start your entity's behavior here!  }  }    syncEntity.notifyOnReady(onReady); syncEntity . notifyOnReady ( onReady ) ;   You can also check if setup has completed by checking SyncEntity.isSetupFinished, which is a boolean. TypeScriptJavaScriptif (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished} TypeScript JavaScript if (this.syncEntity.isSetupFinished) {  // Setup is finished}if (syncEntity.isSetupFinished) {  // Setup is finished} if (this.syncEntity.isSetupFinished) {  // Setup is finished} if (this.syncEntity.isSetupFinished) {  // Setup is finished} if (this.syncEntity.isSetupFinished) {  // Setup is finished} if (this.syncEntity.isSetupFinished) { if   ( this . syncEntity . isSetupFinished )   {    // Setup is finished    // Setup is finished  }  }   if (syncEntity.isSetupFinished) {  // Setup is finished} if (syncEntity.isSetupFinished) {  // Setup is finished} if (syncEntity.isSetupFinished) {  // Setup is finished} if (syncEntity.isSetupFinished) { if   ( syncEntity . isSetupFinished )   {    // Setup is finished    // Setup is finished  }  }   Many actions are completely fine to do before setup is finished, such as subscribing to events, adding storage properties, or subscribing to storage property changes. You can even preemptively request ownership before setup is finished. It is generally good practice to wait until setup has completed before starting any SyncEntity behavior like sending networked events, or modifying storage properties. Ownership\u200b Each SyncEntity can either have an owner or be unowned. If a SyncEntity is owned, only the owner is allowed to modify its values. If a SyncEntity is unowned, any user can modify the values. Ownership follows the same rules as Realtime Store ownership. There are multiple ways to request ownership of a SyncEntity, either when the entity is created or afterward. Ownership can be requested while constructing the SyncEntity by passing a requestOwnership boolean as the third parameter. To request ownership, pass true. TypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true); TypeScript JavaScript this.syncEntity = new SyncEntity(this, null, true);const syncEntity = new SyncEntity(script, null, true); this.syncEntity = new SyncEntity(this, null, true); this.syncEntity = new SyncEntity(this, null, true); this.syncEntity = new SyncEntity(this, null, true); this.syncEntity = new SyncEntity(this, null, true); this . syncEntity  =   new   SyncEntity ( this ,   null ,   true ) ;   const syncEntity = new SyncEntity(script, null, true); const syncEntity = new SyncEntity(script, null, true); const syncEntity = new SyncEntity(script, null, true); const syncEntity = new SyncEntity(script, null, true); const  syncEntity  =   new   SyncEntity ( script ,   null ,   true ) ;   If the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned.   If the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned. If the SyncEntity is already owned, the ownership request will be added to a queue and tried again whenever the entity becomes unowned. After the SyncEntity has been created, ownership can either be requested with a one-time request, or as a queued request. To do a one-time ownership request, use SyncEntity.requestOwnership(). The method accepts onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user already owns it. If the SyncEntity is unowned, ownership is granted and the success callback runs. If the SyncEntity is already owned, the request is not granted and nothing further happens. If an error occurs during the request, the error callback runs. TypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError); TypeScript JavaScript onSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError); onSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError) onSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError) onSuccess() {    print(\"Ownership claimed\");}onError() {    print(\"Error, ownership not claimed);}this.syncEntity.requestOwnership(this.onSuccess, this.onError) onSuccess() { onSuccess ( )   {      print(\"Ownership claimed\");      print ( \"Ownership claimed\" ) ;  }  }    onError() {  onError ( )   {      print(\"Error, ownership not claimed);      print ( \"Error ,  ownership not claimed ) ;  }  }    this.syncEntity.requestOwnership(this.onSuccess, this.onError)  this . syncEntity . requestOwnership ( this . onSuccess ,   this . onError )   function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError); function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError); function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.requestOwnership(onSuccess, onError); function onSuccess() { function   onSuccess ( )   {      print(\"Ownership claimed\");      print ( \"Ownership claimed\" ) ;  }  }    function onError() {  function   onError ( )   {      print(\"Error, ownership not claimed);      print ( \" Error ,  ownership not claimed ) ;  }  }    syncEntity.requestOwnership(onSuccess, onError); syncEntity . requestOwnership ( onSuccess ,  onError ) ;   Alternatively, ownership requests can be placed in a queue and granted when the SyncEntity becomes unowned. This is done by calling SyncEntity.tryClaimOwnership(), which also takes onSuccess and onError callbacks as parameters. This will also immediately callback with success if the local user already owns it. This places the request in a queue, and the SyncEntity will continue to try gaining ownership whenever the SyncEntity becomes unowned. If the SyncEntity has not yet finished setup when this function is called, the ownership request will be put into the queue so that ownership can be requested as soon as possible. TypeScriptJavaScriptonSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError); TypeScript JavaScript onSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError); onSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError) onSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError) onSuccess() {    print(\"Ownership claimed\")}onError() {    print(\"Error, ownership not claimed)}this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError) onSuccess() { onSuccess ( )   {      print(\"Ownership claimed\")      print ( \"Ownership claimed\" )  }  }    onError() {  onError ( )   {      print(\"Error, ownership not claimed)      print ( \"Error ,  ownership not claimed )  }  }    this.syncEntity.tryClaimOwnership(this.onSuccess, this.onError)  this . syncEntity . tryClaimOwnership ( this . onSuccess ,   this . onError )   function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError); function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError); function onSuccess() {    print(\"Ownership claimed\");}function onError() {    print(\"Error, ownership not claimed);}syncEntity.tryClaimOwnership(onSuccess, onError); function onSuccess() { function   onSuccess ( )   {      print(\"Ownership claimed\");      print ( \"Ownership claimed\" ) ;  }  }    function onError() {  function   onError ( )   {      print(\"Error, ownership not claimed);      print ( \" Error ,  ownership not claimed ) ;  }  }    syncEntity.tryClaimOwnership(onSuccess, onError); syncEntity . tryClaimOwnership ( onSuccess ,  onError ) ;   Ownership of a SyncEntity can be revoked using SyncEntity.tryRevokeOwnership(). The method accepts optional onSuccess and onError callbacks as parameters. This will immediately callback with success if the local user does not own the SyncEntity. TypeScriptJavaScriptthis.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');}); TypeScript JavaScript this.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');});syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');}); this.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');}); this.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');}); this.syncEntity.tryRevokeOwnership(() => {  print('Ownership revoked');}); this.syncEntity.tryRevokeOwnership(() => { this . syncEntity . tryRevokeOwnership ( ( )   =>   {    print('Ownership revoked');    print ( 'Ownership revoked' ) ;  });  } ) ;   syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');}); syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');}); syncEntity.tryRevokeOwnership(function () {  print('Ownership revoked');}); syncEntity.tryRevokeOwnership(function () { syncEntity . tryRevokeOwnership ( function   ( )   {    print('Ownership revoked');    print ( 'Ownership revoked' ) ;  });  } ) ;   Check if the SyncEntity\u2019s Realtime Store is owned using SyncEntity.isStoreOwned(). TypeScriptJavaScriptif (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');} TypeScript JavaScript if (this.syncEntity.isStoreOwned()) {  print('Store is owned');}if (syncEntity.isStoreOwned()) {  print('Store is owned');} if (this.syncEntity.isStoreOwned()) {  print('Store is owned');} if (this.syncEntity.isStoreOwned()) {  print('Store is owned');} if (this.syncEntity.isStoreOwned()) {  print('Store is owned');} if (this.syncEntity.isStoreOwned()) { if   ( this . syncEntity . isStoreOwned ( ) )   {    print('Store is owned');    print ( 'Store is owned' ) ;  }  }   if (syncEntity.isStoreOwned()) {  print('Store is owned');} if (syncEntity.isStoreOwned()) {  print('Store is owned');} if (syncEntity.isStoreOwned()) {  print('Store is owned');} if (syncEntity.isStoreOwned()) { if   ( syncEntity . isStoreOwned ( ) )   {    print('Store is owned');    print ( 'Store is owned' ) ;  }  }   Check if the local user owns the SyncEntity\u2019s Realtime Store using SyncEntity.doIOwnStore(). TypeScriptJavaScriptif (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');} TypeScript JavaScript if (this.syncEntity.doIOwnStore()) {  print('I own the store');}if (syncEntity.doIOwnStore()) {  print('I own the store');} if (this.syncEntity.doIOwnStore()) {  print('I own the store');} if (this.syncEntity.doIOwnStore()) {  print('I own the store');} if (this.syncEntity.doIOwnStore()) {  print('I own the store');} if (this.syncEntity.doIOwnStore()) { if   ( this . syncEntity . doIOwnStore ( ) )   {    print('I own the store');    print ( 'I own the store' ) ;  }  }   if (syncEntity.doIOwnStore()) {  print('I own the store');} if (syncEntity.doIOwnStore()) {  print('I own the store');} if (syncEntity.doIOwnStore()) {  print('I own the store');} if (syncEntity.doIOwnStore()) { if   ( syncEntity . doIOwnStore ( ) )   {    print('I own the store');    print ( 'I own the store' ) ;  }  }   Check if the local user is allowed to modify the SyncEntity\u2019s Realtime Store using SyncEntity.canIModifyStore(). TypeScriptJavaScriptif (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();} TypeScript JavaScript if (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();}if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();} if (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();} if (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();} if (this.syncEntity.canIModifyStore()) {  this.syncEntity.requestOwnership();} if (this.syncEntity.canIModifyStore()) { if   ( this . syncEntity . canIModifyStore ( ) )   {    this.syncEntity.requestOwnership();    this . syncEntity . requestOwnership ( ) ;  }  }   if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();} if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();} if (syncEntity.canIModifyStore()) {  syncEntity.requestOwnership();} if (syncEntity.canIModifyStore()) { if   ( syncEntity . canIModifyStore ( ) )   {    syncEntity.requestOwnership();   syncEntity . requestOwnership ( ) ;  }  }   To get the current owner\u2019s UserInfo, use SyncEntity.currentOwner. This can return null, or be a UserInfo object with null fields if the SyncEntity is unowned. TypeScriptJavaScriptconst owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); TypeScript JavaScript const owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner);const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = this.syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = this.syncEntity.currentOwner.displayName; const  owner  =   this . syncEntity . currentOwner . displayName ;  print('Store is owned by ' + owner);  print ( 'Store is owned by '   +  owner ) ;   const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = syncEntity.currentOwner.displayName;print('Store is owned by ' + owner); const owner = syncEntity.currentOwner.displayName; const  owner  =  syncEntity . currentOwner . displayName ;  print('Store is owned by ' + owner);  print ( 'Store is owned by '   +  owner ) ;   Use the SyncEntity.onOwnerUpdated event to be notified when ownership changes. TypeScriptJavaScriptthis.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);}); TypeScript JavaScript this.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);});syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);}); this.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);}); this.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);}); this.syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + this.syncEntity.currentOwner.userId);}); this.syncEntity.onOwnerUpdated.add(function () { this . syncEntity . onOwnerUpdated . add ( function   ( )   {    print('Owner updated to ' + this.syncEntity.currentOwner.userId);    print ( 'Owner updated to '   +   this . syncEntity . currentOwner . userId ) ;  });  } ) ;   syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);}); syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);}); syncEntity.onOwnerUpdated.add(function () {  print('Owner updated to ' + syncEntity.currentOwner.userId);}); syncEntity.onOwnerUpdated.add(function () { syncEntity . onOwnerUpdated . add ( function   ( )   {    print('Owner updated to ' + syncEntity.currentOwner.userId);    print ( 'Owner updated to '   +  syncEntity . currentOwner . userId ) ;  });  } ) ;   Persistence\u200b Each SyncEntity has a persistence setting that is specified during construction. The argument can either be a RealtimeStoreCreateOptions.Persistence value, or a string (e.g., \"Session\"). Session: Default value. The SyncEntity\u2019s state persists as long as at least one user is in the session. If no users are in the session, the SyncEntity\u2019s state will be reset the next time the session is joined by a user. Persist: The SyncEntity\u2019s state persists even after all users leave the session. Not currently supported on Spectacles. Owner: This is meant to be used with SyncEntities that have an owner. If the owner leaves the session, the SyncEntity will automatically be destroyed. Ephemeral: This is not suggested for use with SyncEntity. TypeScriptJavaScriptthis.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session'); TypeScript JavaScript this.syncEntity = new SyncEntity(this, null, false, 'Session');const syncEntity = new SyncEntity(script, null, false, 'Session'); this.syncEntity = new SyncEntity(this, null, false, 'Session'); this.syncEntity = new SyncEntity(this, null, false, 'Session'); this.syncEntity = new SyncEntity(this, null, false, 'Session'); this.syncEntity = new SyncEntity(this, null, false, 'Session'); this . syncEntity  =   new   SyncEntity ( this ,   null ,   false ,   'Session' ) ;   const syncEntity = new SyncEntity(script, null, false, 'Session'); const syncEntity = new SyncEntity(script, null, false, 'Session'); const syncEntity = new SyncEntity(script, null, false, 'Session'); const syncEntity = new SyncEntity(script, null, false, 'Session'); const  syncEntity  =   new   SyncEntity ( script ,   null ,   false ,   'Session' ) ;   Storage Properties\u200b Storage properties allow values to be easily synchronized on a SyncEntity via its Realtime Store. For example, storage properties can be used to share a player\u2019s current score or display name, or to sync the position of a scene object. For more information on how to create, modify, and respond to storage property changes, see Storage Properties. Network ID\u200b A Network ID uniquely identifies a SyncEntity in the session. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. It is the default and should be used in most cases. Custom lets you enter your own Network ID string. The NetworkIdOptions class can be passed into the SyncEntity constructor, although it can usually be skipped. NetworkID.networkIdType: NetworkIdType.ObjectId or NetworkIDType.Custom NetworkID.customNetworkId: Custom string NetworkID.customPrefix: Custom string to prepend to the Network ID Networked Events\u200b Networked events can be used to send messages from the local SyncEntity to other clients in the Connected Lenses session. Networked events do not change the state of the SyncEntity, but enable immediate, one-time communication. For more information on how to create, send, and respond to networked events, see Networked Events. Destroying a SyncEntity\u200b To destroy a SyncEntity, call SyncEntity.destroy(). This will also destroy the scene object that the SyncEntity\u2019s script component is attached to. TypeScriptJavaScriptthis.syncEntity.destroy();syncEntity.destroy(); TypeScript JavaScript this.syncEntity.destroy();syncEntity.destroy(); this.syncEntity.destroy(); this.syncEntity.destroy(); this.syncEntity.destroy(); this.syncEntity.destroy(); this . syncEntity . destroy ( ) ;   syncEntity.destroy(); syncEntity.destroy(); syncEntity.destroy(); syncEntity.destroy(); syncEntity . destroy ( ) ;   Destroying the SyncEntity will mark it as destroyed by setting SyncEntity.destroyed to true. This boolean can be used to check if a SyncEntity has been destroyed. TypeScriptJavaScriptif (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();} TypeScript JavaScript if (!this.syncEntity.destroyed) {  this.syncEntity.destroy();}if (!syncEntity.destroyed) {  syncEntity.destroy();} if (!this.syncEntity.destroyed) {  this.syncEntity.destroy();} if (!this.syncEntity.destroyed) {  this.syncEntity.destroy();} if (!this.syncEntity.destroyed) {  this.syncEntity.destroy();} if (!this.syncEntity.destroyed) { if   ( ! this . syncEntity . destroyed )   {    this.syncEntity.destroy();    this . syncEntity . destroy ( ) ;  }  }   if (!syncEntity.destroyed) {  syncEntity.destroy();} if (!syncEntity.destroyed) {  syncEntity.destroy();} if (!syncEntity.destroyed) {  syncEntity.destroy();} if (!syncEntity.destroyed) { if   ( ! syncEntity . destroyed )   {    syncEntity.destroy();   syncEntity . destroy ( ) ;  }  }   SyncEntities also have events that are triggered when destroyed. SyncEntity.onDestroyed: Called when a SyncEntity is destroyed by a local or remote user. SyncEntity.onLocalDestroyed: Called when a SyncEntity is destroyed by the local user. SyncEntity.onRemoteDestroyed: Called when a SyncEntity is destroyed by another user. A callback function can be added to these events. TypeScriptJavaScriptthis.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); TypeScript JavaScript this.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');});syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); this.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');}); this.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');}); this.syncEntity.onDestroyed.add(() => {  print('Sync entity was destroyed');});this.syncEntity.onLocalDestroyed.add(() => {  print('Sync entity was destroyed by me');});this.syncEntity.onRemoteDestroyed.add(() => {  print('Sync entity was destroyed by another user');}); this.syncEntity.onDestroyed.add(() => { this . syncEntity . onDestroyed . add ( ( )   =>   {    print('Sync entity was destroyed');    print ( 'Sync entity was destroyed' ) ;  });  } ) ;    this.syncEntity.onLocalDestroyed.add(() => {  this . syncEntity . onLocalDestroyed . add ( ( )   =>   {    print('Sync entity was destroyed by me');    print ( 'Sync entity was destroyed by me' ) ;  });  } ) ;    this.syncEntity.onRemoteDestroyed.add(() => {  this . syncEntity . onRemoteDestroyed . add ( ( )   =>   {    print('Sync entity was destroyed by another user');    print ( 'Sync entity was destroyed by another user' ) ;  });  } ) ;   syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); syncEntity.onDestroyed.add(function () {  print('Sync entity was destroyed');});syncEntity.onLocalDestroyed.add(function () {  print('Sync entity was destroyed by me');});syncEntity.onRemoteDestroyed.add(function () {  print('Sync entity was destroyed by another user');}); syncEntity.onDestroyed.add(function () { syncEntity . onDestroyed . add ( function   ( )   {    print('Sync entity was destroyed');    print ( 'Sync entity was destroyed' ) ;  });  } ) ;    syncEntity.onLocalDestroyed.add(function () { syncEntity . onLocalDestroyed . add ( function   ( )   {    print('Sync entity was destroyed by me');    print ( 'Sync entity was destroyed by me' ) ;  });  } ) ;    syncEntity.onRemoteDestroyed.add(function () { syncEntity . onRemoteDestroyed . add ( function   ( )   {    print('Sync entity was destroyed by another user');    print ( 'Sync entity was destroyed by another user' ) ;  });  } ) ;   Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Storage Properties Next Sync Materials Creating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Creating a New SyncEntityGetting an Existing SyncEntitySyncEntity SetupOwnershipPersistenceStorage PropertiesNetwork IDNetworked EventsDestroying a SyncEntity Creating a New SyncEntity Getting an Existing SyncEntity SyncEntity Setup Ownership Persistence Storage Properties Network ID Networked Events Destroying a SyncEntity AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/sync-materials": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto cloneAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto clone Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto clone Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto clone Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto clone Spectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime StoreSetupAuto clone Spectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime Store Spectacles FrameworksSpectacles Sync KitFeaturesSync MaterialsOn this pageCopy pageSync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab.Was this page helpful?YesNoPreviousSync EntityNextSync Realtime Store  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Sync Materials Sync Materials On this page Copy page  Copy page     page Sync Materials\nThe SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.\n\nThe SyncMaterials component supports syncing material properties of the following types:\n\nfloat\nvec2\nvec3\nvec4\n\nSetup\u200b\n\n\nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n\n\nAdd the material to be synced to the Main Material Input.\n\n\nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n\n\nAuto clone\u200b\nIf Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab. Sync Materials The SyncMaterials component can be used to sync material properties, such as color. Any changes to the configured properties are automatically synchronized across the network. Attaching a SyncMaterials component to a scene object turns that object into a SyncEntity. The SyncMaterials component uses storage properties to sync material properties.  The SyncMaterials component supports syncing material properties of the following types: float vec2 vec3 vec4 Setup\u200b \nAdd the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced.\n Add the SyncMaterials component to the scene object in the Scene Hierarchy that uses the material to be synced. \nAdd the material to be synced to the Main Material Input.\n Add the material to be synced to the Main Material Input. \nUnder Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor.\n Under Property Names, click + Add Value and enter the string of the property name to be synced, e.g., baseColor. Auto clone\u200b If Auto Clone is enabled, the Main Material is immediately cloned on Lens start. Next, the scene object that the SyncMaterials component is attached to, and all of its children, are recursively searched to find any MaterialMeshVisuals that are using the assigned Main Material. The main material on each MaterialMeshVisual is then set to the cloned material. This is useful when instantiating a new object and assigning its own material to it, so the material is not shared with other instances of the prefab. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Sync Entity Next Sync Realtime Store SetupAuto clone SetupAuto clone Setup Auto clone AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/sync-transform": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetupAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetup Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetup Spectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser InformationSetup Spectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser Information Spectacles FrameworksSpectacles Sync KitFeaturesSync TransformOn this pageCopy pageSync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.Was this page helpful?YesNoPreviousSync Realtime StoreNextUser Information  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features Sync Transform Sync Transform On this page Copy page  Copy page     page Sync Transform\nThe SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.\n\nSetup\u200b\nTo begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel:\nNetwork ID Type\u200b\nThe Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy.\nSync Settings\u200b\nUse the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available:\n\nNone: Data is not synced.\nLocation: Data is synced relative to the co-located coordinate space.\nLocal: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space.\nWorld: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience.\n\nSends Per Second\u200b\nThe sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second.\nSetting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars.\nSmoothing\u200b\nWhen Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy.\nThe Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender.\nWhen using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.\nSyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values. Sync Transform The SyncTransform component can be used to automatically sync a scene object\u2019s position, rotation, and / or scale, depending on how it is configured in the Inspector panel. Attaching a SyncTransform component to a scene object turns that object into a SyncEntity. The SyncTransform component uses storage properties to synchronize transform data across the network.  Setup\u200b To begin setup, add the SyncTransform component to the scene object to be synced. Next, configure the following inputs in the Inspector panel: The Network ID uniquely identifies a SyncEntity. There are two types of Network ID that can be used \u2013 Object ID, or Custom. Object ID automatically generates a Network ID for the SyncEntity. This should be used in most cases. Custom lets you enter your own Network ID in an additional input field. An example of when you may need to use a Custom Network ID is if you need to use multiple SyncTransforms within the same scene object hierarchy. Use the Sync Settings drop downs to set how the position, rotation, and scale are synced. The following options are available: None: Data is not synced. Location: Data is synced relative to the co-located coordinate space. Local: Data is synced relative to the scene object\u2019s parent\u2019s coordinate space. World: Data is synced relative to Spectacles\u2019 world coordinate system. This is generally not recommended for a colocated experience. The sends per second parameter limits the number of times per second that transform data is sent across the network. The default value for the input is 10 messages per second. To set a limit, the input value must be a number greater than zero. A value of zero stops messages from sending and a value less than zero removes the limit on sends per second. Setting the sends per second value is useful for managing network traffic to ensure the Lens stays within rate limits and performs under different network conditions. This is especially relevant for scene object transforms that change continuously, like player avatars. When Use Smoothing is enabled, synced values are interpolated by receiving clients to produce fluid changes in position, rotation, or scale. Without smoothing enabled, transform changes may look choppy. The Interpolation Target controls the time offset that is used for smoothing. For example, the default value of -0.25 means that the Lens is trying to display the object as it was 0.25 seconds in the past. The further back in time the target is set to, the more likely the component has values to smooth with, and the more lag a receiving client will experience relative to the sender. When using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time.   When using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time. When using SyncTransform smoothing with an unowned object, you may encounter a warning that out of order timestamps were received. This typically means that the object\u2019s transform was modified on multiple devices at the same time, and now the client isn't sure which value is the true value. The same problem can occur when smoothing is turned off, but in that case, we don't detect or warn about the issue. The recommended way to resolve the warning is to handle ownership of the object, so that only one device can update transform values at a time. SyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values.   SyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values. SyncTransform does not support changing syncing parameters (e.g., sends per second, smoothing) or disabling the component via script. Once SyncTransform is initialized, it will continue running for the duration of the Lens based on its configuration in the Inspector. If you want to control when transform data is synced, consider setting up your own storage properties and using custom logic to update their values. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Sync Realtime Store Next User Information Setup Setup Setup AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-sync-kit/features/user-information": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample ScenarioAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample Scenario Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample Scenario Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIsSpectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample Scenario Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser InformationPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync KitGetting StartedLifecycleFeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Spectacles Sync Kit Getting Started Lifecycle FeaturesContent PlacementDebuggingHelper ScriptsNetworked EventsPayload and Rate LimitsPrefab InstantiationSession ControllerStart Menu and Single PlayerStorage PropertiesSync EntitySync MaterialsSync Realtime StoreSync TransformUser Information Features Content Placement Debugging Helper Scripts Networked Events Payload and Rate Limits Prefab Instantiation Session Controller Start Menu and Single Player Storage Properties Sync Entity Sync Materials Sync Realtime Store Sync Transform User Information Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Spectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample Scenario Spectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverviewUserIdConnectionIdDisplayNameExample Scenario Spectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverview Spectacles FrameworksSpectacles Sync KitFeaturesUser InformationOn this pageCopy pageUser Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles BWas this page helpful?YesNoPreviousSync TransformNextOverview  Spectacles Frameworks Spectacles Frameworks Spectacles Sync Kit Spectacles Sync Kit Features Features User Information User Information On this page Copy page  Copy page     page User Information\nThere are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName.\nUserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender.\nUserId\u200b\nUserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string.\nConnectionId\u200b\nUserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string.\nDisplayName\u200b\nUserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string.\nExample Scenario\u200b\nThis is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation.\nLens #1\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aabc123Spectacles ASpectacles User Aabc456Spectacles ASpectacles User Bdef789Spectacles B\nLens #2\u200b\nuserIDconnectionIDdisplayNameSpectacles User Aghi987Spectacles ASpectacles User Aghi654Spectacles ASpectacles User Bjlk321Spectacles B User Information There are multiple ways to identify users in a Connected Lens session through the UserInfo class \u2013 UserInfo.userId, UserInfo.connectionId, and UserInfo.displayName. UserInfo is available from a number of sources, including SessionController APIs, SyncEntity.ownerInfo, and NetworkRoot.ownerInfo. MessageInfo for Networked Events also includes userId and connectionId of the message sender. UserId\u200b UserInfo.userId is tied to a user\u2019s Snapchat account. If two pairs of Spectacles are paired to the same user account, the userId for those devices in the session will be the same. The userId for a given user is different in different Lenses. UserInfo.userId is a string. ConnectionId\u200b UserInfo.connectionId is tied to the device that has joined the session. UserInfo.connectionId is unique to each connected device, and unique to each time the device joins a session. Realtime Store ownership is tied to connectionId, not userId. UserInfo.connectionId is a string. DisplayName\u200b UserInfo.displayName is the name a user chooses on their Snapchat account profile. It is consistent across devices, Lenses, and sessions. UserInfo.displayName is a string. Example Scenario\u200b This is a simple example showing a situation where one user joins a Connected Lenses session on two devices at once, and another user joins on a single device. These example userIds, connectionIds, and displayNames are simplified for explanation. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Sync Transform Next Overview UserIdConnectionIdDisplayNameExample Scenario UserIdConnectionIdDisplayNameExample Scenario UserId ConnectionId DisplayName Example Scenario AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/permission-privacy/overview": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions TypesAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions Types Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions Types Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions Types Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Permissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions Types Permissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIsUsing Non-Declared Permissions FeaturesList of Permissions Types Permissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIs Permissions & PrivacyOverviewOn this pageCopy pageOverview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModuleWas this page helpful?YesNoPreviousUser InformationNextExperimental APIs  Permissions & Privacy Permissions & Privacy Overview Overview On this page Copy page  Copy page     page Overview\nThe Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features.\nAs a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands.\nWhen using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page.\nTo discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings.\n\nUsing Non-Declared Permissions Features\u200b\nIn some cases, you might use capabilities that require permission declarations dynamically through scripts.\nFor example, when you dynamically instantiate a HeadComponent through script:\nlet headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nrequire('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();\nIn this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system.\n\nTo learn more about the types of permissions Lenses can use, review the List of Permissions Types section.\nList of Permissions Types\u200b\nPermission TypeAssets & APIs Requesting PermissionColocated Connected LensesConnectedLensModuleRemote Connected LensesConnectedLensModuleSpeech to TextVoiceMLModuleDialogModuleText to SpeechTextToSpeechModuleRemote APIsRemoteServiceModuleRemoteMediaModuleBitmojiBitmojiModuleCamera - SnapMLCameraTextureProviderCameraModuleMicrophone - SnapMLMicrophoneAudioProviderVoiceMLModuleDialogModuleGPS - PreciseRawLocationModuleLocation - CoarseProcessedLocationModuleFaceFaceRenderObjectProvider (Mesh)HeadComponent (Landmarks)FaceTrackingModuleFoundational TrackersDeviceTrackingDeviceTrackingModuleSupplementary TrackersSegmentationTextureProviderObjectTrackingAssetMarkerTrackingAssetDepthTextureProviderTrackedPointPointCloud World MeshBody TrackersObject3DAssetInternetRemoteServiceModuleInput Framework (Text)TextInputModule Overview The Spectacles camera captures information about hands, faces, and surroundings, enabling interaction with AR content in the environment. The microphone uses information about voice, facilitating features like recorded videos and voice commands. Both camera and microphone access are essential for the device to function properly. Additionally, Spectacles can use location data to support location-based Lenses and features. As a developer, you may use components that involve sensitive user data. For instance, Hand Tracking enables pinch gestures for interaction, and Voice ML lets users interact with experiences via voice commands. When using capabilities or components involving sensitive user data, Lens Studio automatically declares the necessary permissions in the project. While some Lenses are designed for solo use, others require communication with external devices, such as Connected Lenses or RemoteServiceModule, which interact with the internet. During such interactions, Spectacles prevent simultaneous use of these capabilities with sensitive user data to ensure privacy. These APIs are tagged as \u201cExposes User Data\u201d on the API page. To discover what permissions have been declared in your project, go to the Spectacles section inside your Project Settings. Using Non-Declared Permissions Features\u200b In some cases, you might use capabilities that require permission declarations dynamically through scripts. For example, when you dynamically instantiate a HeadComponent through script: let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();   let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); let headComponent = script.getSceneObject().createComponent('Component.Head'); let  headComponent  =  script . getSceneObject ( ) . createComponent ( 'Component.Head' ) ;  let headLandMarks = headComponent.getLandmarks();  let  headLandMarks  =  headComponent . getLandmarks ( ) ;   require('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks();   require('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); require('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); require('LensStudio:FaceTrackingModule');let headComponent = script.getSceneObject().createComponent('Component.Head');let headLandMarks = headComponent.getLandmarks(); require('LensStudio:FaceTrackingModule'); require ( 'LensStudio:FaceTrackingModule' ) ;  let headComponent = script.getSceneObject().createComponent('Component.Head');  let  headComponent  =  script . getSceneObject ( ) . createComponent ( 'Component.Head' ) ;  let headLandMarks = headComponent.getLandmarks();  let  headLandMarks  =  headComponent . getLandmarks ( ) ;   In this case, Lens Studio does not automatically declare the permission compared to attaching a HeadComponent to a SceneObject in the Scene Hierarchy Panel. Lenses that attempt to use features without declaring the permissions in their Lens Project will automatically get a permission denied by the SnapOS system. To learn more about the types of permissions Lenses can use, review the List of Permissions Types section. List of Permissions Types\u200b Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous User Information Next Experimental APIs Using Non-Declared Permissions FeaturesList of Permissions Types Using Non-Declared Permissions FeaturesList of Permissions Types Using Non-Declared Permissions Features List of Permissions Types AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)",
    "https://developers.snap.com/spectacles/permission-privacy/experimental-apis": "Skip to main contentSnap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended PermissionAI-Powered SearchCompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport InfringementPrivacy PolicyTerms of ServiceLanguageEnglish (US) Skip to main content Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI ReferenceSearch\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1 Snap for DevelopersLens StudioCamera KitSpectaclesSnap KitAPI Reference  Search\u2318KDeveloper PortalMy Lenses5.x5.x4.55.1  Search\u2318K Search Search \u2318K Developer Portal  Developer Portal My Lenses  My Lenses 5.x5.x4.55.1 5.x 4.55.1   Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended Permission Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended Permission Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIsPermissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended Permission Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat CamHomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Lens Studio 5DownloadCamera KitActivate SDKSpectaclesSnap KitCreate an AppAPI ReferenceSnapchat for WebSnapchat Cam Lens Studio 5Download  Lens Studio 5 Camera KitActivate SDK  Camera Kit Spectacles  Spectacles Snap KitCreate an App  Snap Kit API Reference  API Reference Snapchat for Web  Snapchat for Web Snapchat Cam  Snapchat Cam HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs   HomeGet StartedIntroductionStart BuildingGitHub Sample ProjectsBest PracticesDesign For SpectaclesPerformance OptimizationProfiling & DebuggingSpectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in SpectaclesSpectacles FrameworksSpectacles Interaction KitSpectacles Sync KitPermissions & PrivacyOverviewExperimental APIs Home Get StartedIntroductionStart BuildingGitHub Sample Projects Get Started Introduction Start Building Start Building GitHub Sample Projects Best PracticesDesign For SpectaclesPerformance OptimizationProfiling & Debugging Best Practices Design For Spectacles Design For Spectacles Performance Optimization Profiling & Debugging Profiling & Debugging Spectacles FeaturesOverviewAPIsFeaturesResourcesConnected Lenses [Beta]Audio in Spectacles Spectacles Features Overview APIs APIs Features Resources Connected Lenses [Beta] Connected Lenses [Beta] Audio in Spectacles Spectacles FrameworksSpectacles Interaction KitSpectacles Sync Kit Spectacles Frameworks Spectacles Interaction Kit Spectacles Interaction Kit Spectacles Sync Kit Spectacles Sync Kit Permissions & PrivacyOverviewExperimental APIs Permissions & Privacy Overview Experimental APIs       Permissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended Permission Permissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverviewOverviewEnabling Experimental APIsLenses CapturesExtended Permission Permissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverview Permissions & PrivacyExperimental APIsOn this pageCopy pageExperimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\nWas this page helpful?YesNoPreviousOverview  Permissions & Privacy Permissions & Privacy Experimental APIs Experimental APIs On this page Copy page  Copy page     page Experimental APIs\nOverview\u200b\nExperimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device.\nEnabling Experimental APIs\u200b\nTo Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API\n\nLenses Captures\u200b\nLenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content.\nRemoval of the watermark on recordings from Lenses that use Experimental APIs is prohibited.\n\nExtended Permission\u200b\nLenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule.\nExtended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project.\nEnable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings.\n Experimental APIs Overview\u200b Experimental APIs provide access to features and capabilities of Spectacles that are under development. These APIs may be modified or deprecated without notice and do not guarantee backward compatibility in future releases. When your Lens project uses experimental mode, you cannot publish the Lens to a wider audience, and it remains self-contained to your Lens project and device. Enabling Experimental APIs\u200b To Enable Experimental APIs, go to Project Settings \u2192 Select Allow Experimental API Lenses Captures\u200b Lenses launched with Experimental APIs and recorded (e.g., Video or Photo Capture) will display an \"Experimental Mode\" watermark on the recorded content. Removal of the watermark on recordings from Lenses that use Experimental APIs is prohibited.   Removal of the watermark on recordings from Lenses that use Experimental APIs is prohibited. Removal of the watermark on recordings from Lenses that use Experimental APIs is prohibited. Extended Permission\u200b Lenses using sensitive features like Camera access through the CameraModule or Microphone access through the VoiceML cannot operate simultaneously with features that communicate with the internet, such as the RemoteServiceModule. Extended Permissions un-restricts the access to the Spectacles camera, microphone, location, and internet for experimental Lenses. These permissions apply only to Lenses installed in the Lens Explorer's draft section with the Experimental APIs enabled in the Lens project. Enable Extended Permissions through the mobile Spectacles App under App Settings \u2192 Developer Settings. Was this page helpful?YesNo Was this page helpful? Yes Yes No No Previous Overview OverviewEnabling Experimental APIsLenses CapturesExtended Permission OverviewEnabling Experimental APIsLenses CapturesExtended Permission Overview Enabling Experimental APIs Lenses Captures Extended Permission AI-Powered Search AI-Powered Search  AI-Powered Search AI-Powered Search    CompanySnap Inc.CareersNewsPrivacy and SafetyCommunitySnapchat SupportSpectacles SupportCommunity GuidelinesAdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions RulesLegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement CompanySnap Inc.CareersNewsPrivacy and Safety Company Company Snap Inc. Careers News Privacy and Safety CommunitySnapchat SupportSpectacles SupportCommunity Guidelines Community Community Snapchat Support Spectacles Support Community Guidelines AdvertisingSnapchat AdsAdvertising PoliciesPolitical Ads LibraryBrand GuidelinesPromotions Rules Advertising Advertising Snapchat Ads Advertising Policies Political Ads Library Brand Guidelines Promotions Rules LegalOther Terms & PoliciesLaw EnforcementCookie PolicyReport Infringement Legal Legal Other Terms & Policies Law Enforcement Cookie Policy Report Infringement Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of ServiceLanguageEnglish (US) Privacy PolicyTerms of Service  Privacy Policy Terms of Service LanguageEnglish (US) LanguageEnglish (US) LanguageEnglish (US) English (US)"
}